{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9457906,"sourceType":"datasetVersion","datasetId":5749623}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# First Five Notebooks for Thesis Work. Learn Code and Integrate them","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Advanced Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Create a long enough 'pe' tensor to hold positional encodings\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape (max_len, 1, d_model)\n\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add positional encoding to the input tensor\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        # Global average pooling\n        avg_out = F.adaptive_avg_pool2d(x, 1).view(batch_size, channels)\n        avg_out = F.relu(self.fc1(avg_out))\n        avg_out = self.fc2(avg_out).view(batch_size, channels, 1, 1)\n\n        # Global max pooling\n        max_out = F.adaptive_max_pool2d(x, 1).view(batch_size, channels)\n        max_out = F.relu(self.fc1(max_out))\n        max_out = self.fc2(max_out).view(batch_size, channels, 1, 1)\n\n        # Channel attention\n        out = torch.sigmoid(avg_out + max_out)\n        return x * out\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out = torch.max(x, dim=1, keepdim=True)[0]\n        # Concatenate along the channel dimension\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        attention_map = torch.sigmoid(self.conv1(x_cat))\n        return x * attention_map\n\n\n# Dilated Convolution Block\nclass DilatedConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DilatedConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=2, dilation=2)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=3, dilation=3)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        identity = x  # Save the input for residual connection\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return x + identity  # Add the input back for residual connection\n\n\n# Multi-Scale Convolution Block\nclass MultiScaleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(MultiScaleConvBlock, self).__init__()\n        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.conv5x5 = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2)\n        self.conv7x7 = nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3)\n\n    def forward(self, x):\n        conv3x3_out = F.relu(self.conv3x3(x))\n        conv5x5_out = F.relu(self.conv5x5(x))\n        conv7x7_out = F.relu(self.conv7x7(x))\n        return conv3x3_out + conv5x5_out + conv7x7_out  # Element-wise summation\n\n\n# Encoder with Swin Transformers, Multi-Scale, and Dilated Convolutions\nclass EncoderWithSwin(nn.Module):\n    def __init__(self, in_channels, out_channels, window_size=8, num_heads=4, input_resolution=(64, 64)):\n        super(EncoderWithSwin, self).__init__()\n        self.multi_scale_conv = MultiScaleConvBlock(in_channels, out_channels)\n        self.dilated_conv = DilatedConvBlock(out_channels, out_channels)\n        self.channel_attention = ChannelAttention(out_channels)\n        self.spatial_attention = SpatialAttention()\n        self.pool = nn.MaxPool2d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.multi_scale_conv(x)\n        x = self.dilated_conv(x)\n        x = self.channel_attention(x)\n        x = self.spatial_attention(x)\n\n        # Ensure pooling keeps valid dimensions\n        x = self.pool(x)\n        return x\n\n\n# Cross-Attention Layer\nclass CrossAttention(nn.Module):\n    def __init__(self, num_heads, embed_dim):\n        super(CrossAttention, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n\n    def forward(self, x, context):\n        # Flatten the spatial dimensions to create sequence data for attention\n        batch_size, channels, height, width = x.shape\n        x_flat = x.view(batch_size, height * width, channels)  # (B, H*W, C)\n        context_flat = context.view(batch_size, height * width, channels)  # (B, H*W, C)\n\n        attn_output, _ = self.attention(query=x_flat, key=context_flat, value=context_flat)\n        return attn_output.view(batch_size, channels, height, width)  # Reshape back to (B, C, H, W)\n\n\n# U-Net with Swin Transformers and Attention Mechanisms\nclass EnhancedSwinUNet(nn.Module):\n    def __init__(self, input_shape, patch_size=2):\n        super(EnhancedSwinUNet, self).__init__()\n\n        # Calculate input resolution from input shape\n        height, width = input_shape[0:2]\n        self.input_resolution = (height // patch_size, width // patch_size)\n\n        # Adjusting Positional Encoding to match the decoder output size\n        self.positional_encoding = PositionalEncoding(d_model=128)\n\n        self.enc1 = self.encoder_block(3, 64, self.input_resolution)\n        self.enc2 = self.encoder_block(64, 128, self.input_resolution)\n        self.enc3 = self.encoder_block(128, 256, self.input_resolution)\n        self.enc4 = self.encoder_block(256, 512, self.input_resolution)\n\n        self.bottleneck = self.bottleneck_block(512, 512, self.input_resolution)\n\n        self.upconv4 = self.upconv_block(512, 512)\n        self.upconv3 = self.upconv_block(512, 256)\n        self.upconv2 = self.upconv_block(256, 128)\n\n        self.final_upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n        self.final_conv = nn.Conv2d(128, 3, kernel_size=1, padding=0)\n\n        self.cross_att4 = CrossAttention(num_heads=4, embed_dim=512)  # Set embed_dim to match enc4\n        self.cross_att3 = CrossAttention(num_heads=4, embed_dim=256)  # Set embed_dim to match enc3\n        self.cross_att2 = CrossAttention(num_heads=4, embed_dim=128)  # Set embed_dim to match enc2\n\n    def encoder_block(self, in_channels, out_channels, input_resolution):\n        return EncoderWithSwin(in_channels, out_channels, window_size=8, num_heads=4, input_resolution=input_resolution)\n\n    def bottleneck_block(self, in_channels, out_channels, input_resolution):\n        return EncoderWithSwin(in_channels, out_channels, window_size=8, num_heads=4, input_resolution=input_resolution)\n\n    def upconv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n\n        # Bottleneck\n        bottleneck = self.bottleneck(enc4)\n\n        # Decoder with progressive refinement and dynamic skip connections\n        dec4 = self.upconv4(bottleneck)\n\n        if dec4.shape != enc4.shape:\n            enc4 = F.interpolate(enc4, size=dec4.shape[2:], mode='bilinear', align_corners=False)\n        dec4 = self.cross_att4(dec4, enc4) + enc4\n\n        dec3 = self.upconv3(dec4)\n        if dec3.shape != enc3.shape:\n            enc3 = F.interpolate(enc3, size=dec3.shape[2:], mode='bilinear', align_corners=False)\n        dec3 = self.cross_att3(dec3, enc3) + enc3\n\n        dec2 = self.upconv2(dec3)\n        if dec2.shape != enc2.shape:\n            enc2 = F.interpolate(enc2, size=dec2.shape[2:], mode='bilinear', align_corners=False)\n        dec2 = self.cross_att2(dec2, enc2) + enc2\n\n        # Positional encoding with correct reshaping\n        b, c, h, w = dec2.size()  # Get the batch, channels, height, and width\n        dec2_flat = dec2.view(b, c, h * w).permute(2, 0, 1)  # Flatten and permute for PositionalEncoding\n        dec2_encoded = self.positional_encoding(dec2_flat)  # Positional encoding\n        dec2 = dec2_encoded.permute(1, 2, 0).view(b, c, h, w)  # Reshape back to original dimensions\n\n        # Final upsampling and output layer\n        out = self.final_upsample(dec2)\n        return self.final_conv(out)\n\n# Dataset Class for Rainy and Clear Images\nclass RainDataset(Dataset):\n    def __init__(self, rainy_images, clear_images, transform=None):\n        self.rainy_images = rainy_images\n        self.clear_images = clear_images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.rainy_images)\n\n    def __getitem__(self, idx):\n        rainy_image = self.rainy_images[idx]\n        clear_image = self.clear_images[idx]\n\n        if self.transform:\n            rainy_image = self.transform(rainy_image)\n            clear_image = self.transform(clear_image)\n\n        return rainy_image, clear_image\n\n# Function to load images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in os.listdir(folder):\n        img_path = os.path.join(folder, filename)\n        img = Image.open(img_path).convert('RGB')  # Ensure images are in RGB format\n        img = img.resize(size)  # Resize image to specified size\n        img = np.array(img, dtype=np.float32) / 255.0  # Normalize pixel values to [0, 1] and ensure float32\n        images.append(img)\n    return images\n\n\n# Function to load and preprocess datasets\ndef load_and_preprocess_datasets(base_folder, size=(128, 128)):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    return rainy_images_train, clear_images_train, rainy_images_test, clear_images_test\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Continue the training loop in the main function\ndef main():\n    # Load datasets\n    base_folder = '/kaggle/input/derainingdata/RainData'\n    rainy_images_train, clear_images_train, rainy_images_test, clear_images_test = load_and_preprocess_datasets(base_folder)\n\n    # Create datasets and dataloaders\n    train_dataset = RainDataset(rainy_images_train, clear_images_train, transform=None)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n\n    # Initialize the model\n    model = EnhancedSwinUNet(input_shape=(128, 128, 3)).to(device)\n\n    # Loss and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            rainy_imgs, clear_imgs = batch\n            rainy_imgs, clear_imgs = rainy_imgs.to(device), clear_imgs.to(device)\n\n            # Forward pass\n            optimizer.zero_grad()  # Zero the gradients\n            outputs = model(rainy_imgs)  # Forward pass through the model\n            loss = criterion(outputs, clear_imgs)  # Compute the loss\n            loss.backward()  # Backpropagation\n            optimizer.step()  # Update weights\n\n            running_loss += loss.item()  # Accumulate loss\n\n        avg_loss = running_loss / len(train_loader)\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'enhanced_swin_unet_model.pth')\n\n    # Optionally, you can add evaluation code here to test on the test dataset\n    # ...\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections.abc\nfrom functools import partial\nfrom itertools import repeat\nfrom typing import Dict\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\n\n# Define your BasicLayer and PatchMerging classes in blocks.py\n# Placeholder implementation (modify according to your actual implementations)\nclass PatchMerging(L.Layer):\n    def __init__(self, input_resolution, dim):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n\n    def call(self, x):\n        # Implement your patch merging logic here\n        return x\n\nclass BasicLayer(L.Layer):\n    def __init__(self, dim, out_dim, input_resolution, depth, num_heads, head_dim,\n                 window_size, mlp_ratio, qkv_bias, drop, attn_drop, drop_path,\n                 norm_layer, downsample, name):\n        super().__init__(name=name)\n        # Store parameters\n        self.dim = dim\n        self.out_dim = out_dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.num_heads = num_heads\n        self.head_dim = head_dim\n        self.window_size = window_size\n        self.mlp_ratio = mlp_ratio\n        self.qkv_bias = qkv_bias\n        self.drop = drop\n        self.attn_drop = attn_drop\n        self.drop_path = drop_path\n        self.norm_layer = norm_layer\n        self.downsample = downsample\n\n    def call(self, x):\n        # Implement the basic layer logic (attention + feed-forward) here\n        return x\n\n# Helper function to create tuples\ndef to_ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n\n    return parse\n\nclass SwinTransformer(keras.Model):\n    def __init__(\n        self,\n        img_size=128,  # Adjusted for your image size\n        patch_size=4,\n        num_classes=1000,\n        global_pool=\"avg\",\n        embed_dim=96,\n        depths=(2, 2, 6, 2),\n        num_heads=(3, 6, 12, 24),\n        head_dim=None,\n        window_size=7,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.1,\n        norm_layer=partial(L.LayerNormalization, epsilon=1e-5),\n        ape=False,\n        patch_norm=True,\n        pre_logits=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.img_size = (\n            img_size\n            if isinstance(img_size, collections.abc.Iterable)\n            else (img_size, img_size)\n        )\n        self.patch_size = (\n            patch_size\n            if isinstance(patch_size, collections.abc.Iterable)\n            else (patch_size, patch_size)\n        )\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.ape = ape\n\n        # Split image into non-overlapping patches\n        self.projection = keras.Sequential(\n            [\n                L.Conv2D(\n                    filters=embed_dim,\n                    kernel_size=(patch_size, patch_size),\n                    strides=(patch_size, patch_size),\n                    padding=\"VALID\",\n                    name=\"conv_projection\",\n                    kernel_initializer=\"lecun_normal\",\n                ),\n                L.Reshape(target_shape=(-1, embed_dim), name=\"flatten_projection\"),\n            ],\n            name=\"projection\",\n        )\n        if patch_norm:\n            self.projection.add(norm_layer())\n\n        self.patch_grid = (\n            self.img_size[0] // self.patch_size[0],\n            self.img_size[1] // self.patch_size[1],\n        )\n        self.num_patches = self.patch_grid[0] * self.patch_grid[1]\n\n        # Absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = tf.Variable(\n                tf.zeros((1, self.num_patches, self.embed_dim)),\n                trainable=True,\n                name=\"absolute_pos_embed\",\n            )\n        else:\n            self.absolute_pos_embed = None\n        self.pos_drop = L.Dropout(drop_rate)\n\n        # Build layers\n        if not isinstance(self.embed_dim, (tuple, list)):\n            self.embed_dim = [\n                int(self.embed_dim * 2 ** i) for i in range(self.num_layers)\n            ]\n        embed_out_dim = self.embed_dim[1:] + [None]\n        head_dim = to_ntuple(self.num_layers)(head_dim)\n        window_size = to_ntuple(self.num_layers)(window_size)\n        mlp_ratio = to_ntuple(self.num_layers)(mlp_ratio)\n        dpr = [float(x) for x in tf.linspace(0.0, drop_path_rate, sum(depths))]\n\n        layers = [\n            BasicLayer(\n                dim=self.embed_dim[i],\n                out_dim=embed_out_dim[i],\n                input_resolution=(\n                    self.patch_grid[0] // (2 ** i),\n                    self.patch_grid[1] // (2 ** i),\n                ),\n                depth=depths[i],\n                num_heads=num_heads[i],\n                head_dim=head_dim[i],\n                window_size=window_size[i],\n                mlp_ratio=mlp_ratio[i],\n                qkv_bias=qkv_bias,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i]): sum(depths[: i + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if (i < self.num_layers - 1) else None,\n                name=f\"basic_layer_{i}\",\n            )\n            for i in range(self.num_layers)\n        ]\n        self.swin_layers = layers\n\n        self.norm = norm_layer()\n\n        self.pre_logits = pre_logits\n        if not self.pre_logits:\n            self.head = L.Dense(num_classes, name=\"classification_head\")\n\n    def forward_features(self, x):\n        x = self.projection(x)\n        if self.absolute_pos_embed is not None:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for swin_layer in self.swin_layers:\n            x = swin_layer(x)\n\n        x = self.norm(x)  # [B, L, C]\n        return x\n\n    def forward_head(self, x):\n        if self.global_pool == \"avg\":\n            x = tf.reduce_mean(x, axis=1)\n        return x if self.pre_logits else self.head(x)\n\n    def call(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n    @tf.function(\n        input_signature=[tf.TensorSpec([None, None, None, 3], tf.float32)]\n    )\n    def get_attention_scores(self, x: tf.Tensor) -> Dict[str, Dict[str, tf.Tensor]]:\n        all_attention_scores = {}\n\n        x = self.projection(x)\n        if self.absolute_pos_embed is not None:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for i, swin_layer in enumerate(self.swin_layers):\n            x, attention_scores = swin_layer(x, return_attns=True)\n            all_attention_scores.update({f\"swin_stage_{i}\": attention_scores})\n\n        return all_attention_scores\n\n# To use the model, instantiate it and call it with input data\nif __name__ == \"__main__\":\n    model = SwinTransformer(num_classes=1)  # For image deraining, you may set classes to 1\n    input_data = tf.random.normal((1, 128, 128, 3))  # Batch size of 1, 128x128 RGB image\n    output = model(input_data)\n    print(\"Model output shape:\", output.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LayerNormalization, Dropout\n\nclass WindowAttention(tf.keras.layers.Layer):\n    def __init__(self, window_size=8, num_heads=8):\n        super(WindowAttention, self).__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (window_size * window_size) ** -0.5  # Scaling factor for attention\n\n        # Define Dense layers for q, k, v projections\n        self.q_dense = Dense(units=window_size * window_size)\n        self.k_dense = Dense(units=window_size * window_size)\n        self.v_dense = Dense(units=window_size * window_size)\n\n    def call(self, x):\n        # Extract shape dynamically\n        shape = tf.shape(x)\n        batch_size = shape[0]\n        height = shape[1]\n        width = shape[2]\n        channels = shape[3]\n\n        # Calculate the number of windows\n        num_windows_height = tf.cast(tf.math.ceil(height / self.window_size), tf.int32)\n        num_windows_width = tf.cast(tf.math.ceil(width / self.window_size), tf.int32)\n\n        # Reshape for window-based attention\n        x = tf.reshape(x, [batch_size, num_windows_height, self.window_size, num_windows_width, self.window_size, channels])\n        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])  # Rearrange to [batch_size, num_windows_height, num_windows_width, window_size, window_size, channels]\n        x = tf.reshape(x, [-1, self.window_size * self.window_size, channels])  # Flatten windows\n\n        # Attention mechanism: Apply Dense to project queries, keys, values\n        q = self.q_dense(x)  # Shape: [batch_size * num_windows, window_size * window_size, channels]\n        k = self.k_dense(x)\n        v = self.v_dense(x)\n\n        # Reshape for multi-head attention\n        head_dim = channels // self.num_heads\n        q = tf.reshape(q, [-1, self.window_size * self.window_size, self.num_heads, head_dim])\n        k = tf.reshape(k, [-1, self.window_size * self.window_size, self.num_heads, head_dim])\n        v = tf.reshape(v, [-1, self.window_size * self.window_size, self.num_heads, head_dim])\n\n        # Transpose for dot product: [batch_size * num_windows, num_heads, window_size * window_size, head_dim]\n        q = tf.transpose(q, [0, 2, 1, 3])  # Shape: [batch_size * num_windows, num_heads, window_size * window_size, head_dim]\n        k = tf.transpose(k, [0, 2, 1, 3])\n        v = tf.transpose(v, [0, 2, 1, 3])\n\n        # Scaled dot-product attention: q @ k^T / sqrt(d_k)\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale  # Shape: [batch_size * num_windows, num_heads, window_size * window_size, window_size * window_size]\n        attn = tf.nn.softmax(attn, axis=-1)\n\n        # Apply attention to v\n        out = tf.matmul(attn, v)  # Shape: [batch_size * num_windows, num_heads, window_size * window_size, head_dim]\n\n        # Reshape back: [batch_size * num_windows, window_size * window_size, channels]\n        out = tf.transpose(out, [0, 2, 1, 3])  # Shape: [batch_size * num_windows, window_size * window_size, head_dim, num_heads]\n        out = tf.reshape(out, [-1, self.window_size * self.window_size, channels])  # Shape: [batch_size * num_windows, window_size * window_size, channels]\n\n        # Resize output to match input tensor's spatial dimensions\n        out = tf.reshape(out, [batch_size, num_windows_height, num_windows_width, self.window_size, self.window_size, channels])\n        out = tf.transpose(out, [0, 1, 3, 2, 4, 5])  # Rearrange back\n        out = tf.reshape(out, [batch_size, height, width, channels])  # Final shape\n\n        return out\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size=8, num_heads=4, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.attention = WindowAttention(window_size, num_heads)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(dropout_rate)\n        self.dropout2 = Dropout(dropout_rate)\n        self.mlp = tf.keras.Sequential([\n            Dense(dim * 4, activation='gelu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n\n    def call(self, x):\n        # Attention block\n        attn_out = self.attention(x)\n        x = self.norm1(x + self.dropout1(attn_out))\n\n        # MLP block\n        mlp_out = self.mlp(x)\n        x = self.norm2(x + self.dropout2(mlp_out))\n\n        return x\ndummy_input = tf.random.normal((1, 32, 32, 64))  # Example shape\nswin_block = SwinTransformerBlock(dim=64)\noutput = swin_block(dummy_input)\nprint(output.shape)  # Should match (1, 32, 32, 64)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Without call method","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, LayerNormalization, GlobalAveragePooling1D, Conv2D\nimport numpy as np\n\n# DropPath function for stochastic depth\ndef drop_path(inputs, drop_prob, is_training):\n    if (not is_training) or (drop_prob == 0.):\n        return inputs\n    keep_prob = 1.0 - drop_prob\n    random_tensor = keep_prob\n    shape = (tf.shape(inputs)[0],) + (1,) * (len(tf.shape(inputs)) - 1)\n    random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n    binary_tensor = tf.floor(random_tensor)\n    output = tf.math.divide(inputs, keep_prob) * binary_tensor\n    return output\n\n# DropPath Layer\nclass DropPath(tf.keras.layers.Layer):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def call(self, x, training=None):\n        return drop_path(x, self.drop_prob, training)\n\n# MLP Layer for Feed-Forward Network\nclass Mlp(tf.keras.layers.Layer):\n    def __init__(self, in_features, hidden_features=None, drop=0., prefix=''):\n        super().__init__()\n        hidden_features = hidden_features or in_features\n        self.fc1 = Dense(hidden_features, name=f'{prefix}/fc1')\n        self.fc2 = Dense(in_features, name=f'{prefix}/fc2')\n        self.drop = Dropout(drop)\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = tf.nn.gelu(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n# Window-based Multi-head Self-Attention\nclass WindowAttention(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0., prefix=''):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n        self.qkv = Dense(dim * 3, use_bias=qkv_bias, name=f'{prefix}/qkv')\n        self.attn_drop = Dropout(attn_drop)\n        self.proj = Dense(dim, name=f'{prefix}/proj')\n        self.proj_drop = Dropout(proj_drop)\n\n    def call(self, x, mask=None):\n        B, N, C = x.get_shape().as_list()\n        qkv = tf.reshape(self.qkv(x), shape=[B, N, 3, self.num_heads, C // self.num_heads])\n        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ tf.transpose(k, [0, 1, 3, 2])) * self.scale\n        if mask is not None:\n            mask = tf.expand_dims(mask, axis=1)\n            attn = attn + mask\n        attn = tf.nn.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v)\n        x = tf.transpose(x, [0, 2, 1, 3])\n        x = tf.reshape(x, shape=[B, N, C])\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n# Helper functions for window partition and reverse\ndef window_partition(x, window_size):\n    B, H, W, C = x.get_shape().as_list()\n    x = tf.reshape(x, shape=[B, H // window_size, window_size, W // window_size, window_size, C])\n    windows = tf.reshape(tf.transpose(x, [0, 1, 3, 2, 4, 5]), shape=[-1, window_size, window_size, C])\n    return windows\n\ndef window_reverse(windows, window_size, H, W, C):\n    x = tf.reshape(windows, shape=[-1, H // window_size, W // window_size, window_size, window_size, C])\n    x = tf.transpose(tf.reshape(x, shape=[-1, H // window_size, W // window_size, C]), [0, 1, 3, 2, 4])\n    return tf.reshape(x, shape=[-1, H, W, C])\n\n# Swin Transformer Block\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4.,\n                 qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path_prob=0., norm_layer=LayerNormalization, prefix=''):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window-size\"\n        self.prefix = prefix\n\n        self.norm1 = norm_layer(epsilon=1e-5, name=f'{self.prefix}/norm1')\n        self.attn = WindowAttention(dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n                                    qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, prefix=self.prefix)\n        self.drop_path = DropPath(drop_path_prob if drop_path_prob > 0. else 0.)\n        self.norm2 = norm_layer(epsilon=1e-5, name=f'{self.prefix}/norm2')\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop, prefix=self.prefix)\n\n    def build(self, input_shape):\n        if self.shift_size > 0:\n            H, W = self.input_resolution\n            img_mask = np.zeros([1, H, W, 1])\n            h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            img_mask = tf.convert_to_tensor(img_mask)\n            mask_windows = window_partition(img_mask, self.window_size)\n            mask_windows = tf.reshape(mask_windows, shape=[-1, self.window_size * self.window_size])\n            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(mask_windows, axis=2)\n            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False, name=f'{self.prefix}/attn_mask')\n        else:\n            self.attn_mask = None\n        self.built = True\n\n    def call(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.get_shape().as_list()\n        assert L == H * W, \"input feature has wrong size\"\n        shortcut = x\n        x = self.norm1(x)\n        x = tf.reshape(x, shape=[-1, H, W, C])\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = tf.reshape(x_windows, shape=[-1, self.window_size * self.window_size, C])\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        # merge windows\n        attn_windows = tf.reshape(attn_windows, shape=[-1, self.window_size, self.window_size, C])\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W, C)\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2])\n        else:\n            x = shifted_x\n\n        x = shortcut + self.drop_path(x)\n        x = x + self.norm2(self.mlp(self.norm2(x)))\n        return x\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, LayerNormalization, Dense, Dropout\nfrom tensorflow.keras import Model\n\n# Patch Embedding Layer\nclass PatchEmbed(tf.keras.layers.Layer):\n    def __init__(self, patch_size=4, in_channels=3, embed_dim=96):\n        super(PatchEmbed, self).__init__()\n        self.conv = Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size)\n\n    def call(self, x):\n        x = self.conv(x)  # [B, H, W, C] -> [B, H/P, W/P, embed_dim]\n        return tf.reshape(x, (tf.shape(x)[0], -1, tf.shape(x)[-1]))  # [B, N, C]\n\n# Drop Path Layer\nclass DropPath(tf.keras.layers.Layer):\n    def __init__(self, drop_prob=0.):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def call(self, x, training=False):\n        if not training or self.drop_prob == 0.:\n            return x\n        keep_prob = 1 - self.drop_prob\n        random_tensor = keep_prob + tf.random.uniform(tf.shape(x))  # [B, N, C]\n        random_tensor = tf.floor(random_tensor)  # 0 or 1\n        return x / keep_prob * random_tensor\n\n# Window Attention Layer\nclass WindowAttention(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n        super(WindowAttention, self).__init__()\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.qkv = Dense(dim * 3, use_bias=qkv_bias)\n        self.attn_drop = Dropout(attn_drop)\n        self.proj = Dense(dim)\n\n    def call(self, x):\n        B, N, C = tf.shape(x)\n        qkv = self.qkv(x)  # [B, N, 3 * dim]\n        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n        q, k, v = [tf.reshape(x, (B, N, self.num_heads, self.head_dim)) for x in qkv]\n\n        q = tf.transpose(q, perm=[0, 2, 1, 3])  # [B, num_heads, N, head_dim]\n        k = tf.transpose(k, perm=[0, 2, 1, 3])  # [B, num_heads, N, head_dim]\n        v = tf.transpose(v, perm=[0, 2, 1, 3])  # [B, num_heads, N, head_dim]\n\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale  # [B, num_heads, N, N]\n        attn = tf.nn.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n\n        out = tf.matmul(attn, v)  # [B, num_heads, N, head_dim]\n        out = tf.transpose(out, perm=[0, 2, 1, 3])  # [B, N, num_heads, head_dim]\n        out = tf.reshape(out, (B, N, C))  # [B, N, C]\n\n        return self.proj(out)  # [B, N, C]\n\n# Swin Transformer Block\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0.):\n        super(SwinTransformerBlock, self).__init__()\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.attn = WindowAttention(dim, window_size=window_size, num_heads=num_heads)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else lambda x: x\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        self.mlp = tf.keras.Sequential([\n            Dense(dim * mlp_ratio, activation='gelu'),\n            Dropout(drop),\n            Dense(dim),\n            Dropout(drop),\n        ])\n\n    def call(self, x):\n        identity = x\n        x = self.norm1(x)\n        x = self.attn(x)\n        x = self.drop_path(x) + identity  # Residual connection\n        identity = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        return x + identity  # Residual connection\n\n# Swin Transformer Model\nclass SwinTransformer(Model):\n    def __init__(self, input_size=128, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7):\n        super(SwinTransformer, self).__init__()\n        self.patch_embed = PatchEmbed(patch_size=4, in_channels=3, embed_dim=embed_dim)\n\n        self.swin_layers = []\n        for i in range(len(depths)):\n            for j in range(depths[i]):\n                layer = SwinTransformerBlock(\n                    dim=embed_dim * (2 ** i),\n                    num_heads=num_heads[i],\n                    window_size=window_size\n                )\n                self.swin_layers.append(layer)\n\n        self.norm = LayerNormalization(epsilon=1e-5)\n        self.output_conv = Conv2D(3, kernel_size=1)  # To restore image size\n\n    def call(self, x):\n        x = self.patch_embed(x)  # [B, H, W, C] -> [B, N, C]\n        for layer in self.swin_layers:\n            x = layer(x)\n        x = self.norm(x)\n\n        # Reshape back to original image dimensions\n        B = tf.shape(x)[0]\n        H = 128 // 4  # height after patching\n        W = 128 // 4  # width after patching\n        x = tf.reshape(x, (B, H, W, -1))  # Adjust based on patch size\n        x = self.output_conv(x)  # Restore to original image channels\n        return x\n\n# Testing the model\nif __name__ == \"__main__\":\n    input_size = 128\n    model = SwinTransformer(input_size=input_size)\n    \n    # Create dummy input\n    dummy_input = tf.random.normal([2, input_size, input_size, 3])  # Batch size of 2\n\n    # Forward pass\n    output = model(dummy_input)\n    print(\"Output shape:\", output.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LayerNormalization, Conv2D, Dropout, GlobalAveragePooling1D\n\n# Patch Embedding\nclass PatchEmbed(tf.keras.layers.Layer):\n    def __init__(self, img_size=(128, 128), patch_size=(4, 4), in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__(name='patch_embed')\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.proj = Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size, name='proj')\n        self.norm = norm_layer(epsilon=1e-5, name='norm') if norm_layer is not None else None\n\n    def call(self, x):\n        B, H, W, C = x.get_shape().as_list()\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        x = tf.reshape(x, shape=[-1, (H // self.patch_size[0]) * (W // self.patch_size[1]), self.embed_dim])\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n# Swin Transformer Block\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path_prob=0., norm_layer=LayerNormalization, downsample=None):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.mlp_ratio = mlp_ratio\n        self.qkv_bias = qkv_bias\n        self.drop = drop\n        self.attn_drop = attn_drop\n        self.drop_path_prob = drop_path_prob\n        self.norm_layer = norm_layer\n\n        # Define layers here (like multi-head attention, MLP, etc.)\n        self.norm1 = norm_layer()\n        self.attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm2 = norm_layer()\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),  # Ensure integer shape\n            Dense(dim)\n        ])\n        self.dropout = Dropout(drop)\n    \n    def call(self, x):\n        shortcut = x\n        x = self.norm1(x)\n        attn_output = self.attn(x, x)\n        x = shortcut + attn_output\n        x = self.norm2(x)\n        x = self.mlp(x)\n        return x\n\n# Swin Transformer Model\nclass SwinTransformerModel(tf.keras.Model):\n    def __init__(self, img_size=(128, 128), patch_size=(4, 4), embed_dim=96, depths=[2, 2], num_heads=[4, 8], window_size=8, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, norm_layer=LayerNormalization):\n        super(SwinTransformerModel, self).__init__()\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.window_size = window_size\n        \n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=3, embed_dim=embed_dim, norm_layer=norm_layer)\n        self.pos_drop = Dropout(drop_rate)\n\n        self.basic_layers = tf.keras.Sequential([\n            SwinTransformerBlock(dim=embed_dim * (2 ** i_layer), \n                                 input_resolution=(img_size[0] // (2 ** i_layer), img_size[1] // (2 ** i_layer)),\n                                 num_heads=num_heads[i_layer],\n                                 window_size=window_size,\n                                 norm_layer=norm_layer)\n            for i_layer in range(self.num_layers)\n        ])\n        self.norm = norm_layer(epsilon=1e-5, name='norm')\n        self.avgpool = GlobalAveragePooling1D()\n\n    def call(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n        x = self.basic_layers(x)\n        x = self.norm(x)\n        x = self.avgpool(x)\n        return x\n\n# Main Code to Run Swin Transformer for Image Deraining\nif __name__ == \"__main__\":\n    input_shape = (1, 128, 128, 3)  # Batch size, height, width, channels\n    inputs = tf.random.normal(input_shape)\n    initial_dim = 96\n    window_size = 8\n    num_heads = 4\n    num_layers = 2\n    iterations = 3\n    output = inputs\n\n    for i in range(iterations):\n        # Modify dim for each iteration\n        current_dim = initial_dim * (2 ** i)\n        print(f\"Iteration {i + 1} - Current Dimension: {current_dim}\")\n        # Instantiate a new Swin Transformer for each iteration\n        model = SwinTransformerModel(img_size=(128, 128), patch_size=(4, 4), embed_dim=current_dim, depths=[2, 2], num_heads=[4, 8], window_size=window_size)\n        # Forward pass\n        output = model(output)\n        print(f\"Iteration {i + 1} - Output shape: {output.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport tensorflow as tf\nfrom tensorflow.keras import layers as L\n\n# Reusing the SwinTransformer, BasicLayer, SwinTransformerBlock, Mlp, and WindowAttention classes\n\n# Define the SwinTransformer model and its blocks again\nclass Mlp(tf.keras.layers.Layer):\n    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.0):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = L.Dense(hidden_features, activation=None, name=\"mlp_fc1\")\n        self.act = L.Activation(\"gelu\", name=\"mlp_gelu\")\n        self.fc2 = L.Dense(out_features, activation=None, name=\"mlp_fc2\")\n        self.drop = L.Dropout(drop)\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass WindowAttention(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0.0, proj_drop=0.0):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.qkv = L.Dense(dim * 3, use_bias=qkv_bias)\n        self.attn_drop = L.Dropout(attn_drop)\n        self.proj = L.Dense(dim)\n        self.proj_drop = L.Dropout(proj_drop)\n\n    def call(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x)\n        qkv = tf.reshape(qkv, (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = tf.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n        attn = tf.nn.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n        x = tf.matmul(attn, v)\n        x = tf.transpose(x, (0, 2, 1, 3))\n        x = tf.reshape(x, (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=7, mlp_ratio=4.0, qkv_bias=True, drop=0.0, attn_drop=0.0,\n                 drop_path=0.0, norm_layer=L.LayerNormalization):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.mlp_ratio = mlp_ratio\n        self.norm1 = norm_layer()\n        self.attn = WindowAttention(dim, window_size=window_size, num_heads=num_heads, qkv_bias=qkv_bias,\n                                    attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = L.Dropout(drop_path)\n        self.norm2 = norm_layer()\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), drop=drop)\n\n    def call(self, x):\n        shortcut = x\n        x = self.norm1(x)\n        x = self.attn(x)\n        x = self.drop_path(x)\n        x = shortcut + x\n        shortcut = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.drop_path(x)\n        x = shortcut + x\n        return x\n\nclass BasicLayer(tf.keras.layers.Layer):\n    def __init__(self, dim, out_dim, input_resolution, depth, num_heads, window_size=7, mlp_ratio=4.0, qkv_bias=True,\n                 drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=L.LayerNormalization, downsample=None):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.downsample = downsample\n        self.blocks = [SwinTransformerBlock(dim=dim, num_heads=num_heads, window_size=window_size,\n                                            mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop,\n                                            drop_path=drop_path, norm_layer=norm_layer) for _ in range(depth)]\n\n    def call(self, x):\n        for block in self.blocks:\n            x = block(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\nclass SwinTransformer(tf.keras.Model):\n    def __init__(self, img_size=224, patch_size=4, num_classes=1000, embed_dim=96, depths=(2, 2, 6, 2),\n                 num_heads=(3, 6, 12, 24), window_size=7, mlp_ratio=4.0, qkv_bias=True, drop_rate=0.0, attn_drop_rate=0.0,\n                 drop_path_rate=0.1, norm_layer=L.LayerNormalization, ape=False, patch_norm=True, **kwargs):\n        super().__init__(**kwargs)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.window_size = window_size\n        self.projection = tf.keras.Sequential(\n            [\n                L.Conv2D(filters=embed_dim, kernel_size=(patch_size, patch_size), strides=(patch_size, patch_size),\n                         padding=\"VALID\"),\n                L.Reshape(target_shape=(-1, embed_dim)),\n                norm_layer() if patch_norm else tf.identity\n            ]\n        )\n        embed_out_dim = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n        window_size = tuple(repeat(window_size, self.num_layers))\n        mlp_ratio = tuple(repeat(mlp_ratio, self.num_layers))\n        dpr = [x.numpy() for x in tf.linspace(0.0, drop_path_rate, sum(depths))]\n        self.swin_layers = [\n            BasicLayer(dim=embed_out_dim[i], out_dim=embed_out_dim[i + 1] if i < self.num_layers - 1 else None,\n                       input_resolution=(img_size // (2 ** i), img_size // (2 ** i)), depth=depths[i],\n                       num_heads=num_heads[i], window_size=window_size[i], mlp_ratio=mlp_ratio[i], qkv_bias=qkv_bias,\n                       drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n                       norm_layer=norm_layer) for i in range(self.num_layers)\n        ]\n        self.norm = norm_layer()\n        self.head = L.Dense(num_classes) if num_classes > 0 else tf.identity\n\n    def forward_features(self, x):\n        x = self.projection(x)\n        for layer in self.swin_layers:\n            x = layer(x)\n        x = self.norm(x)\n        return x\n\n    def call(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n# Main Code to Run Swin Transformer for Image Deraining\nif __name__ == \"__main__\":\n    input_shape = (1, 64, 64, 128)  # Batch size, height, width, channels\n    inputs = tf.random.normal(input_shape)\n    initial_dim = 128\n    window_size = 8\n    num_heads = 4\n    num_layers = 2\n    iterations = 3\n    output = inputs\n\n    for i in range(iterations):\n        # Modify dim for each iteration\n        current_dim = initial_dim * (2 ** i)\n        print(f\"Iteration {i + 1} - Current Dimension: {current_dim}\")\n        # Instantiate a new Swin Transformer for each iteration\n        model = SwinTransformer(img_size=64, patch_size=4, embed_dim=current_dim, depths=(2, 2), num_heads=(4, 8),\n                                window_size=window_size)\n        # Forward pass\n        output = model(output)\n        print(f\"Iteration {i + 1} - Output shape: {output.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\nclass WindowAttention(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size, num_heads):\n        super(WindowAttention, self).__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = dim ** -0.5\n\n        self.qkv = layers.Dense(dim * 3)\n        self.proj = layers.Dense(dim)\n\n    def call(self, x):\n        batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n\n        # Ensure height and width are divisible by window size\n        assert height % self.window_size == 0, \"Height must be divisible by window size\"\n        assert width % self.window_size == 0, \"Width must be divisible by window size\"\n\n        # Reshape and partition into windows\n        num_windows_height = height // self.window_size\n        num_windows_width = width // self.window_size\n        x = tf.reshape(x, [batch_size, num_windows_height, self.window_size, num_windows_width, self.window_size, channels])\n        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])  # (B, num_windows_height, num_windows_width, window_size, window_size, C)\n        x = tf.reshape(x, [-1, self.window_size * self.window_size, channels])  # Flatten windows\n\n        # Compute Q, K, V\n        qkv = self.qkv(x)  # (B * num_windows, window_size * window_size, 3 * channels)\n        qkv = tf.reshape(qkv, [-1, self.window_size * self.window_size, 3, channels])\n        q, k, v = tf.unstack(qkv, axis=-2)\n\n        # Scaled dot-product attention\n        attn_weights = tf.matmul(q, k, transpose_b=True) * self.scale\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        attn_output = tf.matmul(attn_weights, v)  # (B * num_windows, window_size * window_size, channels)\n\n        # Reshape and project back\n        attn_output = tf.reshape(attn_output, [-1, num_windows_height, num_windows_width, self.window_size, self.window_size, channels])\n        attn_output = tf.transpose(attn_output, [0, 1, 3, 2, 4, 5])  # (B, H // window_size, W // window_size, window_size, window_size, C)\n        attn_output = tf.reshape(attn_output, [batch_size, height, width, channels])\n\n        output = self.proj(attn_output)\n\n        return output\n\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size, num_heads, mlp_ratio=4.0):\n        super(SwinTransformerBlock, self).__init__()\n        self.attention = WindowAttention(dim, window_size, num_heads)\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='gelu'),\n            layers.Dense(dim)\n        ])\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n        self.dropout1 = layers.Dropout(0.1)\n        self.dropout2 = layers.Dropout(0.1)\n\n    def call(self, x):\n        # Attention block\n        attn_out = self.attention(x)\n        x = self.norm1(x + self.dropout1(attn_out))\n\n        # MLP block\n        mlp_out = self.mlp(x)\n        x = self.norm2(x + self.dropout2(mlp_out))\n\n        return x\n\nclass SwinTransformer(tf.keras.Model):\n    def __init__(self, num_layers, dim, window_size, num_heads):\n        super(SwinTransformer, self).__init__()\n        self.swin_layers = [SwinTransformerBlock(dim, window_size, num_heads) for _ in range(num_layers)]\n\n    def call(self, x):\n        for layer in self.swin_layers:\n            x = layer(x)\n        return x\n\n# Example usage\nif __name__ == \"__main__\":\n    input_shape = (1, 64, 64, 128)  # Batch size, height, width, channels\n    inputs = tf.random.normal(input_shape)\n    \n    model = SwinTransformer(num_layers=4, dim=128, window_size=8, num_heads=4)\n    output = model(inputs)\n    \n    print(f\"Output shape: {output.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reduced","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, LayerNormalization, Add, Conv2DTranspose, BatchNormalization, ReLU\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers\n\n# Enable mixed precision\nmixed_precision.set_global_policy('mixed_float16')\n\n# Swin Transformer Block\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, dim, num_heads, window_size, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(dim),\n            layers.Dropout(dropout_rate),\n        ])\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(x + attn_out)\n        mlp_out = self.mlp(x)\n        x = self.norm2(x + mlp_out)\n        return x\n\n# Channel Attention Mechanism\nclass ChannelAttention(layers.Layer):\n    def __init__(self, channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.fc1 = layers.Dense(channels // reduction, activation='relu')\n        self.fc2 = layers.Dense(channels, activation='sigmoid')\n\n    def call(self, x):\n        avg_pool = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n        max_pool = tf.reduce_max(x, axis=[1, 2], keepdims=True)\n        avg_out = self.fc2(self.fc1(avg_pool))\n        max_out = self.fc2(self.fc1(max_pool))\n        return x * (avg_out + max_out)\n\n# Spatial Attention Mechanism\nclass SpatialAttention(layers.Layer):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n\n    def call(self, x):\n        avg_out = tf.reduce_mean(x, axis=-1, keepdims=True)\n        max_out = tf.reduce_max(x, axis=-1, keepdims=True)\n        concat = tf.concat([avg_out, max_out], axis=-1)\n        return x * self.conv(concat)\n\n# Dilated Convolution Block\nclass DilatedConvBlock(layers.Layer):\n    def __init__(self, filters):\n        super(DilatedConvBlock, self).__init__()\n        self.conv1 = layers.Conv2D(filters, kernel_size=3, padding='same', dilation_rate=1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, kernel_size=3, padding='same', dilation_rate=2, activation='relu')\n        self.conv3 = layers.Conv2D(filters, kernel_size=3, padding='same', dilation_rate=4, activation='relu')\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        return x\n\n# Multi-Scale Convolution Block\nclass MultiScaleConvBlock(layers.Layer):\n    def __init__(self, filters):\n        super(MultiScaleConvBlock, self).__init__()\n        self.conv3x3 = layers.Conv2D(filters, kernel_size=3, padding='same', activation='relu')\n        self.conv5x5 = layers.Conv2D(filters, kernel_size=5, padding='same', activation='relu')\n        self.conv7x7 = layers.Conv2D(filters, kernel_size=7, padding='same', activation='relu')\n\n    def call(self, x):\n        return tf.concat([\n            self.conv3x3(x),\n            self.conv5x5(x),\n            self.conv7x7(x)\n        ], axis=-1)\n\n# Encoder with Swin Transformer and Multi-Scale Features\nclass EncoderWithSwin(layers.Layer):\n    def __init__(self, filters, num_heads, window_size):\n        super(EncoderWithSwin, self).__init__()\n        self.multi_scale_conv = MultiScaleConvBlock(filters)\n        self.pool = layers.MaxPooling2D((2, 2))\n        self.dilated_conv = DilatedConvBlock(filters)\n        self.swin_transformer = [SwinTransformerBlock(dim=filters, num_heads=num_heads, window_size=window_size) for _ in range(2)]\n        self.channel_attention = ChannelAttention(filters)\n        self.spatial_attention = SpatialAttention()\n\n    def call(self, x):\n        x = self.multi_scale_conv(x)\n        x = self.dilated_conv(x)\n        for block in self.swin_transformer:\n            x = block(x)\n        x = self.channel_attention(x)\n        x = self.spatial_attention(x)\n        x = self.pool(x)\n        return x\n\n# U-Net Architecture with Swin Transformers and Attention\nclass EnhancedSwinUNet(tf.keras.Model):\n    def __init__(self, input_shape):\n        super(EnhancedSwinUNet, self).__init__()\n\n        # Define encoder layers\n        self.enc1 = self.encoder_block(3, 32)    # Reduced the filters\n        self.enc2 = self.encoder_block(32, 64)   # Reduced the filters\n        self.enc3 = self.encoder_block(64, 128)  # Reduced the filters\n        self.enc4 = self.encoder_block(128, 256) # Reduced the filters\n\n        # Bottleneck\n        self.bottleneck = self.bottleneck_block(256, 256)\n\n        # Define decoder layers\n        self.upconv4 = self.upconv_block(256, 128)\n        self.upconv3 = self.upconv_block(128, 64)\n        self.upconv2 = self.upconv_block(64, 32)\n\n        # Predefined layers for the final stage\n        self.final_conv = layers.Conv2D(3, kernel_size=(1, 1), padding='same', activation='sigmoid')\n        self.upconv1 = layers.Conv2DTranspose(32, kernel_size=(2, 2), strides=2, padding='same')\n\n        # Conv blocks for the decoder (predefined in __init__)\n        self.conv_block1 = self.create_conv_block(128)\n        self.conv_block2 = self.create_conv_block(64)\n        self.conv_block3 = self.create_conv_block(32)\n        self.conv_block_final = self.create_conv_block(32)  # For the final conv layer\n\n    def call(self, inputs):\n        # Encoder\n        enc1 = self.enc1(inputs)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n\n        # Bottleneck\n        bottleneck_out = self.bottleneck(enc4)\n\n        # Decoder\n        dec4 = self.upconv4(bottleneck_out)\n        dec4 = self.resize_and_add(dec4, enc4)\n        dec4 = self.conv_block1(dec4)\n\n        dec3 = self.upconv3(dec4)\n        dec3 = self.resize_and_add(dec3, enc3)\n        dec3 = self.conv_block2(dec3)\n\n        dec2 = self.upconv2(dec3)\n        dec2 = self.resize_and_add(dec2, enc2)\n        dec2 = self.conv_block3(dec2)\n\n        # Final decoder stage\n        dec1 = self.upconv1(dec2)\n        dec1 = self.conv_block_final(dec1)\n\n        # Final output\n        outputs = self.final_conv(dec1)\n        outputs = tf.image.resize(outputs, [128, 128])\n        return outputs\n\n    def encoder_block(self, in_channels, out_channels):\n        return EncoderWithSwin(out_channels, num_heads=2, window_size=7)  # Reduced num_heads\n\n    def bottleneck_block(self, in_channels, out_channels):\n        return EncoderWithSwin(out_channels, num_heads=2, window_size=7)  # Reduced num_heads\n\n    def upconv_block(self, in_channels, out_channels):\n        return tf.keras.Sequential([\n            layers.Conv2DTranspose(out_channels, kernel_size=2, strides=2, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU()\n        ])\n\n    def create_conv_block(self, filters):\n        return tf.keras.Sequential([\n            layers.Conv2D(filters, kernel_size=(3, 3), padding='same'),\n            layers.Conv2D(filters, kernel_size=(3, 3), padding='same')\n        ])\n\n    def resize_and_add(self, x, skip):\n        \"\"\"Resize x to match skip and add.\"\"\"\n        # Adjust channels of x to match skip connection channels\n        if x.shape[-1] != skip.shape[-1]:\n            x = layers.Conv2D(skip.shape[-1], kernel_size=1)(x)  # Adjust channels if needed\n        x = tf.image.resize(x, size=(skip.shape[1], skip.shape[2]))  # Resize spatial dimensions\n        return layers.add([x, skip])\n\n\n# Load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  \n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Main Execution\nif __name__ == '__main__':\n    # Load your dataset\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Replace with your dataset path\n    (train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    # Instantiate and compile model\n    input_shape = (128, 128, 3)  # Assuming input images are RGB\n    model = EnhancedSwinUNet(input_shape)\n    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n    # Model summary\n    model.summary()\n\n    # Train the model with smaller batch size\n    history = model.fit(train_rainy, train_clear, validation_split=0.1, epochs=20, batch_size=2)  # Reduced batch size\n\n    # Evaluate the model\n    try:\n        predictions = model.predict(test_rainy, batch_size=1)  # Use smaller batch size to avoid memory issues\n\n        # Display predictions\n        for i in range(min(5, predictions.shape[0])):\n            plt.subplot(1, 3, 1)\n            plt.imshow(test_rainy[i])\n            plt.title('Rainy Image')\n            plt.axis('off')\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(predictions[i])\n            plt.title('Predicted Image')\n            plt.axis('off')\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(test_clear[i])\n            plt.title('Ground Truth Image')\n            plt.axis('off')\n\n            plt.show()\n\n        # Calculate PSNR and SSIM\n        mean_psnr, mean_ssim = calculate_metrics(predictions, test_clear)\n        print(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")\n\n    except Exception as e:\n        print(\"Error during evaluation:\", e)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# swin + unet + dilated conv + multiscale conv","metadata":{}},{"cell_type":"code","source":"#Not working swin transformer block\n\n# Window Attention Layer\nclass WindowAttention(nn.Module):\n    def __init__(self, window_size=8, num_heads=8):\n        super(WindowAttention, self).__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (window_size * window_size) ** -0.5\n        \n        self.q_dense = nn.Linear(window_size * window_size, window_size * window_size)\n        self.k_dense = nn.Linear(window_size * window_size, window_size * window_size)\n        self.v_dense = nn.Linear(window_size * window_size, window_size * window_size)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        \n        num_windows_height = (H + self.window_size - 1) // self.window_size\n        num_windows_width = (W + self.window_size - 1) // self.window_size\n        \n        x = x.view(B, num_windows_height, self.window_size, num_windows_width, self.window_size, C)\n        x = x.permute(0, 1, 3, 2, 4, 5).reshape(-1, self.window_size * self.window_size, C)\n        \n        q = self.q_dense(x)\n        k = self.k_dense(x)\n        v = self.v_dense(x)\n\n        head_dim = C // self.num_heads\n        q = q.view(-1, self.window_size * self.window_size, self.num_heads, head_dim).permute(0, 2, 1, 3)\n        k = k.view(-1, self.window_size * self.window_size, self.num_heads, head_dim).permute(0, 2, 1, 3)\n        v = v.view(-1, self.window_size * self.window_size, self.num_heads, head_dim).permute(0, 2, 1, 3)\n\n        attn = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n        attn = torch.softmax(attn, dim=-1)\n        out = torch.matmul(attn, v)\n        \n        out = out.permute(0, 2, 1, 3).contiguous().view(-1, self.window_size * self.window_size, C)\n        out = out.view(B, num_windows_height, num_windows_width, self.window_size, self.window_size, C)\n        out = out.permute(0, 1, 3, 2, 4, 5).reshape(B, H, W, C)\n\n        return out\n\n# Swin Transformer Block\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self, dim, window_size=8, num_heads=4, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.attention = WindowAttention(window_size, num_heads)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(dim, dim)\n        )\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        attn_out = self.attention(x)\n        x = self.norm1(x + self.dropout1(attn_out))\n        mlp_out = self.mlp(x)\n        x = self.norm2(x + self.dropout2(mlp_out))\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#---------------------------------------------------------\n#Swin Transformer Block used in DRT\n#Originally written by Ze Liu\n#Modified by Yuanchu Liang\n#Licensed under The MIT License [see LICENSE for details]\n#---------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\n#fully connected multilayer perceptron\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        #query, key, value share weights\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        #divide depths according to the number of heads\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n        #attention score is calculated by taking the dot product between query and key\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n        #add relative position bias into the attention score\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n        #calculate a softmax version of the attention scores\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n        #multiply attention with value and project x\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n    def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n        # qkv = self.qkv(x)\n        flops += N * self.dim * 3 * self.dim\n        # attn = (q @ k.transpose(-2, -1))\n        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n        #  x = (attn @ v)\n        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size, patch_size, in_chans, embed_dim, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=self.patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        #print(\"PatchEmbed: \" + str(torch.cuda.memory_allocated(0)))\n        B, C, H, W = x.shape # input should be in shape (B C H W)\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        #print(f\"Patch embedding complete! Size is: {x.shape}\")\n        return x\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\n\nclass PatchUnEmbed(nn.Module):\n    r\"\"\" Image to Patch Unembedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size, patch_size, in_chans, unembed_dim, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.unembed_dim = unembed_dim\n\n        self.proj = nn.Conv2d(in_chans, unembed_dim, kernel_size=3, padding=1)\n        self.sample_layer = nn.Upsample(scale_factor=self.patch_size, mode=\"bilinear\") if patch_size != 1 else nn.Identity()\n        #self.activation = nn.LeakyReLU()\n        if norm_layer is not None:\n            self.norm = norm_layer(unembed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x, x_size):\n        B, HW, C = x.shape\n        x = x.transpose(1, 2).view(B, C, x_size[0], x_size[1])  # B C Ph PW\n        #print(f\"Start unembed: {x.shape}\")\n        x = self.sample_layer(x) #B C H W\n        x = self.proj(x)\n        #x = self.activation(x)\n        #print(f\"Patche unembed complete! Size is: {x.shape}\")\n        return x\n\n    def flops(self):\n        flops = 0\n        return flops\n\n\nclass TransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion (H,W).\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=8, shift_size=0,\n                 mlp_ratio=1., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        H, W = self.input_resolution\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n\n    def forward(self, x):\n        #print(\"transformer: \" + str(torch.cuda.memory_allocated(0)))\n        #print(f\"Calculating attion. Input size: {x.shape}\")\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n\n        x = x.view(B, H * W, C) #B L C\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        # W-MSA/SW-MSA\n        nW = H * W / self.window_size / self.window_size\n        flops += nW * self.attn.flops(self.window_size * self.window_size)\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops","metadata":{"execution":{"iopub.status.busy":"2024-10-12T18:02:13.550476Z","iopub.execute_input":"2024-10-12T18:02:13.551128Z","iopub.status.idle":"2024-10-12T18:02:17.342758Z","shell.execute_reply.started":"2024-10-12T18:02:13.551077Z","shell.execute_reply":"2024-10-12T18:02:17.341794Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport warnings\nimport matplotlib.pyplot as plt\nfrom torch.nn.init import trunc_normal_\n\n'''# Configuration and Hyperparameters\ntraining_image_size = 56\ndtype = torch.cuda.FloatTensor\nbatch_size = 5\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed_all(1234)\nepochs = 4600\nlr = 0.0001\nerror_plot_freq = 20\nINT_MAX = 2147483647\nerror_tolerance = 10\npatch_size = 1\n\n# Paths\ndef base_path():\n    # Define your base path logic here\n    return \"path/to/base\"\n\nbase_pth = base_path()\nckp_pth = base_pth + \"/CheckPoints\"\n\n# Miscellaneous Settings\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (16, 9)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)'''\n\n# Basic Block with Transformer and Residual Connection\nclass BasicBlock(nn.Module):\n    def __init__(self, dim, input_resolution, num_heads):\n        super().__init__()\n        self.dim = dim\n        self.transformer = TransformerBlock(self.dim, (input_resolution[0], input_resolution[1]), num_heads)\n\n    def forward(self, x):\n        shortcut = x  # Save the input for the residual connection\n        x = self.transformer(x)\n        x = x + shortcut  # Add the shortcut (residual connection)\n        return x\n\n# Deep Transformer without Recursion\nclass DeepTransformer(nn.Module):\n    def __init__(self, input_channels, output_channels, input_resolution, patch_size):\n        super().__init__()\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.patch_size = patch_size\n        self.input_resolution = input_resolution\n        self.H, self.W = self.input_resolution\n        \n        assert self.H == self.W, \"Input height and width should be the same\"\n        \n        self.input_conv1 = nn.Conv2d(self.input_channels, self.output_channels, 3, padding=1)\n        self.patch_embed = PatchEmbed(img_size=self.H, patch_size=self.patch_size, in_chans=self.output_channels, embed_dim=self.output_channels)\n        self.patch_unembed = PatchUnEmbed(img_size=self.H, patch_size=self.patch_size, in_chans=self.output_channels, unembed_dim=self.input_channels)\n        \n        # Two Transformer Blocks\n        self.transformer_blocks = nn.ModuleList([\n            BasicBlock(dim=self.output_channels, input_resolution=(self.H // self.patch_size, self.W // self.patch_size), num_heads=2) for _ in range(2)\n        ])\n        \n        self.output_conv1 = nn.Conv2d(self.output_channels, self.input_channels, 3, padding=1)\n\n        # Normalization\n        self.normalise_layer = nn.BatchNorm2d(self.output_channels)  # Normalizing across the output channels\n        self.denormalise_layer = nn.BatchNorm2d(self.output_channels)  # Optional, can be removed if not needed\n\n        self.apply(self._init_weights)\n        self.activation = nn.LeakyReLU()\n\n    def _init_weights(self, l):\n        if isinstance(l, nn.Linear):\n            trunc_normal_(l.weight, std=.02)\n            if isinstance(l, nn.Linear) and l.bias is not None:\n                nn.init.constant_(l.bias, 0)\n        elif isinstance(l, nn.LayerNorm):\n            nn.init.constant_(l.bias, 0)\n            nn.init.constant_(l.weight, 1.0)\n\n    def forward(self, x):\n        # Normalize the input\n        x = self.normalise_layer(x)\n        x = self.patch_embed(x)\n\n        # Pass through the two transformer blocks with residual connections\n        for transformer in self.transformer_blocks:\n            x = transformer(x)\n\n        x = self.patch_unembed(x, (self.H // self.patch_size, self.W // self.patch_size))\n        x = self.denormalise_layer(x)  # If necessary, you can remove this line if not needed\n        return x  # Output shape (B, C, H, W)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T18:02:21.540454Z","iopub.execute_input":"2024-10-12T18:02:21.540919Z","iopub.status.idle":"2024-10-12T18:02:21.564374Z","shell.execute_reply.started":"2024-10-12T18:02:21.540869Z","shell.execute_reply":"2024-10-12T18:02:21.563306Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torch.nn.init import trunc_normal_\nfrom PIL import Image\nimport warnings\n\n\n# Configuration and Hyperparameters\ntraining_image_size = 64\ndtype = torch.cuda.FloatTensor\nbatch_size = 1\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed_all(1234)\nepochs = 10\nlr = 0.0001\nerror_plot_freq = 20\nINT_MAX = 2147483647\nerror_tolerance = 10\npatch_size = 2\n\n# Paths\ndef base_path():\n    # Define your base path logic here\n    return \"/kaggle/input/derainingdata/RainData\"\n\nbase_pth = base_path()\nckp_pth = base_pth + \"/CheckPoints\"\n\n# Miscellaneous Settings\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (16, 9)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# Advanced Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=1048576):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Create a long enough 'pe' tensor to hold positional encodings\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape (max_len, 1, d_model)\n\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add positional encoding to the input tensor\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        # Global average pooling\n        avg_out = F.adaptive_avg_pool2d(x, 1).view(batch_size, channels)\n        avg_out = F.relu(self.fc1(avg_out))\n        avg_out = self.fc2(avg_out).view(batch_size, channels, 1, 1)\n\n        # Global max pooling\n        max_out = F.adaptive_max_pool2d(x, 1).view(batch_size, channels)\n        max_out = F.relu(self.fc1(max_out))\n        max_out = self.fc2(max_out).view(batch_size, channels, 1, 1)\n\n        # Channel attention\n        out = torch.sigmoid(avg_out + max_out)\n        return x * out\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out = torch.max(x, dim=1, keepdim=True)[0]\n        # Concatenate along the channel dimension\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        attention_map = torch.sigmoid(self.conv1(x_cat))\n        return x * attention_map\n    \n# Efficient Squeeze-and-Excitation Block\nclass SEBlock(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(SEBlock, self).__init__()\n        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        avg_out = F.adaptive_avg_pool2d(x, 1).view(batch_size, channels)\n        avg_out = F.relu(self.fc1(avg_out))\n        avg_out = self.fc2(avg_out).view(batch_size, channels, 1, 1)\n        return x * torch.sigmoid(avg_out)\n\n\n# Dilated Convolution Block\nclass DilatedConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DilatedConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=2, dilation=2)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=3, dilation=3)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        identity = x  # Save the input for residual connection\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return x + identity  # Add the input back for residual connection\n\n\n# Multi-Scale Convolution Block\nclass MultiScaleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(MultiScaleConvBlock, self).__init__()\n        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.conv5x5 = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2)\n        self.conv7x7 = nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3)\n\n    def forward(self, x):\n        conv3x3_out = F.relu(self.conv3x3(x))\n        conv5x5_out = F.relu(self.conv5x5(x))\n        conv7x7_out = F.relu(self.conv7x7(x))\n        return conv3x3_out + conv5x5_out + conv7x7_out  # Element-wise summation\n\n\n# Encoder with Swin Transformers, Multi-Scale, and Dilated Convolutions\nclass EncoderWithSwin(nn.Module):\n    def __init__(self, in_channels, out_channels, window_size=8, num_heads=4, input_resolution=(64, 64)):\n        super(EncoderWithSwin, self).__init__()\n        self.multi_scale_conv = MultiScaleConvBlock(in_channels, out_channels)\n        self.dilated_conv = DilatedConvBlock(out_channels, out_channels)\n        \n        # Initialize DeepTransformer once in __init__\n        self.swin_transformer = DeepTransformer(input_channels=out_channels, \n                                                output_channels=out_channels, \n                                                input_resolution=input_resolution, \n                                                patch_size=2)\n        #self.se_block = SEBlock(out_channels) -> to reduce parameter can be used it instead of channel and spatial attention\n        self.channel_attention = ChannelAttention(out_channels)\n        self.spatial_attention = SpatialAttention()\n        #self.pool = nn.MaxPool2d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.multi_scale_conv(x)\n        print(f\"shape after multiscale conv: {x.shape}\")\n        x = self.dilated_conv(x)\n        print(f\"shape after dilated conv: {x.shape}\")\n        \n        # Pass through DeepTransformer without passing input_resolution explicitly\n        x = self.swin_transformer(x)  # No input_resolution here\n        print(f\"shape after swin transformer: {x.shape}\")\n        \n        x = self.channel_attention(x)\n        print(f\"shape after channel attention: {x.shape}\")\n        x = self.spatial_attention(x)\n        print(f\"shape after spatial attention: {x.shape}\")\n\n        # Ensure pooling keeps valid dimensions\n        #x = self.pool(x)\n        #print(f\"shape after pooling: {x.shape}\")\n        \n        return x\n# Improved Decoder Block\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DecoderBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x\n\n# Cross-Attention Layer\nclass CrossAttention(nn.Module):\n    def __init__(self, num_heads, embed_dim):\n        super(CrossAttention, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n\n    def forward(self, x, context):\n        # Flatten the spatial dimensions to create sequence data for attention\n        batch_size, channels, height, width = x.shape\n        x_flat = x.view(batch_size, height * width, channels)  # (B, H*W, C)\n        context_flat = context.view(batch_size, height * width, channels)  # (B, H*W, C)\n\n        attn_output, _ = self.attention(query=x_flat, key=context_flat, value=context_flat)\n        return attn_output.view(batch_size, channels, height, width)  # Reshape back to (B, C, H, W)\n\n\n# U-Net with Swin Transformers and Attention Mechanisms\nclass EnhancedSwinUNet(nn.Module):\n    def __init__(self, input_shape, patch_size=2):\n        super(EnhancedSwinUNet, self).__init__()\n\n        # Calculate input resolution from input shape\n        height, width = input_shape[0:2]\n        self.input_resolution = (height, width)\n\n        # Adjusting Positional Encoding to match the decoder output size\n        self.positional_encoding = PositionalEncoding(d_model=128)\n\n        self.enc1 = self.encoder_block(3, 64, self.input_resolution)\n        self.enc2 = self.encoder_block(64, 128, self.input_resolution)\n        self.enc3 = self.encoder_block(128, 256, self.input_resolution)\n        self.enc4 = self.encoder_block(256, 512, self.input_resolution)\n\n        self.bottleneck = self.bottleneck_block(512, 512, self.input_resolution)\n\n        # Improved upsampling and decoding layers\n        self.dec4 = DecoderBlock(512, 512)\n        self.dec3 = DecoderBlock(512, 256)\n        self.dec2 = DecoderBlock(256, 128)\n\n        self.final_upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n        self.final_conv = nn.Conv2d(128, 3, kernel_size=1, padding=0)\n\n        #self.cross_att4 = CrossAttention(num_heads=1, embed_dim=512)  # Set embed_dim to match enc4\n        #self.cross_att3 = CrossAttention(num_heads=1, embed_dim=256)  # Set embed_dim to match enc3\n        #self.cross_att2 = CrossAttention(num_heads=1, embed_dim=128)  # Set embed_dim to match enc2\n\n    def encoder_block(self, in_channels, out_channels, input_resolution):\n        return EncoderWithSwin(in_channels, out_channels, window_size=8, num_heads=4, input_resolution=input_resolution)\n\n    def bottleneck_block(self, in_channels, out_channels, input_resolution):\n        return EncoderWithSwin(in_channels, out_channels, window_size=8, num_heads=4, input_resolution=input_resolution)\n\n    def upconv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n\n        # Bottleneck\n        bottleneck = self.bottleneck(enc4)\n\n        # Decoder with progressive refinement and dynamic skip connections\n        dec4 = self.dec4(bottleneck)\n\n        if dec4.shape != enc4.shape:\n            enc4 = F.interpolate(enc4, size=dec4.shape[2:], mode='bilinear', align_corners=False)\n        #dec4 = self.cross_att4(dec4, enc4)\n        dec4 = dec4 + enc4\n\n        dec3 = self.dec3(dec4)\n        if dec3.shape != enc3.shape:\n            enc3 = F.interpolate(enc3, size=dec3.shape[2:], mode='bilinear', align_corners=False)\n        #dec3 = self.cross_att3(dec3, enc3)\n        dec3 = dec3 + enc3\n\n        dec2 = self.dec2(dec3)\n        if dec2.shape != enc2.shape:\n            enc2 = F.interpolate(enc2, size=dec2.shape[2:], mode='bilinear', align_corners=False)\n        #dec2 = self.cross_att2(dec2, enc2)\n        dec2 = dec2 + enc2\n\n        # Positional encoding with correct reshaping\n        b, c, h, w = dec2.size()  # Get the batch, channels, height, and width\n        dec2_flat = dec2.view(b, c, h * w).permute(2, 0, 1)  # Flatten and permute for PositionalEncoding\n        dec2_encoded = self.positional_encoding(dec2_flat)  # Positional encoding\n        dec2 = dec2_encoded.permute(1, 2, 0).view(b, c, h, w)  # Reshape back to original dimensions\n\n        # Final upsampling and output layer\n        out = self.final_upsample(dec2)\n        out = self.final_conv(out)\n        print(f\"output shape: {out.shape}\")\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-12T18:02:33.790899Z","iopub.execute_input":"2024-10-12T18:02:33.791294Z","iopub.status.idle":"2024-10-12T18:03:58.998650Z","shell.execute_reply.started":"2024-10-12T18:02:33.791256Z","shell.execute_reply":"2024-10-12T18:03:58.997206Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\nBatch 0: Rainy Image Shape torch.Size([1, 3, 128, 128]), Clear Image Shape torch.Size([1, 3, 128, 128])\nBatch 1: Rainy Image Shape torch.Size([1, 3, 128, 128]), Clear Image Shape torch.Size([1, 3, 128, 128])\nBatch 2: Rainy Image Shape torch.Size([1, 3, 128, 128]), Clear Image Shape torch.Size([1, 3, 128, 128])\nBatch 3: Rainy Image Shape torch.Size([1, 3, 128, 128]), Clear Image Shape torch.Size([1, 3, 128, 128])\nBatch 4: Rainy Image Shape torch.Size([1, 3, 128, 128]), Clear Image Shape torch.Size([1, 3, 128, 128])\nBatch 5: Rainy Image Shape torch.Size([1, 3, 128, 128]), Clear Image Shape torch.Size([1, 3, 128, 128])\nshape after multiscale conv: torch.Size([1, 64, 128, 128])\nshape after dilated conv: torch.Size([1, 64, 128, 128])\nshape after swin transformer: torch.Size([1, 64, 128, 128])\nshape after channel attention: torch.Size([1, 64, 128, 128])\nshape after spatial attention: torch.Size([1, 64, 128, 128])\nshape after multiscale conv: torch.Size([1, 128, 128, 128])\nshape after dilated conv: torch.Size([1, 128, 128, 128])\nshape after swin transformer: torch.Size([1, 128, 128, 128])\nshape after channel attention: torch.Size([1, 128, 128, 128])\nshape after spatial attention: torch.Size([1, 128, 128, 128])\nshape after multiscale conv: torch.Size([1, 256, 128, 128])\nshape after dilated conv: torch.Size([1, 256, 128, 128])\nshape after swin transformer: torch.Size([1, 256, 128, 128])\nshape after channel attention: torch.Size([1, 256, 128, 128])\nshape after spatial attention: torch.Size([1, 256, 128, 128])\nshape after multiscale conv: torch.Size([1, 512, 128, 128])\nshape after dilated conv: torch.Size([1, 512, 128, 128])\nshape after swin transformer: torch.Size([1, 512, 128, 128])\nshape after channel attention: torch.Size([1, 512, 128, 128])\nshape after spatial attention: torch.Size([1, 512, 128, 128])\nshape after multiscale conv: torch.Size([1, 512, 128, 128])\nshape after dilated conv: torch.Size([1, 512, 128, 128])\nshape after swin transformer: torch.Size([1, 512, 128, 128])\nshape after channel attention: torch.Size([1, 512, 128, 128])\nshape after spatial attention: torch.Size([1, 512, 128, 128])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 451\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    454\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","Cell \u001b[0;32mIn[3], line 411\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, epochs, device)\u001b[0m\n\u001b[1;32m    409\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m    410\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(rainy)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    412\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    413\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4096) must match the size of tensor b (128) at non-singleton dimension 3"],"ename":"RuntimeError","evalue":"The size of tensor a (4096) must match the size of tensor b (128) at non-singleton dimension 3","output_type":"error"}]},{"cell_type":"code","source":"# Function to load images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in os.listdir(folder):\n        img_path = os.path.join(folder, filename)\n        img = Image.open(img_path).convert('RGB')  # Ensure images are in RGB format\n        img = img.resize(size)  # Resize image to specified size\n        img = np.array(img, dtype=np.float32) / 255.0  # Normalize pixel values to [0, 1] and ensure float32\n        images.append(img)\n    return images\n\n# Data augmentation and preprocessing using torchvision.transforms\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.RandomRotation(degrees=5),\n            transforms.RandomResizedCrop(size=(128, 128), scale=(0.95, 1.05)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),  # Convert to tensor and normalize\n        ])\n\n        augmented_images = []\n        for img in images:\n            img = (img * 255).astype(np.uint8)  # Convert back to uint8 for PIL\n            img = transform(img)  # Apply augmentation\n            augmented_images.append(img)\n        return augmented_images\n    else:\n        return [transforms.ToTensor()(img) for img in images]  # Just convert to tensor if no augmentation\n\n\n# Dataset class for PyTorch\nclass RainDataset(Dataset):\n    def __init__(self, rainy_images, clear_images, transform=None):\n        self.rainy_images = rainy_images\n        self.clear_images = clear_images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.rainy_images)\n\n    def __getitem__(self, idx):\n        rainy_img = self.rainy_images[idx]\n        clear_img = self.clear_images[idx]\n        \n        if self.transform:\n            rainy_img = self.transform(rainy_img)\n            clear_img = self.transform(clear_img)\n\n        # Convert images to tensors without permuting dimensions\n        rainy_img = torch.tensor(rainy_img, dtype=torch.float32)  # [H, W, C]\n        clear_img = torch.tensor(clear_img, dtype=torch.float32)  # [H, W, C]\n\n        return rainy_img.permute(0, 1, 2), clear_img.permute(0, 1, 2)  # Return as [C, H, W]\n    \n# Optimized dataset loading function using PyTorch\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply augmentation if needed\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nimport torch.nn.functional as F\n\n# Define the custom loss functions\nclass CharbonnierLoss(nn.Module):\n    \"\"\"Charbonnier Loss (L1)\"\"\"\n    def __init__(self, eps=1e-3):\n        super(CharbonnierLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, x, y):\n        diff = x.to('cuda:0') - y.to('cuda:0')\n        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps * self.eps)))\n        return loss\n\nclass EdgeLoss(nn.Module):\n    def __init__(self):\n        super(EdgeLoss, self).__init__()\n        k = torch.Tensor([[.05, .25, .4, .25, .05]])\n        self.kernel = torch.matmul(k.t(), k).unsqueeze(0).repeat(3, 1, 1, 1)\n        if torch.cuda.is_available():\n            self.kernel = self.kernel.to('cuda:0')\n        self.loss = CharbonnierLoss()\n\n    def conv_gauss(self, img):\n        n_channels, _, kw, kh = self.kernel.shape\n        img = F.pad(img, (kw // 2, kh // 2, kw // 2, kh // 2), mode='replicate')\n        return F.conv2d(img, self.kernel, groups=n_channels)\n\n    def laplacian_kernel(self, current):\n        filtered = self.conv_gauss(current)\n        down = filtered[:, :, ::2, ::2]\n        new_filter = torch.zeros_like(filtered)\n        new_filter[:, :, ::2, ::2] = down * 4\n        filtered = self.conv_gauss(new_filter)\n        diff = current - filtered\n        return diff\n\n    def forward(self, x, y):\n        loss = self.loss(self.laplacian_kernel(x.to('cuda:0')), self.laplacian_kernel(y.to('cuda:0')))\n        return loss\n\nclass fftLoss(nn.Module):\n    def __init__(self):\n        super(fftLoss, self).__init__()\n\n    def forward(self, x, y):\n        diff = torch.fft.fft2(x.to('cuda:0')) - torch.fft.fft2(y.to('cuda:0'))\n        loss = torch.mean(abs(diff))\n        return loss\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, charbonnier_weight=1.0, edge_weight=1.0, fft_weight=1.0, l1_weight=1.0):\n        super(CombinedLoss, self).__init__()\n        self.charbonnier_loss = CharbonnierLoss()\n        self.edge_loss = EdgeLoss()\n        self.fft_loss = fftLoss()\n        self.l1_loss = nn.L1Loss()  # Using built-in L1 Loss\n        self.charbonnier_weight = charbonnier_weight\n        self.edge_weight = edge_weight\n        self.fft_weight = fft_weight\n        self.l1_weight = l1_weight\n\n    def forward(self, x, y):\n        charbonnier = self.charbonnier_loss(x, y)\n        edge = self.edge_loss(x, y)\n        fft = self.fft_loss(x, y)\n        l1 = self.l1_loss(x.to('cuda:0'), y.to('cuda:0'))  # Calculate L1 loss\n\n        total_loss = (self.charbonnier_weight * charbonnier +\n                      self.edge_weight * edge +\n                      self.fft_weight * fft +\n                      self.l1_weight * l1)  # Combine all losses\n\n        return total_loss\n\n# Function to calculate PSNR and SSIM\ndef calculate_metrics(original, generated):\n    psnr_value = PSNR(original, generated, data_range=1.0)\n    ssim_value = SSIM(original, generated, multichannel=True)\n    return psnr_value, ssim_value\n\n# Evaluation function for test data\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    total_psnr, total_ssim = 0.0, 0.0\n    count = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n\n            # Calculate metrics for each batch\n            psnr_value = PSNR(targets.cpu().numpy(), outputs.cpu().numpy(), data_range=1.0)\n            ssim_value = SSIM(targets.cpu().numpy(), outputs.cpu().numpy(), multichannel=True)\n            \n            total_psnr += psnr_value\n            total_ssim += ssim_value\n            count += 1\n\n    mean_psnr = total_psnr / count\n    mean_ssim = total_ssim / count\n    print(f\"Mean PSNR: {mean_psnr:.4f}, Mean SSIM: {mean_ssim:.4f}\")\n\n# Training function\ndef train_model(model, train_loader, test_loader, epochs=10, device='cuda'):\n    model.to(device)  # Move model to the specified device\n    criterion = CombinedLoss()  # Use CombinedLoss\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Define your optimizer\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0  # Track loss per epoch\n\n        for batch_idx, (rainy, clear) in enumerate(train_loader):\n            rainy, clear = rainy.to(device), clear.to(device)  # Move data to the specified device\n            \n            optimizer.zero_grad()  # Zero the gradients\n            outputs = model(rainy)  # Forward pass\n            loss = criterion(outputs, clear)  # Compute loss\n            loss.backward()  # Backward pass\n            optimizer.step()  # Update weights\n\n            running_loss += loss.item()  # Accumulate loss\n            \n            if batch_idx % 10 == 0:  # Print every 10 batches\n                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n        # Evaluate on test set after each epoch\n        evaluate_model(model, test_loader, device)\n        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss / len(train_loader):.4f}\")  # Average loss\n\n    print(\"Training complete.\")\n\n# Function to display model summary\ndef model_summary(model, input_size):\n    from torchsummary import summary\n    summary(model, input_size)\n\n# Main Execution\nif __name__ == '__main__':\n    # Load your dataset (Replace with your dataset loading function)\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Replace with your dataset path\n    (train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    # Create datasets\n    train_dataset = RainDataset(train_rainy, train_clear)\n    test_dataset = RainDataset(test_rainy, test_clear)\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n    # Device configuration\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Instantiate the model (assuming EnhancedSwinUNet is defined elsewhere)\n    input_shape = (3, 128, 128)  # Assuming input images are RGB\n    model = EnhancedSwinUNet(input_shape).to(device)  # Move model to GPU if available\n\n    # Display model summary\n    model_summary(model, input_shape)\n\n    # Train the model\n    train_model(model, train_loader, test_loader, epochs=10, device=device)\n    \n    # Evaluate the model on test data and display predictions\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for inputs, _ in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs).cpu().numpy()\n            predictions.append(outputs)\n\n    predictions = np.array(predictions)\n    \n    # Display predictions\n    for i in range(min(5, len(predictions))):\n        plt.figure(figsize=(15, 5))\n\n        plt.subplot(1, 3, 1)\n        plt.imshow(test_rainy[i].transpose(1, 2, 0))  # Adjust the dimensions for display\n        plt.title('Rainy Image')\n        plt.axis('off')\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(predictions[i].transpose(1, 2, 0))  # Adjust the dimensions for display\n        plt.title('Predicted Image')\n        plt.axis('off')\n\n        plt.subplot(1, 3, 3)\n        plt.imshow(test_clear[i].transpose(1, 2, 0))  # Adjust the dimensions for display\n        plt.title('Ground Truth Image')\n        plt.axis('off')\n\n        plt.show()\n\n    # Calculate PSNR and SSIM for the test dataset\n    all_psnr, all_ssim = [], []\n    for i in range(len(predictions)):\n        psnr_value, ssim_value = calculate_metrics(test_clear[i], predictions[i])\n        all_psnr.append(psnr_value)\n        all_ssim.append(ssim_value)\n\n    print(f\"Overall PSNR: {np.mean(all_psnr):.4f}, Overall SSIM: {np.mean(all_ssim):.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchvision --upgrade\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, LayerNormalization, Add, Conv2DTranspose, BatchNormalization, ReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers\n\n\nclass WindowAttention(tf.keras.layers.Layer):\n    def __init__(self, window_size=8, num_heads=8):\n        super(WindowAttention, self).__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (window_size * window_size) ** -0.5  # Scaling factor for attention\n\n        # Define Dense layers for q, k, v projections\n        self.q_dense = Dense(units=window_size * window_size)\n        self.k_dense = Dense(units=window_size * window_size)\n        self.v_dense = Dense(units=window_size * window_size)\n\n    def call(self, x):\n        # Extract shape dynamically\n        shape = tf.shape(x)\n        batch_size = shape[0]\n        height = shape[1]\n        width = shape[2]\n        channels = shape[3]\n\n        # Calculate the number of windows\n        num_windows_height = tf.cast(tf.math.ceil(height / self.window_size), tf.int32)\n        num_windows_width = tf.cast(tf.math.ceil(width / self.window_size), tf.int32)\n\n        # Reshape for window-based attention\n        x = tf.reshape(x, [batch_size, num_windows_height, self.window_size, num_windows_width, self.window_size, channels])\n        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])  # Rearrange to [batch_size, num_windows_height, num_windows_width, window_size, window_size, channels]\n        x = tf.reshape(x, [-1, self.window_size * self.window_size, channels])  # Flatten windows\n\n        # Attention mechanism: Apply Dense to project queries, keys, values\n        q = self.q_dense(x)  # Shape: [batch_size * num_windows, window_size * window_size, channels]\n        k = self.k_dense(x)\n        v = self.v_dense(x)\n\n        # Reshape for multi-head attention\n        head_dim = channels // self.num_heads\n        q = tf.reshape(q, [-1, self.window_size * self.window_size, self.num_heads, head_dim])\n        k = tf.reshape(k, [-1, self.window_size * self.window_size, self.num_heads, head_dim])\n        v = tf.reshape(v, [-1, self.window_size * self.window_size, self.num_heads, head_dim])\n\n        # Transpose for dot product: [batch_size * num_windows, num_heads, window_size * window_size, head_dim]\n        q = tf.transpose(q, [0, 2, 1, 3])  # Shape: [batch_size * num_windows, num_heads, window_size * window_size, head_dim]\n        k = tf.transpose(k, [0, 2, 1, 3])\n        v = tf.transpose(v, [0, 2, 1, 3])\n\n        # Scaled dot-product attention: q @ k^T / sqrt(d_k)\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale  # Shape: [batch_size * num_windows, num_heads, window_size * window_size, window_size * window_size]\n        attn = tf.nn.softmax(attn, axis=-1)\n\n        # Apply attention to v\n        out = tf.matmul(attn, v)  # Shape: [batch_size * num_windows, num_heads, window_size * window_size, head_dim]\n\n        # Reshape back: [batch_size * num_windows, window_size * window_size, channels]\n        out = tf.transpose(out, [0, 2, 1, 3])  # Shape: [batch_size * num_windows, window_size * window_size, head_dim, num_heads]\n        out = tf.reshape(out, [-1, self.window_size * self.window_size, channels])  # Shape: [batch_size * num_windows, window_size * window_size, channels]\n\n        # Resize output to match input tensor's spatial dimensions\n        out = tf.reshape(out, [batch_size, num_windows_height, num_windows_width, self.window_size, self.window_size, channels])\n        out = tf.transpose(out, [0, 1, 3, 2, 4, 5])  # Rearrange back\n        out = tf.reshape(out, [batch_size, height, width, channels])  # Final shape\n\n        return out\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size=8, num_heads=4, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.attention = WindowAttention(window_size, num_heads)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(dropout_rate)\n        self.dropout2 = Dropout(dropout_rate)\n        self.mlp = tf.keras.Sequential([\n            Dense(dim * 4, activation='gelu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n\n    def call(self, x):\n        # Attention block\n        attn_out = self.attention(x)\n        x = self.norm1(x + self.dropout1(attn_out))\n\n        # MLP block\n        mlp_out = self.mlp(x)\n        x = self.norm2(x + self.dropout2(mlp_out))\n\n        return x\n\n\n# Channel Attention Mechanism\nclass ChannelAttention(layers.Layer):\n    def __init__(self, channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.fc1 = layers.Dense(channels // reduction, activation='relu')\n        self.fc2 = layers.Dense(channels, activation='sigmoid')\n\n    def call(self, x):\n        avg_pool = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n        max_pool = tf.reduce_max(x, axis=[1, 2], keepdims=True)\n        avg_out = self.fc2(self.fc1(avg_pool))\n        max_out = self.fc2(self.fc1(max_pool))\n        return x * (avg_out + max_out)\n\n# Spatial Attention Mechanism\nclass SpatialAttention(layers.Layer):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n\n    def call(self, x):\n        avg_out = tf.reduce_mean(x, axis=-1, keepdims=True)\n        max_out = tf.reduce_max(x, axis=-1, keepdims=True)\n        concat = tf.concat([avg_out, max_out], axis=-1)\n        return x * self.conv(concat)\n\n# Dilated Convolution Block\nclass DilatedConvBlock(layers.Layer):\n    def __init__(self, filters):\n        super(DilatedConvBlock, self).__init__()\n        self.conv1 = layers.Conv2D(filters, kernel_size=3, padding='same', dilation_rate=1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, kernel_size=3, padding='same', dilation_rate=2, activation='relu')\n        self.conv3 = layers.Conv2D(filters, kernel_size=3, padding='same', dilation_rate=4, activation='relu')\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        return x\n\n# Multi-Scale Convolution Block\nclass MultiScaleConvBlock(layers.Layer):\n    def __init__(self, filters):\n        super(MultiScaleConvBlock, self).__init__()\n        self.conv3x3 = layers.Conv2D(filters, kernel_size=3, padding='same', activation='relu')\n        self.conv5x5 = layers.Conv2D(filters, kernel_size=5, padding='same', activation='relu')\n        self.conv7x7 = layers.Conv2D(filters, kernel_size=7, padding='same', activation='relu')\n\n    def call(self, x):\n        return tf.concat([\n            self.conv3x3(x),\n            self.conv5x5(x),\n            self.conv7x7(x)\n        ], axis=-1)\n\nclass EncoderWithSwin(tf.keras.layers.Layer):\n    def __init__(self, out_channels, window_size=8, num_heads=4, mlp_ratio=4.0):\n        super(EncoderWithSwin, self).__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.mlp_ratio = mlp_ratio\n        self.multi_scale_conv = MultiScaleConvBlock(out_channels)  # Use out_channels\n        self.dilated_conv = DilatedConvBlock(out_channels)  # Use out_channels\n        self.swin_transformer = [TransformerBlock(dim=out_channels, \n                                          input_resolution=input_resolution, \n                                          num_heads=num_heads, \n                                          window_size=window_size) for _ in range(2)]\n\n        self.channel_attention = ChannelAttention(out_channels)\n        self.spatial_attention = SpatialAttention()\n        self.pool = layers.MaxPooling2D((2, 2))\n\n    def call(self, x):\n        # Process through multi-scale convolution and dilated convolution\n        x = self.multi_scale_conv(x)\n        print(f\"shape after multi scale : {x.shape}\")\n        x = self.dilated_conv(x)\n        print(f\"shape after dilation : {x.shape}\")\n\n        '''# Get the original dimensions\n        original_shape = tf.shape(x)[1:3]  # height, width\n\n        # Resize to maintain aspect ratio\n        x = tf.image.resize(x, (128, 128), method='bilinear')\n        print(f\"shape after resizing : {x.shape}\")\n\n        # Pad if necessary to ensure the final shape is 128x128 without losing information\n        pad_height = tf.maximum(0, 128 - tf.shape(x)[1])\n        pad_width = tf.maximum(0, 128 - tf.shape(x)[2])\n\n        paddings = [[0, 0], [0, pad_height], [0, pad_width], [0, 0]]\n        x = tf.pad(x, paddings, mode='CONSTANT')\n\n        # Ensure final output size is always 128x128\n        x = tf.image.resize(x, (128, 128), method='bilinear')  # Resize again to ensure final shape\n\n        # Process through Swin Transformer blocks\n        for swin_block in self.swin_transformer:\n            x = swin_block(x)\n            x = tf.image.resize(x, (128, 128), method='bilinear')\n        print(f\"shape after swin : {x.shape}\")'''\n\n        # Apply channel and spatial attention\n        x = self.channel_attention(x)\n        print(f\"shape after channel attention : {x.shape}\")\n        x = self.spatial_attention(x)\n        print(f\"shape after spatial attention : {x.shape}\")\n\n        # Pooling\n        x = self.pool(x)\n        print(f\"shape after pooling : {x.shape}\")\n\n        return x\n\n\n# Cross-Attention Layer\nclass CrossAttention(layers.Layer):\n    def __init__(self, num_heads):\n        super(CrossAttention, self).__init__()\n        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=64)\n\n    def call(self, x, context):\n        return self.attention(query=x, value=context, key=context)\n\n# U-Net Architecture with Swin Transformers and Attention\nclass EnhancedSwinUNet(Model):\n    def __init__(self, input_shape):\n        super(EnhancedSwinUNet, self).__init__()\n\n        # Define encoder layers\n        self.enc1 = self.encoder_block(3, 64)    # Input channels = 3 for RGB images\n        self.enc2 = self.encoder_block(64, 128)\n        self.enc3 = self.encoder_block(128, 256)\n        self.enc4 = self.encoder_block(256, 512)\n\n        # Bottleneck\n        self.bottleneck = self.bottleneck_block(512, 512)\n\n        # Define decoder layers\n        self.upconv4 = self.upconv_block(512, 512)  # To match enc4\n        self.upconv3 = self.upconv_block(512, 256)  # To match enc3\n        self.upconv2 = self.upconv_block(256, 128)  # To match enc2\n\n        # Predefined layers for the final stage\n        self.final_upsample = UpSampling2D(size=(4, 4), interpolation='bilinear')  # Upsample 32x32 to 128x128\n        self.final_conv = layers.Conv2D(3, kernel_size=(1, 1), padding='same', activation='sigmoid')\n        self.upconv1 = layers.Conv2DTranspose(128, kernel_size=(2, 2), strides=2, padding='same')\n\n        # Conv blocks for the decoder (predefined in __init__)\n        self.cross_att4 = CrossAttention(num_heads=4)\n        self.cross_att3 = CrossAttention(num_heads=4)\n        self.cross_att2 = CrossAttention(num_heads=4)\n\n    def encoder_block(self, in_channels, out_channels):\n        return EncoderWithSwin(out_channels, window_size=8, num_heads=4)  # Correctly pass parameters\n\n    def bottleneck_block(self, in_channels, out_channels):\n        return EncoderWithSwin(out_channels, window_size=8, num_heads=4)  # Correctly pass parameters\n\n    def upconv_block(self, in_channels, out_channels):\n        return layers.Conv2DTranspose(out_channels, kernel_size=(2, 2), strides=2, padding='same')\n\n    def call(self, inputs):\n        # Encoder\n        enc1 = self.enc1(inputs)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n\n        # Bottleneck\n        bottleneck = self.bottleneck(enc4)\n\n        # Decoder with Cross-Attention\n        dec4 = self.upconv4(bottleneck)\n        dec4 = self.cross_att4(dec4, enc4)  # Cross-attention with encoder features\n        dec4 = Add()([dec4, enc4])\n\n        dec3 = self.upconv3(dec4)\n        dec3 = self.cross_att3(dec3, enc3)  # Cross-attention with encoder features\n        dec3 = Add()([dec3, enc3])\n\n        dec2 = self.upconv2(dec3)\n        dec2 = self.cross_att2(dec2, enc2)  # Cross-attention with encoder features\n        dec2 = Add()([dec2, enc2])\n\n        dec2 = self.final_upsample(dec2)  # Upsample to (128, 128)\n        output = self.final_conv(dec2)\n\n        return output\n\n\n# Function to calculate PSNR and SSIM\ndef calculate_metrics(original, generated):\n    psnr_value = PSNR(original, generated)\n    ssim_value = SSIM(original, generated, multichannel=True)\n    return psnr_value, ssim_value\n\n# Training function\ndef train_model(model, train_data, val_data, epochs=10, batch_size=1):\n    callbacks = [\n        EarlyStopping(patience=10, restore_best_weights=True),\n        ModelCheckpoint('best_model.keras', save_best_only=True),\n        ReduceLROnPlateau(patience=5)\n    ]\n\n    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n    history = model.fit(\n        train_data,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=val_data,\n        callbacks=callbacks\n    )\n    return history\n\n# Load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  \n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Main Execution\nif __name__ == '__main__':\n    # Load your dataset\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Replace with your dataset path\n    (train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    # Instantiate and compile model\n    input_shape = (128, 128, 3)  # Assuming input images are RGB\n    model = EnhancedSwinUNet(input_shape)\n    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n    # Model summary\n    model.summary()\n\n    # Train the model\n    history = model.fit(train_rainy, train_clear, validation_split=0.1, epochs=10, batch_size=1)\n\n    # Evaluate the model\n    try:\n        predictions = model.predict(test_rainy, batch_size=1)  # Use smaller batch size to avoid memory issues\n\n        # Display predictions\n        for i in range(min(5, predictions.shape[0])):\n            plt.subplot(1, 3, 1)\n            plt.imshow(test_rainy[i])\n            plt.title('Rainy Image')\n            plt.axis('off')\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(predictions[i])\n            plt.title('Predicted Image')\n            plt.axis('off')\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(test_clear[i])\n            plt.title('Ground Truth Image')\n            plt.axis('off')\n\n            plt.show()\n\n        # Calculate PSNR and SSIM\n        mean_psnr, mean_ssim = calculate_metrics(predictions, test_clear)\n        print(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")\n\n    except Exception as e:\n        print(\"Error during evaluation:\", e)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learned Positional Encoding Layer\nclass LearnedPositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(LearnedPositionalEncoding, self).__init__(**kwargs)\n        height, width, channels = input_shape\n        self.positional_encoding = self.add_weight(\n            shape=(1, height, width, channels),\n            initializer='random_normal',\n            trainable=True\n        )\n\n    def call(self, x):\n        return x + self.positional_encoding\n\nclass GatedWindowedCrossAttention(Layer):\n    def __init__(self, dim, num_heads, window_size=8, **kwargs):\n        super(GatedWindowedCrossAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.gate = Dense(1, activation='sigmoid')\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(dim * 4, activation='relu'),\n            Dropout(0.1),\n            Dense(dim)\n        ])\n        self.dropout = Dropout(0.1)  # Initialize dropout layer\n    \n    def call(self, x, context):\n        shape = tf.shape(x)\n        batch_size = shape[0]\n        height = shape[1]\n        width = shape[2]\n        channels = shape[3]\n        \n        num_windows_y = height // self.window_size\n        num_windows_x = width // self.window_size\n\n        # Reshape for windowed attention\n        x = tf.reshape(x, (batch_size, num_windows_y, self.window_size, num_windows_x, self.window_size, channels))\n        context = tf.reshape(context, (batch_size, num_windows_y, self.window_size, num_windows_x, self.window_size, channels))\n\n        x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n        context = tf.transpose(context, perm=[0, 1, 3, 2, 4, 5])\n\n        x = tf.reshape(x, (batch_size, num_windows_y * num_windows_x, self.window_size * self.window_size, channels))\n        context = tf.reshape(context, (batch_size, num_windows_y * num_windows_x, self.window_size * self.window_size, channels))\n\n        attn_out = self.attn(x, context)\n        attn_out = self.dropout(attn_out)  # Use the initialized dropout layer\n        gate = self.gate(attn_out)\n        attn_out *= gate  # Gate the attention output\n\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n\n\n\n# Squeeze-and-Excitation Block\nclass SqueezeExcitation(Layer):\n    def __init__(self, channels, reduction=16, **kwargs):\n        super(SqueezeExcitation, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.dense1 = Dense(channels // reduction, activation='relu')\n        self.dense2 = Dense(channels, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_avg_pool(inputs)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        scale = tf.reshape(x, [-1, 1, 1, tf.shape(inputs)[-1]])\n        return inputs * scale\n\n# Spatial Attention Block\nclass SpatialAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SpatialAttention, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n        concat = tf.concat([avg_pool, max_pool], axis=-1)\n        attention = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')(concat)\n        return inputs * attention\n\n# Channel Attention Block\nclass ChannelAttention(Layer):\n    def __init__(self, channels, reduction=16, **kwargs):\n        super(ChannelAttention, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.dense1 = Dense(channels // reduction, activation='relu')\n        self.dense2 = Dense(channels, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_avg_pool(inputs)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        scale = tf.reshape(x, [-1, 1, 1, tf.shape(inputs)[-1]])\n        return inputs * scale\n\n# Multi-Scale Feature Aggregation Block\nclass MultiScaleFeatureAggregation(Layer):\n    def __init__(self, filters, **kwargs):\n        super(MultiScaleFeatureAggregation, self).__init__(**kwargs)\n        self.convs = [Conv2D(filters, (3, 3), padding='same', dilation_rate=dilation) for dilation in [1, 2, 4]]\n\n    def call(self, x):\n        return tf.concat([conv(x) for conv in self.convs], axis=-1)  # Concatenate features from different scales\n\n# Pixel-Level Feature Extractor\nclass PixelLevelFeatureExtractor(Layer):\n    def __init__(self, filters, **kwargs):\n        super(PixelLevelFeatureExtractor, self).__init__(**kwargs)\n        self.conv1 = Conv2D(filters, (3, 3), padding='same', activation='relu')\n        self.conv2 = Conv2D(filters, (3, 3), padding='same', activation='relu')\n\n    def call(self, x):\n        return self.conv2(self.conv1(x))\n\n# Global Feature Aggregation Layer\nclass GlobalFeatureAggregation(Layer):\n    def __init__(self, **kwargs):\n        super(GlobalFeatureAggregation, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.fc = Dense(512, activation='relu')\n\n    def call(self, x):\n        global_features = self.global_avg_pool(x)\n        return self.fc(global_features)\n\nclass EnhancedSwinUNet(Model):\n    def __init__(self, input_shape, **kwargs):\n        super(EnhancedSwinUNet, self).__init__(**kwargs)\n\n        # Learned Positional Encoding Layer\n        self.positional_encoding = LearnedPositionalEncoding(input_shape)\n\n        # Encoder Blocks\n        self.encoder1 = self.build_encoder_block(64)\n        self.encoder2 = self.build_encoder_block(128)\n        self.encoder3 = self.build_encoder_block(256)\n\n        # Bottleneck\n        self.bottleneck = self.build_encoder_block(512)\n\n        # Decoder with Progressive Fusion of Pixel-Level and Global Features\n        self.upconv1 = Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder1 = self.build_decoder_block(256)\n\n        self.upconv2 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder2 = self.build_decoder_block(128)\n\n        self.upconv3 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder3 = self.build_decoder_block(64)\n\n        # Pixel-Level Feature Extractor\n        self.pixel_feature_extractor = PixelLevelFeatureExtractor(64)\n\n        # Global Feature Aggregation Layer\n        self.global_feature_aggregation = GlobalFeatureAggregation()\n\n        # Output Layer\n        self.output_conv = Conv2D(3, kernel_size=(1, 1), padding='same', activation='sigmoid')\n\n    def build_encoder_block(self, filters):\n        inputs = Input(shape=(None, None, 3))  # Accept any spatial dimensions\n        x = Conv2D(filters, 3, padding='same', activation='relu')(inputs)\n        x = BatchNormalization()(x)\n        x = Dropout(0.3)(x)\n        x = SqueezeExcitation(filters)(x)  # Ensure this is defined elsewhere\n\n        # Create context using the output of the previous layer\n        context = Conv2D(filters, (1, 1), padding='same')(x)\n\n        # Pass through GatedWindowedCrossAttention\n        x = GatedWindowedCrossAttention(dim=filters, num_heads=4)(x, context)\n    \n        # Option 1: Downsample using MaxPooling\n        x = MaxPooling2D(pool_size=(2, 2))(x)  # Downsample by a factor of 2\n    \n        # Option 2: Downsample using Conv2D with strides\n        # x = Conv2D(filters, (3, 3), strides=(2, 2), padding='same')(x)\n\n        output = Conv2D(filters, (1, 1), padding='same')(x)\n        return Model(inputs, output)\n\n\n    def build_decoder_block(self, filters):\n        return [\n            Conv2D(filters, 3, padding='same', activation='relu'),\n            BatchNormalization(),\n            Dropout(0.3),  # Add Dropout for Regularization\n            SqueezeExcitation(filters),\n        ]\n\n    def call(self, inputs):\n        # Apply positional encoding\n        x = self.positional_encoding(inputs)\n        print(f'After Positional Encoding: {x.shape}')  # Print shape after positional encoding\n\n        # Encoder\n        enc_outputs = []\n        for idx, encoder in enumerate([self.encoder1, self.encoder2, self.encoder3]):\n            x = encoder(x)  # Pass x through the entire encoder block\n            enc_outputs.append(x)\n            print(f'After Encoder Block {idx + 1}: {x.shape}')  # Print shape after each encoder block\n\n        # Bottleneck\n        x = self.bottleneck(x)  # Pass through the bottleneck\n        print(f'After Bottleneck: {x.shape}')  # Print shape after bottleneck\n\n        # Decoder\n        for idx, (upconv, decoder) in enumerate(zip([self.upconv1, self.upconv2, self.upconv3],\n                                                    [self.decoder1, self.decoder2, self.decoder3])):\n            x = upconv(x)  # Upsampl\n            skip_connection = enc_outputs[-(idx + 1)]\n\n            # Extract pixel-level and global features\n            pixel_features = self.pixel_feature_extractor(skip_connection)\n            global_features = self.global_feature_aggregation(skip_connection)\n\n            # Multi-scale feature aggregation\n            multi_scale_features = MultiScaleFeatureAggregation(64)(skip_connection)\n\n            # Progressively fuse pixel and global features\n            fused_features = Add()([pixel_features, global_features, multi_scale_features])\n            combined_features = Add()([fused_features, skip_connection])  # Dynamic skip connection\n\n            for layer in decoder:\n                x = layer(combined_features)  # Pass through the decoder block\n\n            print(f'After Decoder Block {idx + 1}: {x.shape}')  # Print shape after each decoder block\n\n        # Final Output Layer\n        x = self.output_conv(x)\n        print(f'After Output Layer: {x.shape}')  # Print shape after output layer\n\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# swin with cross attention + pixel level + global features + unet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (Layer, Dense, Conv2D, Conv2DTranspose, BatchNormalization, \n                                     Input, LayerNormalization, MultiHeadAttention, Add, \n                                     MaxPooling2D, Dropout, Concatenate, UpSampling2D, GlobalAveragePooling2D)\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR, structural_similarity as ssim\nfrom skimage import io, img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Layer\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Dropout, LayerNormalization, MultiHeadAttention\nfrom tensorflow.keras.layers import BatchNormalization, Add, GlobalAveragePooling2D, MaxPooling2D\n\n\n# Learned Positional Encoding Layer\nclass LearnedPositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(LearnedPositionalEncoding, self).__init__(**kwargs)\n        height, width, channels = input_shape\n        self.positional_encoding = self.add_weight(\n            shape=(1, height, width, channels),\n            initializer='random_normal',\n            trainable=True\n        )\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n\nclass GatedWindowedCrossAttention(Layer):\n    def __init__(self, dim, num_heads, window_size=8, **kwargs):\n        super(GatedWindowedCrossAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.gate = Dense(1, activation='sigmoid')\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(dim * 4, activation='relu'),\n            Dropout(0.1),\n            Dense(dim)\n        ])\n        self.dropout = Dropout(0.1)  # Initialize dropout layer\n    \n    def call(self, x, context):\n        shape = tf.shape(x)\n        batch_size = shape[0]\n        height = shape[1]\n        width = shape[2]\n        channels = shape[3]\n        \n        num_windows_y = height // self.window_size\n        num_windows_x = width // self.window_size\n\n        # Reshape for windowed attention\n        x = tf.reshape(x, (batch_size, num_windows_y, self.window_size, num_windows_x, self.window_size, channels))\n        context = tf.reshape(context, (batch_size, num_windows_y, self.window_size, num_windows_x, self.window_size, channels))\n\n        x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n        context = tf.transpose(context, perm=[0, 1, 3, 2, 4, 5])\n\n        x = tf.reshape(x, (batch_size, num_windows_y * num_windows_x, self.window_size * self.window_size, channels))\n        context = tf.reshape(context, (batch_size, num_windows_y * num_windows_x, self.window_size * self.window_size, channels))\n\n        attn_out = self.attn(x, context)\n        attn_out = self.dropout(attn_out)  # Use the initialized dropout layer\n        gate = self.gate(attn_out)\n        attn_out *= gate  # Gate the attention output\n\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n\n# Squeeze-and-Excitation Block\nclass SqueezeExcitation(Layer):\n    def __init__(self, channels, reduction=16, **kwargs):\n        super(SqueezeExcitation, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.dense1 = Dense(channels // reduction, activation='relu')\n        self.dense2 = Dense(channels, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_avg_pool(inputs)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        scale = tf.reshape(x, [-1, 1, 1, tf.shape(inputs)[-1]])\n        return inputs * scale\n\n\n# Spatial Attention Block\nclass SpatialAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SpatialAttention, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n        concat = tf.concat([avg_pool, max_pool], axis=-1)\n        attention = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')(concat)\n        return inputs * attention\n\n\n# Channel Attention Block\nclass ChannelAttention(Layer):\n    def __init__(self, channels, reduction=16, **kwargs):\n        super(ChannelAttention, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.dense1 = Dense(channels // reduction, activation='relu')\n        self.dense2 = Dense(channels, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_avg_pool(inputs)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        scale = tf.reshape(x, [-1, 1, 1, tf.shape(inputs)[-1]])\n        return inputs * scale\n\n\n# Multi-Scale Feature Aggregation Block\nclass MultiScaleFeatureAggregation(Layer):\n    def __init__(self, filters, **kwargs):\n        super(MultiScaleFeatureAggregation, self).__init__(**kwargs)\n        self.convs = [Conv2D(filters, (3, 3), padding='same', dilation_rate=dilation) for dilation in [1, 2, 4]]\n\n    def call(self, x):\n        return tf.concat([conv(x) for conv in self.convs], axis=-1)  # Concatenate features from different scales\n\n\n# Pixel-Level Feature Extractor\nclass PixelLevelFeatureExtractor(Layer):\n    def __init__(self, filters, **kwargs):\n        super(PixelLevelFeatureExtractor, self).__init__(**kwargs)\n        self.conv1 = Conv2D(filters, (3, 3), padding='same', activation='relu')\n        self.conv2 = Conv2D(filters, (3, 3), padding='same', activation='relu')\n\n    def call(self, x):\n        return self.conv2(self.conv1(x))\n\n\n# Global Feature Aggregation Layer\nclass GlobalFeatureAggregation(Layer):\n    def __init__(self, **kwargs):\n        super(GlobalFeatureAggregation, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.fc = Dense(512, activation='relu')\n\n    def call(self, x):\n        global_features = self.global_avg_pool(x)\n        return self.fc(global_features)\n\n\nclass EnhancedSwinUNet(Model):\n    def __init__(self, input_shape, **kwargs):\n        super(EnhancedSwinUNet, self).__init__(**kwargs)\n\n        # Learned Positional Encoding Layer\n        self.positional_encoding = LearnedPositionalEncoding(input_shape)\n\n        # Encoder Blocks\n        self.encoder1 = self.build_encoder_block(64)\n        self.encoder2 = self.build_encoder_block(128)\n        self.encoder3 = self.build_encoder_block(256)\n\n        # Bottleneck\n        self.bottleneck = self.build_encoder_block(512)\n\n        # Decoder with Progressive Fusion of Pixel-Level and Global Features\n        self.upconv1 = Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder1 = self.build_decoder_block(256)\n\n        self.upconv2 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder2 = self.build_decoder_block(128)\n\n        self.upconv3 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder3 = self.build_decoder_block(64)\n\n        # Pixel-Level Feature Extractor\n        self.pixel_feature_extractor = PixelLevelFeatureExtractor(64)\n\n        # Global Feature Aggregation Layer\n        self.global_feature_aggregation = GlobalFeatureAggregation()\n\n        # Output Layer\n        self.output_conv = Conv2D(3, kernel_size=(1, 1), padding='same', activation='sigmoid')\n\n    def build_encoder_block(self, filters):\n        inputs = Input(shape=(None, None, 3))  # Accept any spatial dimensions\n        x = Conv2D(filters, 3, padding='same', activation='relu')(inputs)\n        x = BatchNormalization()(x)\n        x = Dropout(0.3)(x)\n        x = SqueezeExcitation(filters)(x)\n\n        print(f'After Conv2D (filters={filters}): {x.shape}')  # Print output shape\n\n        # Create context using the output of the previous layer\n        context = Conv2D(filters, (1, 1), padding='same')(x)\n        \n        print(f'After Context Conv2D (filters={filters}): {context.shape}')  # Print output shape\n\n        # Cross Attention\n        x = GatedWindowedCrossAttention(filters, num_heads=4)(x, context)\n\n        print(f'After Cross Attention (filters={filters}): {x.shape}')  # Print output shape\n\n        x = MaxPooling2D(pool_size=(2, 2))(x)  # Reduce spatial dimensions\n\n        print(f'After MaxPooling2D (filters={filters}): {x.shape}')  # Print output shape\n\n        return Model(inputs, x, name=f'encoder_block_{filters}')\n\n    def build_decoder_block(self, filters):\n        inputs = Input(shape=(None, None, filters))  # Accept any spatial dimensions\n        x = Conv2D(filters, (3, 3), padding='same', activation='relu')(inputs)\n        x = BatchNormalization()(x)\n        x = Dropout(0.3)(x)\n        x = SqueezeExcitation(filters)(x)\n\n        print(f'After Conv2D (filters={filters}): {x.shape}')  # Print output shape\n\n        # Spatial attention layer\n        x = SpatialAttention()(x)\n\n        print(f'After Spatial Attention (filters={filters}): {x.shape}')  # Print output shape\n\n        x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.3)(x)\n        x = SqueezeExcitation(filters)(x)\n\n        print(f'After Conv2D (filters={filters}): {x.shape}')  # Print output shape\n\n        return Model(inputs, x, name=f'decoder_block_{filters}')\n\n    def call(self, inputs):\n        x = self.positional_encoding(inputs)\n\n        print(f'After Positional Encoding: {x.shape}')  # Print output shape\n\n        # Encoder\n        enc1 = self.encoder1(x)\n        print(f'After Encoder1: {enc1.shape}')  # Print output shape\n\n        enc2 = self.encoder2(enc1)\n        print(f'After Encoder2: {enc2.shape}')  # Print output shape\n\n        enc3 = self.encoder3(enc2)\n        print(f'After Encoder3: {enc3.shape}')  # Print output shape\n\n        # Bottleneck\n        bottleneck = self.bottleneck(enc3)\n        print(f'After Bottleneck: {bottleneck.shape}')  # Print output shape\n\n        # Decoder\n        dec1 = self.decoder1(self.upconv1(bottleneck))\n        dec1 = Concatenate()([dec1, enc3])\n        print(f'After Decoder1: {dec1.shape}')  # Print output shape\n\n        dec2 = self.decoder2(self.upconv2(dec1))\n        dec2 = Concatenate()([dec2, enc2])\n        print(f'After Decoder2: {dec2.shape}')  # Print output shape\n\n        dec3 = self.decoder3(self.upconv3(dec2))\n        dec3 = Concatenate()([dec3, enc1])\n        print(f'After Decoder3: {dec3.shape}')  # Print output shape\n\n        # Pixel-Level Feature Extraction\n        pixel_features = self.pixel_feature_extractor(dec3)\n        print(f'After Pixel-Level Feature Extraction: {pixel_features.shape}')  # Print output shape\n\n        # Global Feature Aggregation\n        global_features = self.global_feature_aggregation(pixel_features)\n        print(f'After Global Feature Aggregation: {global_features.shape}')  # Print output shape\n\n        # Output Layer\n        outputs = self.output_conv(global_features)\n        print(f'After Output Layer: {outputs.shape}')  # Print output shape\n\n        return outputs\n\n# Load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  \n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Training function with enhancements\ndef train_model(model, train_data, val_data, epochs=10, batch_size=2, learning_rate=1e-4):\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n\n    # Callbacks for learning rate reduction and early stopping\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n        ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    ]\n\n    history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=callbacks)\n    return history\n\n# Evaluate model performance with metrics\ndef evaluate_model(model, test_data):\n    test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(1)\n    psnr_values = []\n    ssim_values = []\n\n    for rainy_image, clear_image in test_dataset:\n        pred_image = model(rainy_image)\n        pred_image = tf.clip_by_value(pred_image, 0, 1)\n\n        psnr_val = PSNR(clear_image.numpy(), pred_image.numpy())\n        ssim_val = ssim(clear_image.numpy(), pred_image.numpy(), multichannel=True)\n\n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n\n    print(f'Mean PSNR: {np.mean(psnr_values)} dB')\n    print(f'Mean SSIM: {np.mean(ssim_values)}')\n\n# Plotting function\ndef plot_results(original, predicted, title='Results'):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(predicted)\n    plt.title('Predicted Image')\n    plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()\n\n# Main function to execute training and evaluation\ndef main():\n    input_shape = (128, 128, 3)\n    model = EnhancedSwinUNet(input_shape)\n    \n    model.summary()\n    \n    base_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n    (train_data, train_labels), (test_data, test_labels) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    history = train_model(model, (train_data, train_labels), (test_data, test_labels), epochs=50)\n\n    # Evaluate on test data\n    evaluate_model(model, (test_data, test_labels))\n\n    # Plot some results\n    for i in range(5):\n        plot_results(test_data[i], model(test_data[i:i+1]).numpy()[0])\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# swin with cross attention + unet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (Layer, Dense, Conv2D, Conv2DTranspose, BatchNormalization, \n                                     Input, LayerNormalization, MultiHeadAttention, Add, \n                                     MaxPooling2D, Dropout, Concatenate, UpSampling2D, GlobalAveragePooling2D)\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR, structural_similarity as ssim\nfrom skimage import io, img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Learned Positional Encoding Layer\nclass LearnedPositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(LearnedPositionalEncoding, self).__init__(**kwargs)\n        height, width, channels = input_shape\n        self.positional_encoding = self.add_weight(\n            shape=(height, width, channels),\n            initializer='random_normal',\n            trainable=True\n        )\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n# Windowed Cross Attention Layer\nclass WindowedCrossAttention(Layer):\n    def __init__(self, dim, num_heads, window_size=8, **kwargs):\n        super(WindowedCrossAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(dim * 4, activation='relu'),\n            Dropout(0.1),\n            Dense(dim)\n        ])\n\n    def call(self, x, context):\n        batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n\n        # Reshape to windowed format for attention\n        x = tf.reshape(x, (batch_size, height // self.window_size, self.window_size, width // self.window_size, self.window_size, channels))\n        context = tf.reshape(context, (batch_size, height // self.window_size, self.window_size, width // self.window_size, self.window_size, channels))\n\n        x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])  # [B, H', W', window_size, window_size, C]\n        context = tf.transpose(context, perm=[0, 1, 3, 2, 4, 5])\n\n        x = tf.reshape(x, (batch_size, -1, self.window_size * self.window_size, channels))  # Flatten windows\n        context = tf.reshape(context, (batch_size, -1, self.window_size * self.window_size, channels))\n\n        attn_out = self.attn(x, context)\n        x = self.norm1(attn_out + x)  # Add and normalize\n        mlp_out = self.mlp(x)          # MLP after attention\n        return self.norm2(mlp_out + x) # Add and normalize again\n\n# Swin Transformer Block\nclass SwinTransformerBlock(Layer):\n    def __init__(self, dim, num_heads, window_size=8, **kwargs):\n        super(SwinTransformerBlock, self).__init__(**kwargs)\n        self.attn = WindowedCrossAttention(dim=dim, num_heads=num_heads, window_size=window_size)\n\n    def call(self, x, context):\n        return self.attn(x, context)\n\n# Squeeze-and-Excitation Block\nclass SqueezeExcitation(Layer):\n    def __init__(self, channels, reduction=16, **kwargs):\n        super(SqueezeExcitation, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.dense1 = Dense(channels // reduction, activation='relu')\n        self.dense2 = Dense(channels, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_avg_pool(inputs)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        scale = tf.reshape(x, [-1, 1, 1, tf.shape(inputs)[-1]])\n        return inputs * scale\n\n# Spatial Attention Block\nclass SpatialAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SpatialAttention, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n        concat = tf.concat([avg_pool, max_pool], axis=-1)\n        attention = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')(concat)\n        return inputs * attention\n\n# Channel Attention Block\nclass ChannelAttention(Layer):\n    def __init__(self, channels, reduction=16, **kwargs):\n        super(ChannelAttention, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.dense1 = Dense(channels // reduction, activation='relu')\n        self.dense2 = Dense(channels, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_avg_pool(inputs)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        scale = tf.reshape(x, [-1, 1, 1, tf.shape(inputs)[-1]])\n        return inputs * scale\n\nclass EnhancedSwinUNet(Model):\n    def __init__(self, input_shape, **kwargs):\n        super(EnhancedSwinUNet, self).__init__(**kwargs)\n\n        # Learned Positional Encoding Layer\n        self.positional_encoding = LearnedPositionalEncoding(input_shape)\n\n        # Encoder\n        self.encoder1 = self.build_encoder_block(64)\n        self.encoder2 = self.build_encoder_block(128)\n        self.encoder3 = self.build_encoder_block(256)\n\n        # Bottleneck\n        self.bottleneck = self.build_bottleneck_block(512)\n\n        # Decoder\n        self.upconv1 = UpSampling2D(size=(2, 2))\n        self.decoder1 = self.build_decoder_block(256)\n\n        self.upconv2 = UpSampling2D(size=(2, 2))\n        self.decoder2 = self.build_decoder_block(128)\n\n        self.upconv3 = UpSampling2D(size=(2, 2))\n        self.decoder3 = self.build_decoder_block(64)\n\n        # Output Layer\n        self.output_conv = Conv2D(3, kernel_size=(1, 1), activation='sigmoid')\n\n    def build_encoder_block(self, filters):\n        return [\n            Conv2D(filters, 3, padding='same', activation='relu'),\n            BatchNormalization(),\n            SqueezeExcitation(filters),\n            SwinTransformerBlock(dim=filters, num_heads=4, window_size=7),\n            MaxPooling2D(pool_size=(2, 2))\n        ]\n\n    def build_bottleneck_block(self, filters):\n        return [\n            Conv2D(filters, 3, padding='same', activation='relu'),\n            BatchNormalization(),\n            SwinTransformerBlock(dim=filters, num_heads=4, window_size=7)\n        ]\n\n    def build_decoder_block(self, filters):\n        return [\n            Conv2D(filters, 3, padding='same', activation='relu'),\n            BatchNormalization(),\n            SqueezeExcitation(filters),\n            SpatialAttention(),\n            ChannelAttention(filters),\n            SwinTransformerBlock(dim=filters, num_heads=4, window_size=7)\n        ]\n\n    def call(self, inputs):\n        # Apply positional encoding\n        x = self.positional_encoding(inputs)\n\n        # Encoder\n        enc_outputs = []\n        for idx, encoder in enumerate([self.encoder1, self.encoder2, self.encoder3]):\n            for layer in encoder:\n                if isinstance(layer, SwinTransformerBlock):\n                    context = enc_outputs[-1] if enc_outputs else x\n                    x = layer(x, context)\n                else:\n                    x = layer(x)\n            enc_outputs.append(x)\n\n        # Bottleneck\n        context = enc_outputs[-1]\n        for layer in self.bottleneck:\n            if isinstance(layer, SwinTransformerBlock):\n                x = layer(x, context)\n            else:\n                x = layer(x)\n\n        # Decoder\n        for idx, decoder in enumerate([self.decoder1, self.decoder2, self.decoder3]):\n            x = self.upconv1(x) if idx == 0 else self.upconv2(x) if idx == 1 else self.upconv3(x)\n\n            # Crop or Pad encoder output to match decoder output\n            encoder_output = enc_outputs[2 - idx]\n            if encoder_output.shape[1] != x.shape[1] or encoder_output.shape[2] != x.shape[2]:\n                # Resize encoder_output to match the shape of x (if necessary)\n                encoder_output = tf.image.resize(encoder_output, size=(x.shape[1], x.shape[2]))\n\n            x = Concatenate()([x, encoder_output])  # Concatenate with corresponding encoder output\n            context = encoder_output  # Use corresponding encoder output as context for the decoder\n            for layer in decoder:\n                if isinstance(layer, SwinTransformerBlock):\n                    x = layer(x, context)\n                else:\n                    x = layer(x)\n\n        # Output\n        return self.output_conv(x)\n\n\n\n\n# Load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  \n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Training function with enhancements\ndef train_model(model, train_data, val_data, epochs=10, batch_size=2, learning_rate=1e-4):\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n\n    # Callbacks for learning rate reduction and early stopping\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n        ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    ]\n\n    history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=callbacks)\n    return history\n\n# Evaluate model performance with metrics\ndef evaluate_model(model, test_data):\n    test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(1)\n    psnr_values = []\n    ssim_values = []\n\n    for rainy_image, clear_image in test_dataset:\n        pred_image = model(rainy_image)\n        pred_image = tf.clip_by_value(pred_image, 0, 1)\n\n        psnr_val = PSNR(clear_image.numpy(), pred_image.numpy())\n        ssim_val = ssim(clear_image.numpy(), pred_image.numpy(), multichannel=True)\n\n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n\n    print(f'Mean PSNR: {np.mean(psnr_values)} dB')\n    print(f'Mean SSIM: {np.mean(ssim_values)}')\n\n# Plotting function\ndef plot_results(original, predicted, title='Results'):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(predicted)\n    plt.title('Predicted Image')\n    plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()\n\n# Main function to execute training and evaluation\ndef main():\n    input_shape = (128, 128, 3)\n    model = EnhancedSwinUNet(input_shape)\n\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n    (train_data, train_labels), (test_data, test_labels) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    history = train_model(model, (train_data, train_labels), (test_data, test_labels), epochs=50)\n\n    # Evaluate on test data\n    evaluate_model(model, (test_data, test_labels))\n\n    # Plot some results\n    for i in range(5):\n        plot_results(test_data[i], model(test_data[i:i+1]).numpy()[0])\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Enhanced swin with unet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (Layer, Dense, Conv2D, Conv2DTranspose, BatchNormalization, \n                                     Input, LayerNormalization, MultiHeadAttention, Add, \n                                     MaxPooling2D, Dropout, Concatenate, UpSampling2D, GlobalAveragePooling2D)\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR, structural_similarity as ssim\nfrom skimage import io, img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Learned Positional Encoding Layer\nclass LearnedPositionalEncoding(Layer):\n    def __init__(self, input_shape, **kwargs):\n        super(LearnedPositionalEncoding, self).__init__(**kwargs)\n        height, width, channels = input_shape\n        self.positional_encoding = self.add_weight(\n            shape=(height, width, channels),\n            initializer='random_normal',\n            trainable=True\n        )\n\n    def call(self, x):\n        return x + self.positional_encoding\n\n# Enhanced Attention Block\nclass EnhancedAttentionBlock(Layer):\n    def __init__(self, dim, num_heads, **kwargs):\n        super(EnhancedAttentionBlock, self).__init__(**kwargs)\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(dim * 4, activation='relu'),\n            Dropout(0.1),\n            Dense(dim)\n        ])\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n# Swin Transformer Block\nclass SwinTransformerBlock(Layer):\n    def __init__(self, dim, num_heads, window_size=8, **kwargs):\n        super(SwinTransformerBlock, self).__init__(**kwargs)\n        self.attn = EnhancedAttentionBlock(dim=dim, num_heads=num_heads)\n        self.window_size = window_size\n\n    def call(self, x):\n        # Reshape for window partitioning (as in original implementation)\n        batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n        x = tf.reshape(x, (batch_size, height // self.window_size, self.window_size, width // self.window_size, self.window_size, channels))\n        x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n        x = tf.reshape(x, (batch_size, -1, self.window_size * self.window_size, channels))\n\n        x = self.attn(x)  # Use Enhanced Attention Block\n        return x\n\n# Squeeze-and-Excitation Block\nclass SqueezeExcitation(Layer):\n    def __init__(self, channels, reduction=16, **kwargs):\n        super(SqueezeExcitation, self).__init__(**kwargs)\n        self.global_avg_pool = GlobalAveragePooling2D()\n        self.dense1 = Dense(channels // reduction, activation='relu')\n        self.dense2 = Dense(channels, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_avg_pool(inputs)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        scale = tf.reshape(x, [-1, 1, 1, tf.shape(inputs)[-1]])\n        return inputs * scale\n\nclass EnhancedSwinUNet(Model):\n    def __init__(self, input_shape, **kwargs):\n        super(EnhancedSwinUNet, self).__init__(**kwargs)\n\n        # Learned Positional Encoding Layer\n        self.positional_encoding = LearnedPositionalEncoding(input_shape)\n\n        # Encoder\n        self.encoder1 = self.build_encoder_block(64)\n        self.encoder2 = self.build_encoder_block(128)\n        self.encoder3 = self.build_encoder_block(256)\n\n        # Bottleneck\n        self.bottleneck = self.build_encoder_block(512)\n\n        # Decoder\n        self.upconv1 = Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder1 = self.build_decoder_block(256)\n\n        self.upconv2 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder2 = self.build_decoder_block(128)\n\n        self.upconv3 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')\n        self.decoder3 = self.build_decoder_block(64)\n\n        # Output Layer\n        self.output_conv = Conv2D(3, kernel_size=(1, 1), activation='sigmoid')\n\n    def build_encoder_block(self, filters):\n        return [\n            Conv2D(filters, 3, padding='same', activation='relu'),\n            BatchNormalization(),\n            SqueezeExcitation(filters),\n            SwinTransformerBlock(dim=filters, num_heads=4, window_size=7),\n            MaxPooling2D(pool_size=(2, 2))\n        ]\n\n    def build_decoder_block(self, filters):\n        return [\n            Conv2D(filters, 3, padding='same', activation='relu'),\n            BatchNormalization(),\n            SqueezeExcitation(filters),\n            SpatialAttention(),\n            ChannelAttention(filters)\n        ]\n\n    def call(self, inputs):\n        # Apply positional encoding\n        x = self.positional_encoding(inputs)\n        print(f'After Positional Encoding: {x.shape}')  # Print shape after positional encoding\n\n        # Encoder\n        enc_outputs = []\n        for idx, encoder in enumerate([self.encoder1, self.encoder2, self.encoder3]):\n            for layer in encoder:\n                if isinstance(layer, SwinTransformerBlock):\n                    x = layer(x, enc_outputs[-1] if enc_outputs else x)  # Pass previous output as context\n                else:\n                    x = layer(x)\n            enc_outputs.append(x)\n            print(f'After Encoder Block: {x.shape}')  # Print shape after each encoder block\n\n        # Bottleneck\n        for layer in self.bottleneck:\n            x = layer(x)\n        print(f'After Bottleneck: {x.shape}')  # Print shape after bottleneck\n\n        # Decoder\n        for idx, decoder in enumerate([self.decoder1, self.decoder2, self.decoder3]):\n            x = self.upconv1(x) if idx == 0 else self.upconv2(x) if idx == 1 else self.upconv3(x)\n            x = Concatenate()([x, enc_outputs[2 - idx]])  # Concatenate with corresponding encoder output\n            print(f'After Upconv {idx + 1}: {x.shape}')  # Print shape after up-convolution\n            for layer in decoder:\n                if isinstance(layer, SwinTransformerBlock):\n                    x = layer(x, enc_outputs[2 - idx])  # Use corresponding encoder output as context\n                else:\n                    x = layer(x)\n            print(f'After Decoder Block {idx + 1}: {x.shape}')  # Print shape after each decoder block\n\n        # Output\n        return self.output_conv(x)\n\n\n# Load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  \n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Training function with enhancements\ndef train_model(model, train_data, val_data, epochs=50, batch_size=2, learning_rate=1e-4):\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n\n    # Callbacks for learning rate reduction and early stopping\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n        ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    ]\n\n    history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=callbacks)\n    return history\n\n# Evaluate model performance with metrics\ndef evaluate_model(model, test_data):\n    test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(1)\n    psnr_values = []\n    ssim_values = []\n\n    for rainy_image, clear_image in test_dataset:\n        pred_image = model(rainy_image)\n        pred_image = tf.clip_by_value(pred_image, 0, 1)\n\n        psnr_val = PSNR(clear_image.numpy(), pred_image.numpy())\n        ssim_val = ssim(clear_image.numpy(), pred_image.numpy(), multichannel=True)\n\n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n\n    print(f'Mean PSNR: {np.mean(psnr_values)} dB')\n    print(f'Mean SSIM: {np.mean(ssim_values)}')\n\n# Plotting function\ndef plot_results(original, predicted, title='Results'):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(predicted)\n    plt.title('Predicted Image')\n    plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()\n\n# Main function to execute training and evaluation\ndef main():\n    input_shape = (128, 128, 3)\n    model = EnhancedSwinUNet(input_shape)\n    \n    model.summary()\n    \n    base_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n    (train_data, train_labels), (test_data, test_labels) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    history = train_model(model, (train_data, train_labels), (test_data, test_labels), epochs=50)\n\n    # Evaluate on test data\n    evaluate_model(model, (test_data, test_labels))\n\n    # Plot some results\n    for i in range(5):\n        plot_results(test_data[i], model(test_data[i:i+1]).numpy()[0])\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unet with pretrained swin","metadata":{}},{"cell_type":"code","source":"pip install transformers tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D\nfrom transformers import TFSwinModel\n\nclass ResizeAndTransformLayer(tf.keras.layers.Layer):\n    def __init__(self, swin_transformer, target_size=(224, 224), **kwargs):\n        super(ResizeAndTransformLayer, self).__init__(**kwargs)\n        self.swin_transformer = swin_transformer\n        self.target_size = target_size\n\n    def call(self, inputs):\n        # Resize to target size\n        x_resized = tf.image.resize(inputs, self.target_size)\n\n        # Ensure the input has 3 channels (RGB)\n        if x_resized.shape[-1] != 3:\n            raise ValueError(\"Input must have 3 channels (RGB). Current channels: {}\".format(x_resized.shape[-1]))\n\n        # Normalize the image to [0, 1] range\n        x_resized = tf.cast(x_resized, tf.float32) / 255.0\n\n        # The Swin model expects the input to be shaped as [batch_size, channels, height, width]\n        # So, we need to change the shape from (batch_size, height, width, channels) to (batch_size, channels, height, width)\n        x_resized = tf.transpose(x_resized, perm=[0, 3, 1, 2])  # Change shape to [1, 3, 224, 224]\n\n        # Call Swin Transformer\n        try:\n            swin_output = self.swin_transformer(pixel_values=x_resized)\n        except ValueError as e:\n            print(f\"Error encountered when calling Swin Transformer: {e}\")\n            raise\n\n        return swin_output.last_hidden_state\n\n# Load model\nswin_transformer = TFSwinModel.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n\n\n# Create instance of the custom layer\nresize_and_transform_layer = ResizeAndTransformLayer(swin_transformer)\n\n# Example usage\ndummy_input = tf.random.uniform((1, 300, 300, 3))  # Example input\noutput = resize_and_transform_layer(dummy_input)\n\nprint(\"Output Shape:\", output.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, \n                                     Concatenate, BatchNormalization, Conv2DTranspose, Multiply, \n                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Add, Layer)\nfrom transformers import TFSwinModel\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage import io, img_as_float\nimport glob\nimport cv2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom keras.layers import Layer\n\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(224, 224)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        try:\n            img = io.imread(filename)\n            img = img_as_float(img)\n            img_resized = tf.image.resize(img, size).numpy()\n            images.append(img_resized)\n        except Exception as e:\n            print(f\"Error loading image {filename}: {e}\")\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False, batch_size=1):\n    if augment and len(images) > 0:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        return next(augmented_images)\n    return images\n\n# Dataset loading and augmentation using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(224, 224), augment=False):\n    datasets = ['Rain200L', 'Rain200H']\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_images_train.extend(load_images_from_folder(os.path.join(train_folder, 'input'), size))\n        clear_images_train.extend(load_images_from_folder(os.path.join(train_folder, 'target'), size))\n\n        rainy_images_test.extend(load_images_from_folder(os.path.join(test_folder, 'input'), size))\n        clear_images_test.extend(load_images_from_folder(os.path.join(test_folder, 'target'), size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Channel attention mechanism\ndef channel_attention(x, reduction_ratio=16):\n    channel_avg = GlobalAveragePooling2D()(x)\n    channel_max = GlobalMaxPooling2D()(x)\n\n    channel_avg = Reshape((1, 1, x.shape[-1]))(channel_avg)\n    channel_max = Reshape((1, 1, x.shape[-1]))(channel_max)\n\n    concat = Concatenate(axis=-1)([channel_avg, channel_max])\n\n    channel_weights = Conv2D(filters=x.shape[-1] // reduction_ratio, kernel_size=1, activation='relu')(concat)\n    channel_weights = Conv2D(filters=x.shape[-1], kernel_size=1, activation='sigmoid')(channel_weights)\n\n    return Multiply()([x, channel_weights])\n\nclass SpatialAttention(Layer):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n\n    def call(self, x):\n        avg_pool = tf.reduce_mean(x, axis=[3], keepdims=True)\n        max_pool = tf.reduce_max(x, axis=[3], keepdims=True)\n\n        concat = tf.concat([avg_pool, max_pool], axis=-1)\n\n        conv = Conv2D(1, kernel_size=7, padding='same')(concat)\n        attention = tf.sigmoid(conv)\n\n        return x * attention  # Element-wise multiplication to apply attention\n\nclass ResizeAndTransformLayer(tf.keras.layers.Layer):\n    def __init__(self, swin_transformer, target_height, target_width):\n        super(ResizeAndTransformLayer, self).__init__()\n        self.swin_transformer = swin_transformer\n        self.target_height = target_height\n        self.target_width = target_width\n        # Conv2D layer to adjust channels to 3 if needed\n        self.conv_to_rgb = tf.keras.layers.Conv2D(3, (1, 1), padding=\"same\")\n\n    def call(self, inputs, training=False):\n        # Resize input tensor to match Swin input requirements\n        x_resized = tf.image.resize(inputs, (self.target_height, self.target_width))\n\n        # Ensure that the input tensor has 3 channels\n        if x_resized.shape[-1] != 3:\n            x_resized = self.conv_to_rgb(x_resized)\n\n        # Make sure the input is float32 (as Swin expects this)\n        x_resized = tf.cast(x_resized, dtype=tf.float32)\n\n        # Pass the resized tensor through the Swin transformer\n        swin_output = self.swin_transformer(pixel_values=x_resized, training=training)\n\n        # Check if swin_output is returned as a dictionary\n        if isinstance(swin_output, dict):\n            return swin_output['last_hidden_state']\n        else:\n            return swin_output.last_hidden_state\n\n\n\n# Custom layer for reshaping Swin Transformer output to fit U-Net\nclass ReshapeSwinOutput(tf.keras.layers.Layer):\n    def __init__(self, target_shape):\n        super(ReshapeSwinOutput, self).__init__()\n        self.target_shape = target_shape\n\n    def call(self, inputs):\n        reshaped_output = tf.reshape(inputs, (-1, *self.target_shape))\n        return reshaped_output\n\n# Dummy Swin Transformer model (replace with actual model loading if necessary)\ndef load_pretrained_swin():\n    swin_transformer = TFSwinModel.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n    return swin_transformer\n\nclass ResizeLayer(Layer):\n    def __init__(self, target_size=(112, 112), **kwargs):\n        super(ResizeLayer, self).__init__(**kwargs)\n        self.target_size = target_size\n\n    def call(self, inputs):\n        return tf.image.resize(inputs, self.target_size, method='bilinear')\n    \n# Building the simplified U-Net with Swin Transformer\ndef build_simplified_unet_swin(input_shape=(224, 224, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder Block 1\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)\n    conv1 = channel_attention(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    # Load Swin Transformer\n    swin_transformer = load_pretrained_swin()\n\n    # Swin Transformer Block 1\n    swin1_block = ResizeAndTransformLayer(swin_transformer=swin_transformer, target_height=112, target_width=112)\n    swin1 = swin1_block(pool1)\n    \n    # Reshape and adjust the channels using the custom layer\n    swin1_reshaped = ReshapeSwinOutput(target_shape=(112, 112, 64))(swin1)\n    swin1_adjusted = Conv2D(64, (1, 1), padding='same')(swin1_reshaped)\n\n    # Create a resizing layer for swin1\n    swin1_resized = ResizeLayer(target_size=(112, 112))(swin1_adjusted)\n\n    # Add skip connection\n    swin1_residual = Add()([pool1, swin1_resized])\n\n    # Encoder Block 2\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1_residual)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    conv2 = channel_attention(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Swin Transformer Block 2\n    swin2_block = ResizeAndTransformLayer(swin_transformer=swin_transformer, target_height=56, target_width=56)\n    swin2 = swin2_block(pool2)\n\n    # Reshape and adjust the channels using the custom layer\n    swin2_reshaped = ReshapeSwinOutput(target_shape=(56, 56, 128))(swin2)\n    swin2_adjusted = Conv2D(128, (1, 1), padding='same')(swin2_reshaped)\n\n    # Create a resizing layer for swin2\n    swin2_resized = ResizeLayer(target_size=(56, 56))(swin2_adjusted)\n\n    # Add skip connection\n    swin2_residual = Add()([pool2, swin2_resized])\n\n    # Bottleneck\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2_residual)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n    conv3 = channel_attention(conv3)\n\n    # Swin Transformer Bottleneck\n    swin2_block = ResizeAndTransformLayer(swin_transformer=swin_transformer, target_height=56, target_width=56)\n    swin_bottleneck = swin_bottleneck_block(conv3)\n\n    # Reshape and adjust the channels using the custom layer\n    swin_bottleneck_reshaped = ReshapeSwinOutput(target_shape=(56, 56, 256))(swin_bottleneck)\n    swin_bottleneck_adjusted = Conv2D(256, (1, 1), padding='same')(swin_bottleneck_reshaped)\n\n    # Resize to match shape before adding\n    swin_bottleneck_resized = ResizeLayer(target_size=(56, 56))(swin_bottleneck_adjusted)\n\n    # Add residual connection for bottleneck\n    swin_bottleneck_residual = Add()([conv3, swin_bottleneck_resized])\n\n    # Decoder Block 1\n    up1 = UpSampling2D(size=(2, 2))(swin_bottleneck_residual)\n    \n    # Resize swin2_residual to match the shape of up1\n    swin2_residual_resized = ResizeLayer(target_size=(112, 112))(swin2_residual)  # Resize to (112, 112)\n    \n    up1 = Concatenate()([up1, swin2_residual_resized])  # Now adding the resized swin2_residual\n    conv4 = Conv2D(128, 3, padding='same', activation='relu')(up1)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Conv2D(128, 1, activation='relu', padding='same')(conv4)\n\n    # Decoder Block 2\n    up2 = UpSampling2D(size=(2, 2))(conv4)\n    \n    # Resize swin1_residual to match the shape of up2\n    swin1_residual_resized = ResizeLayer(target_size=(224, 224))(swin1_residual)  # Resize to (224, 224)\n    \n    up2 = Concatenate()([up2, swin1_residual_resized])  # Now adding the resized swin1_residual\n    conv5 = Conv2D(64, 3, padding='same', activation='relu')(up2)\n    conv5 = BatchNormalization()(conv5)\n    conv5 = Conv2D(64, 1, activation='relu', padding='same')(conv5)\n\n    outputs = Conv2D(3, (1, 1), activation='sigmoid')(conv5)\n\n    model = Model(inputs, outputs)\n    return model\n\n\n# Load and preprocess datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Adjust to your folder path\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Build and compile the model\nmodel = build_simplified_unet_swin(input_shape=(224, 224, 3))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error', metrics=['accuracy'])\n\n# Model Summary\nmodel.summary()\n\n# Callbacks for early stopping and learning rate reduction\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n]\n\n# Training the model\nhistory = model.fit(\n    train_rainy, train_clear,\n    validation_split=0.2,\n    batch_size=1,  # Experiment with batch sizes\n    epochs=15,\n    callbacks=callbacks\n)\n\n# Evaluate model performance on the test set\npredictions = model.predict(test_rainy, batch_size = 1)\n\n# PSNR Calculation\ndef PSNR(target, prediction):\n    mse = np.mean((target - prediction) ** 2)\n    if mse == 0:  # MSE is zero means no noise is present in the signal.\n        return 100  # Return a high value for PSNR\n    max_pixel = 1.0  # Assuming the pixel value range is [0, 1]\n    return 20 * np.log10(max_pixel / np.sqrt(mse))\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    win_size = 3  \n    data_range = 1.0  \n    \n    for i in range(predictions.shape[0]):\n        # Calculate PSNR\n        psnr_val = PSNR(ground_truth[i], predictions[i])\n        \n        # Calculate SSIM\n        ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Apply post-processing on the predictions\ndef post_process_images(predictions):\n    processed_images = []\n    for img in predictions:\n        # Bilateral filter for noise reduction\n        img_bilateral = cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n        \n        # Non-local means denoising\n        img_denoised = cv2.fastNlMeansDenoisingColored((img_bilateral * 255).astype(np.uint8), None, 10, 10, 7, 21)\n        \n        processed_images.append(img_denoised / 255.0)  # Normalize back to [0, 1]\n    return np.array(processed_images)\n\n# Post-process the predicted images\nprocessed_images = post_process_images(predictions)\n\n# Display the results after post-processing\ndef display_processed_results(test_rainy, test_clear, predictions):\n    for i in range(5):\n        plt.figure(figsize=(15, 5))\n        plt.subplot(1, 3, 1)\n        plt.title(\"Rainy Image\")\n        plt.imshow(test_rainy[i])\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 2)\n        plt.title(\"Predicted Clear Image (After Post-Processing)\")\n        plt.imshow(predictions[i])  # After post-processing\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 3)\n        plt.title(\"Ground Truth Clear Image\")\n        plt.imshow(test_clear[i])\n        plt.axis('off')\n        \n        plt.show()\n\n# Run the display function after post-processing\ndisplay_processed_results(test_rainy, test_clear, processed_images)\n\n# Example usage of calculate_metrics with processed images\npsnr, ssim = calculate_metrics(processed_images, test_clear)\n\nprint(f'Test PSNR: {psnr:.2f} dB')\nprint(f'Test SSIM: {ssim:.4f}')\n\n# Save the model if needed\nmodel.save('hybrid_unet_swin_deraining_model.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More advance with attention and postprocessing","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization, Conv2DTranspose, ReLU, Multiply, Add, BatchNormalization, Dropout, GlobalAveragePooling2D, Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage import io, img_as_float\nimport glob\nimport cv2  # Ensure you have opencv installed\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Conv2D, Multiply\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False, batch_size = 1):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  \n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\ndef channel_attention(x, reduction_ratio=16):\n    # Apply global average and max pooling\n    channel_avg = GlobalAveragePooling2D()(x)\n    channel_max = GlobalMaxPooling2D()(x)\n\n    # Reshape to match the number of channels\n    channel_avg = Reshape((1, 1, x.shape[-1]))(channel_avg)\n    channel_max = Reshape((1, 1, x.shape[-1]))(channel_max)\n\n    # Concatenate the pooled features\n    concat = Concatenate(axis=-1)([channel_avg, channel_max])\n\n    # Pass through a convolutional layer to generate attention weights\n    channel_weights = Conv2D(filters=x.shape[-1] // reduction_ratio, kernel_size=1, activation='relu')(concat)\n    channel_weights = Conv2D(filters=x.shape[-1], kernel_size=1, activation='sigmoid')(channel_weights)\n\n    return Multiply()([x, channel_weights])\n\n\ndef spatial_attention(x):\n    # Apply global average and max pooling\n    avg_pool = GlobalAveragePooling2D()(x)  # Use Keras layer\n    max_pool = GlobalMaxPooling2D()(x)      # Use Keras layer\n\n    # Reshape to match the input dimensions\n    avg_pool = Reshape((1, 1, x.shape[-1]))(avg_pool)  # Reshape for broadcasting\n    max_pool = Reshape((1, 1, x.shape[-1]))(max_pool)  # Reshape for broadcasting\n\n    # Concatenate the pooled features\n    concat = Concatenate(axis=-1)([avg_pool, max_pool])\n\n    # Use convolution to get the attention weights\n    attention = Conv2D(filters=1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n\n    # Apply the attention weights to the input\n    return Multiply()([x, attention])\n\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=1, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n# Building a U-Net with one less layer\ndef build_simplified_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder Block 1\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)\n    conv1 = channel_attention(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    swin1 = SwinTransformerBlock(dim=64, num_heads=3, shift_size=1)(pool1)\n    swin1_residual = Add()([pool1, swin1])  # Residual connection after Swin Transformer\n\n    # Encoder Block 2\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1_residual)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    conv2 = channel_attention(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    swin2 = SwinTransformerBlock(dim=128, num_heads=3, shift_size=1)(pool2)\n    swin2_residual = Add()([pool2, swin2])  # Residual connection after Swin Transformer\n\n    # Encoder Block 3 (Deeper Layer)\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2_residual)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n    conv3 = channel_attention(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    swin3 = SwinTransformerBlock(dim=256, num_heads=3, shift_size=1)(pool3)\n    swin3_residual = Add()([pool3, swin3])  # Residual connection after Swin Transformer\n\n    # Bottleneck (Deepest Layer)\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(swin3_residual)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Conv2D(512, 1, activation='relu', padding='same')(conv4)\n\n    swin_bottleneck = SwinTransformerBlock(dim=512, num_heads=3, shift_size=1)(conv4)\n    swin_bottleneck_residual = Add()([conv4, swin_bottleneck])  # Residual connection after Swin Transformer\n\n    # Decoder Block 1\n    up1 = Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin_bottleneck_residual)\n    up1 = Concatenate()([up1, conv3])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv5 = BatchNormalization()(conv5)\n    conv5 = Conv2D(256, 1, activation='relu', padding='same')(conv5)\n    conv5 = channel_attention(conv5)\n    conv5 = spatial_attention(conv5)\n\n    swin4 = SwinTransformerBlock(dim=256, num_heads=3, shift_size=1)(conv5)\n    swin4_residual = Add()([conv5, swin4])  # Residual connection after Swin Transformer\n\n    # Decoder Block 2\n    up2 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin4_residual)\n    up2 = Concatenate()([up2, conv2])\n    conv6 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv6 = BatchNormalization()(conv6)\n    conv6 = Conv2D(128, 1, activation='relu', padding='same')(conv6)\n    conv6 = channel_attention(conv6)\n    conv6 = spatial_attention(conv6)\n\n    swin5 = SwinTransformerBlock(dim=128, num_heads=3, shift_size=1)(conv6)\n    swin5_residual = Add()([conv6, swin5])  # Residual connection after Swin Transformer\n\n    # Decoder Block 3 (Shallowest Layer)\n    up3 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin5_residual)\n    up3 = Concatenate()([up3, conv1])\n    conv7 = DepthwiseConv2D(3, padding='same', activation='relu')(up3)\n    conv7 = BatchNormalization()(conv7)\n    conv7 = Conv2D(64, 1, activation='relu', padding='same')(conv7)\n    conv7 = channel_attention(conv7)\n    conv7 = spatial_attention(conv7)\n    \n    swin6 = SwinTransformerBlock(dim=64, num_heads=3, shift_size=1)(conv7)\n    swin6_residual = Add()([conv7, swin6])  # Residual connection after Swin Transformer\n\n    # Output Layer\n    output = Conv2D(3, kernel_size=(1, 1), padding='same', activation='sigmoid')(swin6_residual)\n\n    model = Model(inputs=inputs, outputs=output)\n    return model\n\n# Build and compile the model\n#model = build_simplified_unet_swin(input_shape=(128, 128, 3))\n\n# Load and preprocess datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Adjust to your folder path\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Build and compile the model\nmodel = build_simplified_unet_swin(input_shape=(128, 128, 3))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error', metrics=['accuracy'])\n\n# Model Summary\nmodel.summary()\n\n# Callbacks for early stopping and learning rate reduction\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n]\n\n# Training the model\nhistory = model.fit(\n    train_rainy, train_clear,\n    validation_split=0.2,\n    batch_size=1,  # Experiment with batch sizes\n    epochs=15,\n    callbacks=callbacks\n)\n\n# Evaluate model performance on the test set\npredictions = model.predict(test_rainy, batch_size = 1)\n\n# PSNR Calculation\ndef PSNR(target, prediction):\n    mse = np.mean((target - prediction) ** 2)\n    if mse == 0:  # MSE is zero means no noise is present in the signal.\n        return 100  # Return a high value for PSNR\n    max_pixel = 1.0  # Assuming the pixel value range is [0, 1]\n    return 20 * np.log10(max_pixel / np.sqrt(mse))\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    win_size = 3  \n    data_range = 1.0  \n    \n    for i in range(predictions.shape[0]):\n        # Calculate PSNR\n        psnr_val = PSNR(ground_truth[i], predictions[i])\n        \n        # Calculate SSIM\n        ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Apply post-processing on the predictions\ndef post_process_images(predictions):\n    processed_images = []\n    for img in predictions:\n        # Bilateral filter for noise reduction\n        img_bilateral = cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n        \n        # Non-local means denoising\n        img_denoised = cv2.fastNlMeansDenoisingColored((img_bilateral * 255).astype(np.uint8), None, 10, 10, 7, 21)\n        \n        processed_images.append(img_denoised / 255.0)  # Normalize back to [0, 1]\n    return np.array(processed_images)\n\n# Post-process the predicted images\nprocessed_images = post_process_images(predictions)\n\n# Display the results after post-processing\ndef display_processed_results(test_rainy, test_clear, predictions):\n    for i in range(5):\n        plt.figure(figsize=(15, 5))\n        plt.subplot(1, 3, 1)\n        plt.title(\"Rainy Image\")\n        plt.imshow(test_rainy[i])\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 2)\n        plt.title(\"Predicted Clear Image (After Post-Processing)\")\n        plt.imshow(predictions[i])  # After post-processing\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 3)\n        plt.title(\"Ground Truth Clear Image\")\n        plt.imshow(test_clear[i])\n        plt.axis('off')\n        \n        plt.show()\n\n# Run the display function after post-processing\ndisplay_processed_results(test_rainy, test_clear, processed_images)\n\n# Example usage of calculate_metrics with processed images\npsnr, ssim = calculate_metrics(processed_images, test_clear)\n\nprint(f'Test PSNR: {psnr:.2f} dB')\nprint(f'Test SSIM: {ssim:.4f}')\n\n# Save the model if needed\nmodel.save('hybrid_unet_swin_deraining_model.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More depth unet + swin","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization, Conv2DTranspose, ReLU, Multiply, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, GlobalAveragePooling2D, Reshape\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  # Add this line to define clear_test_folder\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n\n# Define Channel Attention Mechanism\ndef channel_attention(x, reduction_ratio=16):\n    channel_avg = GlobalAveragePooling2D()(x)\n    channel_avg = Reshape((1, 1, channel_avg.shape[-1]))(channel_avg)\n    channel_avg = Dense(x.shape[-1] // reduction_ratio, activation='relu')(channel_avg)\n    channel_avg = Dense(x.shape[-1], activation='sigmoid')(channel_avg)\n    \n    return Multiply()([x, channel_avg])\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=2, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n# Build the efficient hybrid U-Net with Swin Transformer using Depthwise Convolution\ndef build_efficient_hybrid_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder with Depthwise Convolutions and residual connections\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)\n    conv1 = channel_attention(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    swin1 = SwinTransformerBlock(dim=64, num_heads=4, shift_size=1)(pool1)\n\n    # Add residual connection to pass the feature map forward\n    swin1_residual = Add()([pool1, swin1])\n\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1_residual)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    conv2 = channel_attention(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    swin2 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(pool2)\n\n    # Pass feature map forward with residual connection\n    swin2_residual = Add()([pool2, swin2])\n\n    # Bottleneck with Swin Transformer blocks and residual connections\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2_residual)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n\n    swin3 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(conv3)\n    swin4 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(swin3)\n\n    # Pass feature map forward with residual connection\n    swin4_residual = Add()([conv3, swin4])\n\n    # Decoder with Skip Connections, Depthwise Convolutions, and residual connections\n    up1 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin4_residual)\n    up1 = Concatenate()([up1, conv2])\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv4 = Conv2D(128, 1, activation='relu', padding='same')(conv4)\n    conv4 = channel_attention(conv4)\n\n    swin5 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(conv4)\n\n    # Pass feature map forward with residual connection\n    swin5_residual = Add()([conv4, swin5])\n\n    up2 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin5_residual)\n    up2 = Concatenate()([up2, conv1])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv5 = Conv2D(64, 1, activation='relu', padding='same')(conv5)\n    conv5 = channel_attention(conv5)\n\n    swin6 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=1)(conv5)\n\n    # Final residual connection before output\n    swin6_residual = Add()([conv5, swin6])\n\n    # Output layer with Sigmoid activation\n    outputs = Conv2D(3, 1, activation='sigmoid')(swin6_residual)\n\n    return Model(inputs, outputs)\n\n# Instantiate and compile the model\nmodel = build_efficient_hybrid_unet_swin()\nmodel.summary()\n\n# Using L1 Loss\nmodel.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    win_size = 3  \n    data_range = 1.0  \n    \n    for i in range(predictions.shape[0]):\n        psnr_val = PSNR(ground_truth[i], predictions[i])\n        ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Function to train the model\ndef train_model(model, rainy_images_train, clear_images_train, rainy_images_test, clear_images_test, epochs=5, batch_size=2):\n    # Compile the model using Adam optimizer and L1 Loss\n    model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(\n        rainy_images_train, clear_images_train,\n        validation_data=(rainy_images_test, clear_images_test),\n        epochs=epochs,\n        batch_size=batch_size\n    )\n    \n    # Plot training history (loss and accuracy)\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n    return model, history\n\n# Visualize predictions and ground truth images\ndef visualize_predictions(model, rainy_images_test, clear_images_test, num_samples=5):\n    predictions = model.predict(rainy_images_test[:num_samples])\n\n    for i in range(num_samples):\n        plt.figure(figsize=(15, 5))\n        \n        plt.subplot(1, 3, 1)\n        plt.title('Rainy Image')\n        plt.imshow((rainy_images_test[i] + 1) / 2)  # Scaling back from [-1,1] to [0,1]\n        \n        plt.subplot(1, 3, 2)\n        plt.title('Ground Truth (Clear)')\n        plt.imshow((clear_images_test[i] + 1) / 2)  # Scaling back from [-1,1] to [0,1]\n        \n        plt.subplot(1, 3, 3)\n        plt.title('Predicted Clear Image')\n        plt.imshow((predictions[i] + 1) / 2)  # Scaling back from [-1,1] to [0,1]\n        \n        plt.show()\n\n# Load and preprocess datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Update this path to your dataset location\n(rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Train the model\nmodel, history = train_model(model, rainy_images_train, clear_images_train, rainy_images_test, clear_images_test, epochs=5, batch_size=2)\n\n# Visualize predictions on test data\nvisualize_predictions(model, rainy_images_test, clear_images_test, num_samples=5)\n\n# Calculate PSNR and SSIM metrics for the test dataset\npredictions = model.predict(rainy_images_test)\npsnr, ssim = calculate_metrics(predictions, clear_images_test)\nprint(f\"Average PSNR: {psnr}, Average SSIM: {ssim}\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unet with attenton + skip connection + dropout with swin","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, GlobalAveragePooling2D, Reshape, Multiply\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=10,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.1,\n            zoom_range=[0.9, 1.1],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False, batch_size=1):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Define Channel Attention Mechanism\ndef channel_attention(x, reduction_ratio=16):\n    channel_avg = GlobalAveragePooling2D()(x)\n    channel_avg = Reshape((1, 1, channel_avg.shape[-1]))(channel_avg)\n    channel_avg = Dense(x.shape[-1] // reduction_ratio, activation='relu')(channel_avg)\n    channel_avg = Dense(x.shape[-1], activation='sigmoid')(channel_avg)\n    \n    return Multiply()([x, channel_avg])\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=2, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n# Build the efficient hybrid U-Net with Swin Transformer using Depthwise Convolution\ndef build_efficient_hybrid_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder with Depthwise Convolutions\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)\n    conv1 = channel_attention(conv1)  # Channel attention\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    swin1 = SwinTransformerBlock(dim=64, num_heads=4, shift_size=1)(pool1)\n\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    conv2 = channel_attention(conv2)  # Channel attention\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    swin2 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(pool2)\n\n    # Bottleneck\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n\n    # Third Swin Transformer Block in bottleneck\n    swin3 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(conv3)\n\n    # Decoder with Skip Connections and Depthwise Convolutions\n    up1 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin3)  # Learned upsampling\n    up1 = Concatenate()([up1, conv2])\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv4 = Conv2D(128, 1, activation='relu', padding='same')(conv4)\n    conv4 = channel_attention(conv4)  # Channel attention\n\n    swin4 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(conv4)\n\n    up2 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin4)  # Learned upsampling\n    up2 = Concatenate()([up2, conv1])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv5 = Conv2D(64, 1, activation='relu', padding='same')(conv5)\n    conv5 = channel_attention(conv5)  # Channel attention\n\n    swin5 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=1)(conv5)\n\n    # Output layer with Tanh activation\n    outputs = Conv2D(3, 1, activation='tanh')(swin5)\n\n    return Model(inputs, outputs)\n\n# Instantiate and compile the model\nmodel = build_efficient_hybrid_unet_swin()\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    win_size = 3  \n    data_range = 1.0  \n    \n    for i in range(predictions.shape[0]):\n        psnr_val = PSNR(ground_truth[i], predictions[i])\n        \n        if ground_truth[i].shape[0] < 7 or ground_truth[i].shape[1] < 7:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        else:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Function to train the model\ndef train_model(model, rainy_images_train, clear_images_train, epochs=5, batch_size=1):\n    history = model.fit(rainy_images_train, clear_images_train, \n                        epochs=epochs, \n                        batch_size=batch_size, \n                        validation_split=0.1)\n    return history\n\n# Function to visualize predictions\ndef visualize_predictions(model, rainy_images_test, clear_images_test):\n    predictions = model.predict(rainy_images_test, batch_size = 1)\n    psnr, ssim = calculate_metrics(predictions, clear_images_test)\n\n    print(f'Test PSNR: {psnr:.2f} dB')\n    print(f'Test SSIM: {ssim:.4f}')\n\n    # Plotting results\n    plt.figure(figsize=(15, 5))\n    for i in range(5):\n        plt.subplot(3, 5, i + 1)\n        plt.imshow((rainy_images_test[i] + 1) / 2)  # Rescale back to [0, 1]\n        plt.title('Rainy Image')\n        plt.axis('off')\n\n        plt.subplot(3, 5, i + 6)\n        plt.imshow((clear_images_test[i] + 1) / 2)  # Rescale back to [0, 1]\n        plt.title('Ground Truth')\n        plt.axis('off')\n\n        plt.subplot(3, 5, i + 11)\n        plt.imshow((predictions[i] + 1) / 2)  # Rescale back to [0, 1]\n        plt.title('Predicted Image')\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Load and preprocess the datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Update with the actual path\n(rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Train the model\nhistory = train_model(model, rainy_images_train, clear_images_train, epochs=5, batch_size=1)\n\n# Visualize predictions on test data\nvisualize_predictions(model, rainy_images_test, clear_images_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Updated mix model (unet + swin)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob  # Import glob to handle file loading\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=10,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.1,\n            zoom_range=[0.9, 1.1],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False, batch_size=2):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=2, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        # Apply attention with shift size\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n\n# Build the efficient hybrid U-Net with Swin Transformer using Depthwise Convolution\ndef build_efficient_hybrid_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder with Depthwise Convolutions (Parameter reduction)\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)  # Pointwise convolution\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    # Swin Transformer Block for Global Feature Extraction\n    swin1 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=2)(pool1)\n\n    # Encoder continued with Depthwise Convolutions\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Second Swin Transformer Block\n    swin2 = SwinTransformerBlock(dim=128, num_heads=2, shift_size=2)(pool2)\n\n    # Bottleneck\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n\n    # Third Swin Transformer Block in bottleneck\n    swin3 = SwinTransformerBlock(dim=256, num_heads=2, shift_size=2)(conv3)\n\n    # Decoder with Skip Connections and Depthwise Convolutions\n    up1 = UpSampling2D(size=(2, 2))(swin3)\n    up1 = Concatenate()([up1, conv2])\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv4 = Conv2D(128, 1, activation='relu', padding='same')(conv4)\n\n    # Fourth Swin Transformer Block\n    swin4 = SwinTransformerBlock(dim=128, num_heads=2, shift_size=2)(conv4)\n\n    up2 = UpSampling2D(size=(2, 2))(swin4)\n    up2 = Concatenate()([up2, conv1])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv5 = Conv2D(64, 1, activation='relu', padding='same')(conv5)\n\n    # Fifth Swin Transformer Block\n    swin5 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=2)(conv5)\n\n    # Output layer with Tanh activation\n    outputs = Conv2D(3, 1, activation='tanh')(swin5)\n\n    return Model(inputs, outputs)\n\n# Instantiate and compile the model\nmodel = build_efficient_hybrid_unet_swin()\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    # Define a smaller window size if images are less than 7x7\n    win_size = 3  # Use a smaller window size if necessary\n    \n    # Determine the data range\n    data_range = 1.0  # Assuming images are normalized between [0, 1]\n    \n    for i in range(predictions.shape[0]):\n        psnr_val = PSNR(ground_truth[i], predictions[i])  # Use PSNR instead of psnr\n        \n        # Ensure the window size is appropriate for the image dimensions\n        if ground_truth[i].shape[0] < 7 or ground_truth[i].shape[1] < 7:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)  # Use SSIM instead of ssim\n        else:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n\n# Training function\ndef train_model(model, train_data, train_labels, epochs=1, batch_size=2):\n    history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n    return history\n\n# Load the dataset\nbase_folder = \"/kaggle/input/derainingdata/RainData\"  # Update with your actual dataset path\n(train_data, train_labels), (test_data, test_labels) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Train the model\nhistory = train_model(model, train_data, train_labels, epochs=1)\n\n# Predict and display results\npredictions = model.predict(test_data, batch_size = 1)\npredictions = (predictions + 1) / 2  # Scale predictions back to [0, 1]\n\n# Show prediction vs ground truth\ndef display_predictions(test_data, predictions, ground_truth, index=0):\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.title('Input')\n    plt.imshow((test_data[index] + 1) / 2)  # Scale back to [0, 1]\n    plt.subplot(1, 3, 2)\n    plt.title('Prediction')\n    plt.imshow(predictions[index])\n    plt.subplot(1, 3, 3)\n    plt.title('Ground Truth')\n    plt.imshow((ground_truth[index] + 1) / 2)  # Scale back to [0, 1]\n    plt.show()\n\n# Test and display metrics for an example image\nfor i in range(5):  # Test and display first 5 images\n    display_predictions(test_data, predictions, test_labels, index=i)\n\n # Calculate PSNR and SSIM\nmean_psnr, mean_ssim = calculate_metrics(predictions, test_labels)\nprint(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unet + official swin","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage import io, img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, shift_size):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        \n        # Layers for the transformer block\n        self.attn = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.dim)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        \n        # Feedforward network (FFN)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.dim * 2, activation='gelu'),  # Reduced dimensionality\n            tf.keras.layers.Dense(self.dim)\n        ])\n\n    def call(self, x):\n        attn_output = self.attn(x, x)\n        x = self.layernorm1(x + attn_output)\n        ffn_output = self.ffn(x)\n        return self.layernorm2(x + ffn_output)\n\nclass AggregationLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(AggregationLayer, self).__init__()\n\n    def call(self, skip_connections):\n        return tf.reduce_mean(tf.stack(skip_connections), axis=0)\n\ndef build_swin_transformer(input_shape=(128, 128, 3), num_blocks=3):\n    inputs = Input(shape=input_shape)\n    x = Conv2D(32, kernel_size=3, padding='same')(inputs)  # Reduced channels\n    skip_connections = []  # To store skip connection outputs\n\n    # Using the Swin Transformer block specified number of times\n    for _ in range(num_blocks):\n        x = SwinTransformerBlock(dim=32, num_heads=2, window_size=8, shift_size=0)(x)\n        skip_connections.append(x)\n\n    # Use Aggregation Layer to combine skip connections\n    aggregated_features = AggregationLayer()(skip_connections)\n    \n    # Print shape for debugging\n    print(f\"Shape after aggregation: {aggregated_features.shape}\")  # Debug print\n    \n    # Add a Conv2D layer to reduce channels from 32 to 3\n    x = Conv2D(3, kernel_size=1, padding='same')(aggregated_features)\n    \n    # Print shape for debugging\n    print(f\"Shape before returning model: {x.shape}\")  # Debug print\n    \n    return Model(inputs, x)\n\n# Define the U-Net model\ndef build_unet(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n    \n    # Encoder\n    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)  # Reduced channels\n    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    # Bottleneck\n    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n\n    # Decoder\n    up6 = UpSampling2D(size=(2, 2))(conv5)\n    up6 = Concatenate()([up6, conv4])\n    conv6 = Conv2D(256, 3, activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n\n    up7 = UpSampling2D(size=(2, 2))(conv6)\n    up7 = Concatenate()([up7, conv3])\n    conv7 = Conv2D(128, 3, activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n\n    up8 = UpSampling2D(size=(2, 2))(conv7)\n    up8 = Concatenate()([up8, conv2])\n    conv8 = Conv2D(64, 3, activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n\n    up9 = UpSampling2D(size=(2, 2))(conv8)\n    up9 = Concatenate()([up9, conv1])\n    conv9 = Conv2D(32, 3, activation='relu', padding='same')(up9)  # Reduced channels\n    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n\n    outputs = Conv2D(3, 1, activation='tanh')(conv9)  # Adjust activation if necessary\n    \n    return Model(inputs, outputs)\n\n# Main function\ndef main():\n    # Load datasets\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n    (rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    # Print shapes of the training data\n    print(f\"Rainy train shape: {rainy_train.shape}\")\n    print(f\"Clear train shape: {clear_train.shape}\")\n\n    # Define models\n    swin_model = build_swin_transformer()\n    unet_model = build_unet()\n\n    # Combine models\n    inputs = Input(shape=(128, 128, 3))\n    swin_output = swin_model(inputs)\n    outputs = unet_model(swin_output)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])\n\n    # Model summary\n    model.summary()\n\n    # Define callbacks\n    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n    early_stopping = EarlyStopping(patience=10, monitor='val_loss')\n\n    # Train the model\n    history = model.fit(rainy_train, clear_train,\n                        validation_split=0.1,\n                        epochs=5,\n                        batch_size=1,  # Adjust batch size if necessary\n                        callbacks=[checkpoint, early_stopping])\n\n    # Evaluate the model on test data\n    test_loss, test_accuracy = model.evaluate(rainy_test, clear_test)\n    print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n\n    # Visualize some predictions\n    predictions = model.predict(test_rainy, batch_size = 1)\n    \n    plt.figure(figsize=(15, 5))\n    for i in range(5):\n        plt.subplot(2, 5, i + 1)\n        plt.imshow((rainy_test[i] + 1) / 2)  # Convert back to [0, 1]\n        plt.axis('off')\n        plt.title('Rainy Image')\n\n        plt.subplot(2, 5, i + 6)\n        plt.imshow((predictions[i] + 1) / 2)  # Convert back to [0, 1]\n        plt.axis('off')\n        plt.title('Predicted Image')\n        \n    plt.show()\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unet + window based swin (not working)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG19\nimport os\nimport glob\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer, Conv2D, LayerNormalization, Dense, Dropout, MultiHeadAttention, Input\n\nK.clear_session()\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n\n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Global variable for VGG model\nvgg = VGG19(include_top=False, input_shape=(128, 128, 3))\nvgg.trainable = False\nvgg_feat_extractor = models.Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n\n# Perceptual Loss Function\ndef perceptual_loss(y_true, y_pred):\n    true_features = vgg_feat_extractor(y_true)\n    pred_features = vgg_feat_extractor(y_pred)\n    return tf.reduce_mean(tf.square(true_features - pred_features))\n\n# SSIM Loss\ndef ssim_loss(y_true, y_pred):\n    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n\n# Total Variation Loss (for sharpness)\ndef total_variation_loss(y_pred):\n    return tf.image.total_variation(y_pred)\n\n# Combined loss function to incorporate perceptual, SSIM, and total variation losses\ndef combined_loss(y_true, y_pred):\n    perceptual = perceptual_loss(y_true, y_pred)\n    ssim_loss_val = ssim_loss(y_true, y_pred)\n    tv_loss = total_variation_loss(y_pred)\n\n    # Combine losses with different weights\n    return 0.8 * perceptual + 0.1 * ssim_loss_val + 0.1 * tv_loss\n\n# Define the MultiHeadSelfAttention Layer if not already defined\nclass MultiHeadSelfAttention(Layer):\n    def __init__(self, num_heads, key_dim):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n\n    def call(self, queries, keys):\n        return self.attention(queries, keys)\n\n# Define the MLP Layer if not already defined\nclass MLP(Layer):\n    def __init__(self, dim):\n        super(MLP, self).__init__()\n        self.fc1 = Dense(dim * 4, activation='gelu')\n        self.fc2 = Dense(dim)\n\n    def call(self, x):\n        return self.fc2(Dropout(0.1)(self.fc1(x)))\n\n# Define window partition and reverse functions\ndef window_partition(x, window_size):\n    B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n    x = tf.reshape(x, (B, H // window_size, window_size, W // window_size, window_size, C))\n    windows = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n    return tf.reshape(windows, (-1, window_size, window_size, C)), H, W, 0, 0  # Padding values as 0 for now\n\ndef window_reverse(windows, H_padded, W_padded, pad_h, pad_w, window_size):\n    B = tf.shape(windows)[0]\n    x = tf.reshape(windows, (B, H_padded // window_size, W_padded // window_size, window_size, window_size, -1))\n    x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n    return tf.reshape(x, (B, H_padded, W_padded, -1))[:, :H_padded - pad_h, :W_padded - pad_w, :]\n\n# Define the SwinTransformerBlock\nclass SwinTransformerBlock(Layer):\n    def __init__(self, dim, num_heads, window_size):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        \n        self.attn = MultiHeadSelfAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        self.mlp = MLP(dim)\n\n    def build(self, input_shape):\n        # Build the attention layer to create variables based on input shape\n        self.attn.build(input_shape)\n        super(SwinTransformerBlock, self).build(input_shape)\n\n    def call(self, x):\n        # Print input shape for debugging\n        print(\"Input shape before window partition:\", x.shape)\n\n        windows, H_padded, W_padded, pad_h, pad_w = window_partition(x, self.window_size)\n\n        # Print shape after window partition\n        print(\"Shape after window partitioning:\", windows.shape)\n\n        # Ensure windows shape is correct before applying attention\n        if tf.shape(windows)[2] is None or tf.shape(windows)[3] is None:\n            raise ValueError(\"Window dimensions must be defined\")\n\n        # Apply attention\n        windows_attn = self.attn(windows, windows)\n        windows_attn = self.norm1(windows_attn + windows)  # Add & Norm\n\n        # Print shape after attention\n        print(\"Shape after attention:\", windows_attn.shape)\n\n        # Reverse window partition\n        x = window_reverse(windows_attn, H_padded, W_padded, pad_h, pad_w, self.window_size)\n\n        # Print shape after window reversing\n        print(\"Shape after window reversing:\", x.shape)\n\n        x = self.norm2(x)  # Add & Norm\n        return x\n\n# Example U-Net + Swin Transformer Hybrid Model\ndef build_hybrid_model(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Initial Conv Layer\n    x = Conv2D(64, (3, 3), padding='same')(inputs)\n    print(f\"After initial Conv, shape: {x.shape}\")\n\n    # Add 12 Swin Transformer blocks with alternating window sizes\n    for i in range(12):\n        window_size = 8 if i % 2 == 0 else 7  # Alternate window size\n        x = SwinTransformerBlock(dim=64, num_heads=4, window_size=window_size)(x)\n        print(f\"After Swin Transformer Block {i + 1}, shape: {x.shape}\")  # Debugging output\n\n\n    # U-Net Encoder\n    encoder_output = []\n    for filters in [64, 128, 256]:\n        x = layers.Conv2D(filters, kernel_size=3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        encoder_output.append(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        print(f\"After Encoder Conv, shape: {x.shape}\")  # Debugging output\n\n    # Bottleneck\n    x = layers.Conv2D(512, kernel_size=3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    print(f\"After Bottleneck, shape: {x.shape}\")  # Debugging output\n\n    # U-Net Decoder\n    for filters in [256, 128, 64]:\n        x = layers.Conv2DTranspose(filters, kernel_size=2, strides=2, padding='same')(x)\n        x = layers.concatenate([x, encoder_output.pop()])\n        x = layers.Conv2D(filters, kernel_size=3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        print(f\"After Decoder Conv, shape: {x.shape}\")  # Debugging output\n\n    outputs = layers.Conv2D(3, kernel_size=1, activation='sigmoid')(x)  # Output layer\n    model = models.Model(inputs, outputs)\n    return model\n\n# Load datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Set your dataset path here\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Build model\nmodel = build_hybrid_model()\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss=combined_loss, metrics=[psnr])\n\n# Train the model\nhistory = model.fit(train_rainy, train_clear, validation_data=(test_rainy, test_clear), epochs=50, batch_size=8)\n\n# Evaluate PSNR and SSIM metrics\ndef evaluate_model(model, test_rainy, test_clear):\n    predictions = model.predict(test_rainy)\n    psnr_values = []\n    ssim_values = []\n    \n    for pred, true in zip(predictions, test_clear):\n        psnr_val = psnr(true, pred)\n        ssim_val = ssim(true, pred, multichannel=True)\n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n\n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Evaluate the model\naverage_psnr, average_ssim = evaluate_model(model, test_rainy, test_clear)\nprint(f'Average PSNR: {average_psnr:.2f} dB, Average SSIM: {average_ssim:.4f}')\n\n# Show output images\ndef show_images(original, predicted, ground_truth, num=5):\n    plt.figure(figsize=(15, 5))\n    for i in range(num):\n        plt.subplot(3, num, i + 1)\n        plt.imshow(original[i] * 0.5 + 0.5)  # Rescale to [0, 1]\n        plt.title('Original (Rainy)')\n        plt.axis('off')\n        \n        plt.subplot(3, num, i + 1 + num)\n        plt.imshow(predicted[i] * 0.5 + 0.5)\n        plt.title('Predicted')\n        plt.axis('off')\n\n        plt.subplot(3, num, i + 1 + 2 * num)\n        plt.imshow(ground_truth[i] * 0.5 + 0.5)\n        plt.title('Ground Truth')\n        plt.axis('off')\n\n    plt.show()\n\n# Display output images\npredicted_images = model.predict(test_rainy)\nshow_images(test_rainy, predicted_images, test_clear)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More Revised Unet + swin","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG19\nimport os\nimport glob\nfrom tensorflow.keras import backend as K\n\nK.clear_session()\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # , 'Rain200H' Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Global variable for VGG model\nvgg = VGG19(include_top=False, input_shape=(128, 128, 3))\nvgg.trainable = False\nvgg_feat_extractor = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n\n# Perceptual Loss Function\ndef perceptual_loss(y_true, y_pred):\n    true_features = vgg_feat_extractor(y_true)\n    pred_features = vgg_feat_extractor(y_pred)\n    return tf.reduce_mean(tf.square(true_features - pred_features))\n\n# SSIM Loss\ndef ssim_loss(y_true, y_pred):\n    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n\n# Total Variation Loss (for sharpness)\ndef total_variation_loss(y_pred):\n    return tf.image.total_variation(y_pred)\n\n# Combined loss function to incorporate perceptual, SSIM, and total variation losses\ndef combined_loss(y_true, y_pred):\n    perceptual = perceptual_loss(y_true, y_pred)\n    ssim_loss_val = ssim_loss(y_true, y_pred)\n    tv_loss = total_variation_loss(y_pred)\n    \n    # Combine losses with different weights\n    return 0.8 * perceptual + 0.1 * ssim_loss_val + 0.1 * tv_loss\n\n\n# Swin Transformer block implementation\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim, dropout=dropout)\n\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='relu'),\n            layers.Dense(dim)\n        ])\n\n    def call(self, x):\n        h = self.norm1(x)\n        attn_output = self.attn(h, h)\n        x = x + attn_output\n        h = self.norm2(x)\n        x = x + self.mlp(h)\n        return x\n\n# Swin Transformer Layer\nclass SwinTransformerLayer(layers.Layer):\n    def __init__(self, dim, depth, num_heads, mlp_ratio=4., dropout=0.1):\n        super(SwinTransformerLayer, self).__init__()\n        self.blocks = [SwinTransformerBlock(dim, num_heads, mlp_ratio, dropout) for _ in range(depth)]\n\n    def call(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n# Swin Transformer Block (with variable depths)\ndef swin_transformer_block(input_tensor, num_heads, embed_dim, depth):\n    height, width = input_tensor.shape[1], input_tensor.shape[2]  \n    embed_layer = layers.Conv2D(embed_dim, kernel_size=4, strides=2, padding='same')(input_tensor)\n    reshaped_layer = layers.Reshape((height // 2 * width // 2, embed_dim))(embed_layer)\n\n    swin_layer = SwinTransformerLayer(embed_dim, depth, num_heads)\n    transformed_layer = swin_layer(reshaped_layer)\n\n    output_layer = layers.Reshape((height // 2, width // 2, embed_dim))(transformed_layer)\n    return output_layer\n\n# U-Net Encoder Block\ndef unet_encoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    return x\n\n# U-Net Decoder Block\ndef unet_decoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    return x\n\n# Hybrid U-Net + Swin Transformer Model\ndef unet_swin_transformer(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder\n    enc1 = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n    swin1 = swin_transformer_block(enc1, num_heads=4, embed_dim=64, depth=6)  # Swin1: 4 times\n    pool1 = layers.Conv2D(128, (3, 3), strides=2, padding='same')(swin1)\n\n    enc2 = unet_encoder_block(pool1, 128)\n    swin2 = swin_transformer_block(enc2, num_heads=4, embed_dim=128, depth=4)  # Swin2: 2 times\n    pool2 = layers.Conv2D(256, (3, 3), strides=2, padding='same')(swin2)\n\n    enc3 = unet_encoder_block(pool2, 256)\n    swin3 = swin_transformer_block(enc3, num_heads=4, embed_dim=256, depth=2)  # Swin3: 1 time\n    pool3 = layers.Conv2D(512, (3, 3), strides=2, padding='same')(swin3)\n\n    # Bottleneck\n    bottleneck = unet_encoder_block(pool3, 512)\n\n    # Decoder with Skip Connections\n    dec3 = unet_decoder_block(bottleneck, 256)\n    dec3 = layers.UpSampling2D(size=(2, 2))(dec3)  # Upsample to match swin3 (16x16)\n    dec3 = layers.Concatenate()([dec3, swin3])\n\n    dec2 = unet_decoder_block(dec3, 128)\n    dec2 = layers.UpSampling2D(size=(2, 2))(dec2)  # Upsample to match swin2 (32x32)\n    dec2 = layers.UpSampling2D(size=(2, 2))(dec2)\n    dec2 = layers.Concatenate()([dec2, swin2])\n\n    dec1 = unet_decoder_block(dec2, 64)\n    dec1 = layers.UpSampling2D(size=(2, 2))(dec1)  # Upsample to match swin1 (64x64)\n    dec1 = layers.UpSampling2D(size=(2, 2))(dec1)\n    dec1 = layers.Concatenate()([dec1, swin1])\n    \n    print(\"Shape of dec1:\", dec1.shape)\n    \n    outputs = layers.UpSampling2D(size=(2, 2))(dec1)  # Upsample from (64, 64) to (128, 128)\n    outputs = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(outputs)  # Optional Conv2D for refinement\n\n    outputs = layers.Conv2D(3, (1, 1), activation='tanh')(outputs)\n    print(\"Shape of outputs:\", outputs.shape)\n    \n    return Model(inputs, outputs)\n\n\n# Compile the model\ninput_shape = (128, 128, 3)\nmodel = unet_swin_transformer(input_shape)\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss=combined_loss, metrics=['mae'])\n\n# Model summary to check the layer dimensions\nmodel.summary()\n\n# Loading datasets\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets('/kaggle/input/derainingdata/RainData', augment=True)\n\n# Train the model\nmodel.fit(train_rainy, train_clear, batch_size=2, epochs=1, validation_data=(test_rainy, test_clear))\n\n# Evaluate model performance on testing set\npredictions = model.predict(test_rainy, batch_size = 1)\nfor i in range(3):  # Display 3 examples\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow((test_rainy[i] + 1) / 2)  # Convert back to [0, 1]\n    plt.title(\"Input\")\n    plt.subplot(1, 3, 2)\n    plt.imshow((predictions[i] + 1) / 2)  # Convert back to [0, 1]\n    plt.title(\"Prediction\")\n    plt.subplot(1, 3, 3)\n    plt.imshow((test_clear[i] + 1) / 2)  # Convert back to [0, 1]\n    plt.title(\"Ground Truth\")\n    plt.show()\n\n# Calculate PSNR and SSIM metrics for the evaluation\nfor i in range(len(test_rainy)):\n    pred_img = (predictions[i] + 1) / 2  # Convert to [0, 1]\n    gt_img = (test_clear[i] + 1) / 2  # Convert to [0, 1]\n    \n    psnr_value = psnr(gt_img, pred_img)\n    ssim_value = ssim(gt_img, pred_img, multichannel=True)\n    print(f\"Image {i+1}: PSNR = {psnr_value}, SSIM = {ssim_value}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# Revised Unet + swin ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport glob\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)  # Convert image to float in [0, 1]\n        img_resized = tf.image.resize(img, size).numpy()\n        # Remove normalization if img_as_float is already in [0, 1]\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Swin Transformer block implementation\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n\n        # Layer normalization\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        # Multi-head self-attention layer\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim, dropout=dropout)\n\n        # MLP layer\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='relu'),\n            layers.Dense(dim)\n        ])\n\n    def call(self, x):\n        h = self.norm1(x)  # Apply normalization for attention\n        attn_output = self.attn(h, h)  # Self-attention\n\n        # Residual connection\n        x = x + attn_output\n\n        h = self.norm2(x)  # Apply normalization for MLP\n        x = x + self.mlp(h)  # Residual connection for MLP\n        return x\n\n# Window-based Swin Transformer Layer (basic building block)\nclass SwinTransformerLayer(layers.Layer):\n    def __init__(self, dim, depth, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerLayer, self).__init__()\n        self.blocks = [SwinTransformerBlock(dim, num_heads, mlp_ratio, dropout) for _ in range(depth)]\n\n    def call(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n# Window-based Swin Transformer block applied after each downsampling\ndef swin_transformer_block(input_tensor, num_heads, embed_dim, depths):\n    height, width = input_tensor.shape[1], input_tensor.shape[2]  # Keras tensor shape\n    embed_layer = layers.Conv2D(embed_dim, kernel_size=4, strides=2, padding='same')(input_tensor)  # Reduce spatial size\n    print(f\"After embedding: {embed_layer.shape}\")  # Print dimension after embedding\n\n    # Reshape for Swin Transformer: Batch, Height * Width, Channels\n    reshaped_layer = layers.Reshape((height // 2 * width // 2, embed_dim))(embed_layer)\n\n    # Apply Swin Transformer layers (recursive depth)\n    swin_layer = SwinTransformerLayer(embed_dim, depths[0], num_heads)\n    transformed_layer = swin_layer(reshaped_layer)\n\n    # Reshape back to original spatial dimensions\n    output_layer = layers.Reshape((height // 2, width // 2, embed_dim))(transformed_layer)\n    print(f\"Output of Swin block: {output_layer.shape}\")  # Print dimension after Swin Transformer block\n\n    return output_layer\n\n# U-Net Encoder Block\ndef unet_encoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    print(f\"After encoder block with {filters} filters: {x.shape}\")  # Print dimension after encoder block\n    return x\n\n# U-Net Decoder Block\ndef unet_decoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    print(f\"After decoder block with {filters} filters: {x.shape}\")  # Print dimension after decoder block\n    return x\n\n# Hybrid U-Net + Swin Transformer Model\ndef unet_swin_transformer(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder (with multi-scale feature extraction)\n    enc1 = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n    swin1 = swin_transformer_block(enc1, num_heads=4, embed_dim=64, depths=[2])\n    pool1 = layers.Conv2D(128, (3, 3), strides=2, padding='same')(swin1)\n    print(f\"After first pooling: {pool1.shape}\")  # Print dimension after first pooling\n\n    enc2 = unet_encoder_block(pool1, 128)\n    swin2 = swin_transformer_block(enc2, num_heads=4, embed_dim=128, depths=[2])\n    pool2 = layers.Conv2D(256, (3, 3), strides=2, padding='same')(swin2)\n    print(f\"After second pooling: {pool2.shape}\")  # Print dimension after second pooling\n\n    enc3 = unet_encoder_block(pool2, 256)\n    swin3 = swin_transformer_block(enc3, num_heads=4, embed_dim=256, depths=[2])\n    pool3 = layers.Conv2D(512, (3, 3), strides=2, padding='same')(swin3)\n    print(f\"After third pooling: {pool3.shape}\")  # Print dimension after third pooling\n\n    # Bottleneck\n    bottleneck = unet_encoder_block(pool3, 512)\n    print(f\"After bottleneck: {bottleneck.shape}\")  # Print dimension after bottleneck\n\n    # Decoder\n    dec3 = unet_decoder_block(bottleneck, 256)\n    dec3 = layers.UpSampling2D(size=(2, 2))(dec3)  # Upsample to match swin3\n    print(f\"After upsampling dec3 to match swin3: {dec3.shape}\")  # Print upsampled dimension\n    dec3 = layers.Concatenate()([dec3, swin3])  # Concatenate with skip connection from swin3\n    print(f\"After concatenation with skip connection from swin3: {dec3.shape}\")  # Print dimension after concatenation\n\n    dec2 = unet_decoder_block(dec3, 128)\n    dec2 = layers.UpSampling2D(size=(2, 2))(dec2)  # Upsample to match swin2\n    print(f\"After upsampling dec2 to match swin2: {dec2.shape}\")  # Print upsampled dimension\n\n    # Downsample swin2 to match dec2's shape\n    swin2_downsampled = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(swin2)\n    print(f\"After downsampling swin2: {swin2_downsampled.shape}\")  # Print downsampled swin2 shape\n    dec2 = layers.Concatenate()([dec2, swin2_downsampled])  # Concatenate with skip connection from downsampled swin2\n    print(f\"After concatenation with skip connection from swin2: {dec2.shape}\")  # Print dimension after concatenation\n\n    dec1 = unet_decoder_block(dec2, 64)\n    dec1 = layers.UpSampling2D(size=(2, 2))(dec1)  # Upsample to match swin1\n    print(f\"After upsampling dec1 to match swin1: {dec1.shape}\")  # Print upsampled dimension\n\n    # Upsample swin1 to match dec1's shape\n    swin1_downsampled = layers.Conv2D(64, kernel_size=3, strides=(4, 4), padding='same')(swin1)\n    #swin1_upsampled = layers.UpSampling2D(size=(2, 2))(swin1_upsampled)  # Ensure the dimensions match after upsampling\n    print(f\"After upsampling swin1: {swin1_downsampled.shape}\")  # Print upsampled swin1 shape\n    dec1 = layers.Concatenate()([dec1, swin1_downsampled])  # Concatenate with skip connection from upsampled swin1\n    print(f\"After concatenation with skip connection from swin1: {dec1.shape}\")  # Print dimension after concatenation\n    \n    upsampled = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same')(dec1)\n    upsampled = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same')(upsampled)\n    upsampled = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same')(upsampled)\n\n\n    # Output layer (assuming 3-channel RGB output)\n    outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(upsampled)\n    print(f\"Output layer shape: {outputs.shape}\")  # Print output shape\n\n    return Model(inputs, outputs)\n\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Replace with your dataset path\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Instantiate and compile model\ninput_shape = (128, 128, 3)  # Assuming input images are RGB\nmodel = unet_swin_transformer(input_shape)\nmodel.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])\n\n# Model summary\nmodel.summary()\n\n# Training\nhistory = model.fit(train_rainy, train_clear, validation_split=0.1, epochs=20, batch_size=4)\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    # Define a smaller window size if images are less than 7x7\n    win_size = 3  # Use a smaller window size if necessary\n    \n    # Determine the data range\n    data_range = 1.0  # Assuming images are normalized between [0, 1]\n    \n    for i in range(predictions.shape[0]):\n        psnr_val = psnr(ground_truth[i], predictions[i])\n        \n        # Ensure the window size is appropriate for the image dimensions\n        if ground_truth[i].shape[0] < 7 or ground_truth[i].shape[1] < 7:\n            ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, win_size=3, data_range=data_range)\n        else:\n            ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Evaluation\ntry:\n    predictions = model.predict(test_rainy, batch_size=1)  # Use smaller batch size to avoid memory issues\n\n    # Display predictions\n    for i in range(min(5, predictions.shape[0])):\n        plt.subplot(1, 3, 1)\n        plt.imshow(test_rainy[i])\n        plt.title('Rainy Image')\n        plt.axis('off')\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(predictions[i])\n        plt.title('Predicted Image')\n        plt.axis('off')\n\n        plt.subplot(1, 3, 3)\n        plt.imshow(test_clear[i])\n        plt.title('Ground Truth Image')\n        plt.axis('off')\n\n        plt.show()\n\n    # Calculate PSNR and SSIM\n    mean_psnr, mean_ssim = calculate_metrics(predictions, test_clear)\n    print(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")\n\nexcept Exception as e:\n    print(\"Error during evaluation:\", e)\n\n\n\n'''# Evaluation\npredictions = model.predict(test_rainy)\n\n# Display predictions\nfor i in range(min(5, predictions.shape[0])):\n    plt.subplot(1, 3, 1)\n    plt.imshow(test_rainy[i])\n    plt.title('Rainy Image')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(predictions[i])\n    plt.title('Predicted Image')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(test_clear[i])\n    plt.title('Ground Truth Image')\n    plt.axis('off')\n\n    plt.show()\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    for i in range(predictions.shape[0]):\n        psnr_val = psnr(ground_truth[i], predictions[i])\n        ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True)\n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    return np.mean(psnr_values), np.mean(ssim_values)\n\nmean_psnr, mean_ssim = calculate_metrics(predictions, test_clear)\nprint(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")'''\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNet + swin","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport glob\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Swin Transformer block implementation\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n\n        # Layer normalization\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        # Multi-head self-attention layer\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim, dropout=dropout)\n\n        # MLP layer\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='relu'),\n            layers.Dense(dim)\n        ])\n\n    def call(self, x):\n        h = self.norm1(x)  # Apply normalization for attention\n        attn_output = self.attn(h, h)  # Self-attention\n\n        # Residual connection\n        x = x + attn_output\n\n        h = self.norm2(x)  # Apply normalization for MLP\n        x = x + self.mlp(h)  # Residual connection for MLP\n        return x\n\n# Window-based Swin Transformer Layer (basic building block)\nclass SwinTransformerLayer(layers.Layer):\n    def __init__(self, dim, depth, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerLayer, self).__init__()\n        self.blocks = [SwinTransformerBlock(dim, num_heads, mlp_ratio, dropout) for _ in range(depth)]\n\n    def call(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n# Swin transformer block applied after each downsampling\ndef swin_transformer_block(input_tensor, num_heads, embed_dim, depths):\n    # Patch partition (flatten input image into non-overlapping patches)\n    height, width = input_tensor.shape[1], input_tensor.shape[2]  # Keras tensor shape\n    embed_layer = layers.Conv2D(embed_dim, kernel_size=4, strides=2, padding='same')(input_tensor)  # Reduce spatial size\n\n    # Reshape for Swin Transformer: Batch, Height * Width, Channels\n    reshaped_layer = layers.Reshape((height // 2 * width // 2, embed_dim))(embed_layer)\n\n    # Apply Swin Transformer layers (recursive depth)\n    swin_layer = SwinTransformerLayer(embed_dim, depths[0], num_heads)\n    transformed_layer = swin_layer(reshaped_layer)\n\n    # Reshape back to original spatial dimensions\n    output_layer = layers.Reshape((height // 2, width // 2, embed_dim))(transformed_layer)\n\n    return output_layer\n\n# U-Net Encoder Block (with Strided Convolutions for downsampling)\ndef unet_encoder_block(input_tensor, filters, kernel_size=3, padding='same', strides=1):\n    x = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(input_tensor)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x\n\n# U-Net Decoder Block\ndef unet_decoder_block(input_tensor, skip_tensor, filters, kernel_size=3, padding='same', strides=1):\n    x = layers.Conv2DTranspose(filters, kernel_size, padding=padding, strides=2)(input_tensor)\n    # Ensure dimensions are correct before concatenation\n    if skip_tensor.shape[1] != x.shape[1] or skip_tensor.shape[2] != x.shape[2]:\n        x = layers.Resizing(skip_tensor.shape[1], skip_tensor.shape[2])(x)  # Resize if needed\n    print(f\"Decoding: {x.shape}, Skip: {skip_tensor.shape}\")\n    x = layers.Concatenate()([x, skip_tensor])\n    x = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x\n\n# Hybrid U-Net + Swin Transformer Model\ndef unet_swin_transformer(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # U-Net Encoder with Strided Convolutions and Swin Transformer at each downsampling step\n    enc1 = unet_encoder_block(inputs, 64)\n    swin1 = swin_transformer_block(enc1, num_heads=4, embed_dim=64, depths=[2])\n    pool1 = layers.Conv2D(128, (3, 3), strides=2, padding='same')(swin1)  # Strided convolution for downsampling\n\n    enc2 = unet_encoder_block(pool1, 128)\n    swin2 = swin_transformer_block(enc2, num_heads=4, embed_dim=128, depths=[2])\n    pool2 = layers.Conv2D(256, (3, 3), strides=2, padding='same')(swin2)  # Strided convolution for downsampling\n\n    enc3 = unet_encoder_block(pool2, 256)\n    swin3 = swin_transformer_block(enc3, num_heads=4, embed_dim=256, depths=[2])\n    pool3 = layers.Conv2D(512, (3, 3), strides=2, padding='same')(swin3)  # Strided convolution for downsampling\n\n    enc4 = unet_encoder_block(pool3, 512)\n    swin4 = swin_transformer_block(enc4, num_heads=4, embed_dim=512, depths=[2])\n\n    # Bottleneck\n    bottleneck = layers.Conv2D(1024, (3, 3), strides=2, padding='same')(swin4)\n\n    # Decoder with Skip Connections\n    dec4 = unet_decoder_block(bottleneck, enc4, 512)\n    dec3 = unet_decoder_block(dec4, enc3, 256)\n    dec2 = unet_decoder_block(dec3, enc2, 128)\n    dec1 = unet_decoder_block(dec2, enc1, 64)\n\n    # Final output layer\n    outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(dec1)\n\n    # Model definition\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Instantiate and compile the model\ninput_shape = (128, 128, 3)  # Input image dimensions\nmodel = unet_swin_transformer(input_shape)\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n\n# Print model summary\nmodel.summary()\n# Function to calculate PSNR and SSIM\ndef calculate_metrics(original, generated):\n    psnr_value = psnr(original, generated)\n    ssim_value = ssim(original, generated, multichannel=True)\n    return psnr_value, ssim_value\n\n# Training function\ndef train_model(model, train_data, epochs=100, batch_size=16):\n    rainy_images, clear_images = train_data\n    model.fit(rainy_images, clear_images, epochs=epochs, batch_size=batch_size)\n\n# Evaluation function on the test set\ndef evaluate_model(model, test_data):\n    rainy_images_test, clear_images_test = test_data\n    predictions = model.predict(rainy_images_test)\n\n    # Calculate metrics for each image\n    psnr_values = []\n    ssim_values = []\n    for i in range(len(predictions)):\n        psnr_value, ssim_value = calculate_metrics(clear_images_test[i], predictions[i])\n        psnr_values.append(psnr_value)\n        ssim_values.append(ssim_value)\n\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    return avg_psnr, avg_ssim\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_data, test_data) = load_and_preprocess_datasets(base_folder, augment=True)\ntrain_model(model, train_data, epochs=10, batch_size=16)\navg_psnr, avg_ssim = evaluate_model(model, test_data)\n\nprint(f'Average PSNR: {avg_psnr}, Average SSIM: {avg_ssim}')\n\n# Display predicted images (for the first 5 test images)\npredictions = model.predict(test_data[0])\nfor i in range(5):\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(test_data[0][i])  # Display rainy image\n    plt.axis('off')\n    plt.subplot(3, 5, i + 6)\n    plt.imshow(predictions[i])  # Display predicted clear image\n    plt.axis('off')\n    plt.subplot(3, 5, i + 11)\n    plt.imshow(test_data[1][i])  # Display ground truth image\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []  # Corrected initialization\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\nprint(f\"Training Rainy Images Shape: {train_rainy.shape}\")\nprint(f\"Training Clear Images Shape: {train_clear.shape}\")\nprint(f\"Testing Rainy Images Shape: {test_rainy.shape}\")\nprint(f\"Testing Clear Images Shape: {test_clear.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pyramid Model","metadata":{}},{"cell_type":"code","source":"# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        \n        # Use Depthwise Separable Convolutions\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)  # Pointwise Conv\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Swin transformer for image Restoration","metadata":{}},{"cell_type":"code","source":"# -----------------------------------------------------------------------------------\n# SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257\n# Originally Written by Ze Liu, Modified by Jingyun Liang.\n# -----------------------------------------------------------------------------------\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n    def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n        # qkv = self.qkv(x)\n        flops += N * self.dim * 3 * self.dim\n        # attn = (q @ k.transpose(-2, -1))\n        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n        #  x = (attn @ v)\n        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            attn_mask = self.calculate_mask(self.input_resolution)\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def calculate_mask(self, x_size):\n        # calculate attention mask for SW-MSA\n        H, W = x_size\n        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n        h_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        w_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n\n        return attn_mask\n\n    def forward(self, x, x_size):\n        H, W = x_size\n        B, L, C = x.shape\n        # assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n        if self.input_resolution == x_size:\n            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n        else:\n            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        # W-MSA/SW-MSA\n        nW = H * W / self.window_size / self.window_size\n        flops += nW * self.attn.flops(self.window_size * self.window_size)\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = H * W * self.dim\n        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n        return flops\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x, x_size):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, x_size)\n            else:\n                x = blk(x, x_size)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        if self.downsample is not None:\n            flops += self.downsample.flops()\n        return flops\n\n\nclass RSTB(nn.Module):\n    \"\"\"Residual Swin Transformer Block (RSTB).\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        img_size: Input image size.\n        patch_size: Patch size.\n        resi_connection: The convolutional block before residual connection.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n                 img_size=224, patch_size=4, resi_connection='1conv'):\n        super(RSTB, self).__init__()\n\n        self.dim = dim\n        self.input_resolution = input_resolution\n\n        self.residual_group = BasicLayer(dim=dim,\n                                         input_resolution=input_resolution,\n                                         depth=depth,\n                                         num_heads=num_heads,\n                                         window_size=window_size,\n                                         mlp_ratio=mlp_ratio,\n                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                         drop=drop, attn_drop=attn_drop,\n                                         drop_path=drop_path,\n                                         norm_layer=norm_layer,\n                                         downsample=downsample,\n                                         use_checkpoint=use_checkpoint)\n\n        if resi_connection == '1conv':\n            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n        elif resi_connection == '3conv':\n            # to save parameters and memory\n            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n            norm_layer=None)\n\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n            norm_layer=None)\n\n    def forward(self, x, x_size):\n        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n\n    def flops(self):\n        flops = 0\n        flops += self.residual_group.flops()\n        H, W = self.input_resolution\n        flops += H * W * self.dim * self.dim * 9\n        flops += self.patch_embed.flops()\n        flops += self.patch_unembed.flops()\n\n        return flops\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        flops = 0\n        H, W = self.img_size\n        if self.norm is not None:\n            flops += H * W * self.embed_dim\n        return flops\n\n\nclass PatchUnEmbed(nn.Module):\n    r\"\"\" Image to Patch Unembedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n    def forward(self, x, x_size):\n        B, HW, C = x.shape\n        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n        return x\n\n    def flops(self):\n        flops = 0\n        return flops\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n        super(Upsample, self).__init__(*m)\n\n\nclass UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n\n    \"\"\"\n\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution\n        m = []\n        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n        m.append(nn.PixelShuffle(scale))\n        super(UpsampleOneStep, self).__init__(*m)\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = H * W * self.num_feat * 3 * 9\n        return flops\n\n\nclass SwinIR(nn.Module):\n    r\"\"\" SwinIR\n        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n        img_range: Image range. 1. or 255.\n        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n    \"\"\"\n\n    def __init__(self, img_size=64, patch_size=1, in_chans=3,\n                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],\n                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n                 use_checkpoint=False, upscale=2, img_range=1., upsampler='', resi_connection='1conv',\n                 **kwargs):\n        super(SwinIR, self).__init__()\n        num_in_ch = in_chans\n        num_out_ch = in_chans\n        num_feat = 64\n        self.img_range = img_range\n        if in_chans == 3:\n            rgb_mean = (0.4488, 0.4371, 0.4040)\n            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n        else:\n            self.mean = torch.zeros(1, 1, 1, 1)\n        self.upscale = upscale\n        self.upsampler = upsampler\n        self.window_size = window_size\n\n        #####################################################################################################\n        ################################### 1, shallow feature extraction ###################################\n        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n\n        #####################################################################################################\n        ################################### 2, deep feature extraction ######################################\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = embed_dim\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # merge non-overlapping patches into image\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n            trunc_normal_(self.absolute_pos_embed, std=.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build Residual Swin Transformer blocks (RSTB)\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = RSTB(dim=embed_dim,\n                         input_resolution=(patches_resolution[0],\n                                           patches_resolution[1]),\n                         depth=depths[i_layer],\n                         num_heads=num_heads[i_layer],\n                         window_size=window_size,\n                         mlp_ratio=self.mlp_ratio,\n                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n                         drop=drop_rate, attn_drop=attn_drop_rate,\n                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n                         norm_layer=norm_layer,\n                         downsample=None,\n                         use_checkpoint=use_checkpoint,\n                         img_size=img_size,\n                         patch_size=patch_size,\n                         resi_connection=resi_connection\n\n                         )\n            self.layers.append(layer)\n        self.norm = norm_layer(self.num_features)\n\n        # build the last conv layer in deep feature extraction\n        if resi_connection == '1conv':\n            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n        elif resi_connection == '3conv':\n            # to save parameters and memory\n            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n\n        #####################################################################################################\n        ################################ 3, high quality image reconstruction ################################\n        if self.upsampler == 'pixelshuffle':\n            # for classical SR\n            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n                                                      nn.LeakyReLU(inplace=True))\n            self.upsample = Upsample(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n        elif self.upsampler == 'pixelshuffledirect':\n            # for lightweight SR (to save parameters)\n            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n                                            (patches_resolution[0], patches_resolution[1]))\n        elif self.upsampler == 'nearest+conv':\n            # for real-world SR (less artifacts)\n            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n                                                      nn.LeakyReLU(inplace=True))\n            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            if self.upscale == 4:\n                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'absolute_pos_embed'}\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {'relative_position_bias_table'}\n\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n        return x\n\n    def forward_features(self, x):\n        x_size = (x.shape[2], x.shape[3])\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x, x_size)\n\n        x = self.norm(x)  # B L C\n        x = self.patch_unembed(x, x_size)\n\n        return x\n\n    def forward(self, x):\n        H, W = x.shape[2:]\n        x = self.check_image_size(x)\n        \n        self.mean = self.mean.type_as(x)\n        x = (x - self.mean) * self.img_range\n\n        if self.upsampler == 'pixelshuffle':\n            # for classical SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.conv_last(self.upsample(x))\n        elif self.upsampler == 'pixelshuffledirect':\n            # for lightweight SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.upsample(x)\n        elif self.upsampler == 'nearest+conv':\n            # for real-world SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n            if self.upscale == 4:\n                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            x_first = self.conv_first(x)\n            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n            x = x + self.conv_last(res)\n\n        x = x / self.img_range + self.mean\n\n        return x[:, :, :H*self.upscale, :W*self.upscale]\n\n    def flops(self):\n        flops = 0\n        H, W = self.patches_resolution\n        flops += H * W * 3 * self.embed_dim * 9\n        flops += self.patch_embed.flops()\n        for i, layer in enumerate(self.layers):\n            flops += layer.flops()\n        flops += H * W * 3 * self.embed_dim * self.embed_dim\n        flops += self.upsample.flops()\n        return flops\n\n\nif __name__ == '__main__':\n    upscale = 4\n    window_size = 8\n    height = (1024 // upscale // window_size + 1) * window_size\n    width = (720 // upscale // window_size + 1) * window_size\n    model = SwinIR(upscale=2, img_size=(height, width),\n                   window_size=window_size, img_range=1., depths=[6, 6, 6, 6],\n                   embed_dim=60, num_heads=[6, 6, 6, 6], mlp_ratio=2, upsampler='pixelshuffledirect')\n    print(model)\n    print(height, width, model.flops() / 1e9)\n\n    x = torch.randn((1, 3, height, width))\n    x = model(x)\n    print(x.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(inputs, num_classes=3):\n    # Normalize the input features\n    x = LayerNormalization()(inputs)\n    \n    # Using Depthwise Separable Convolutions\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n    \n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compile Model ","metadata":{}},{"cell_type":"code","source":"# Full Model Creation\ndef build_full_model(input_shape):\n    # Input Layer\n    inputs = Input(shape=input_shape)\n\n    # Dynamic Pyramid Model\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n\n    # Image Restoration and Enhancement\n    outputs = image_restoration_and_enhancement(pyramid_features, num_classes=input_shape[-1])\n\n    return Model(inputs=inputs, outputs=outputs)\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Show some example images to verify data loading\nplt.imshow(rainy_train[0])\nplt.title(\"Sample Rainy Image\")\nplt.show()\n\nplt.imshow(clear_train[0])\nplt.title(\"Sample Clear Image\")\nplt.show()\n\n# Build the model\ninput_shape = rainy_train[0].shape  # Assuming your input images are all the same size\nmodel = build_full_model(input_shape)\nmodel.summary()\n\n# Enable mixed precision training\nfrom tensorflow.keras.mixed_precision import set_global_policy\nset_global_policy('mixed_float16')\n\n# Proper TensorFlow logging\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n\n# Custom callback to print CPU/GPU usage\nclass ResourceMonitorCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        # Print resource information at the beginning of each epoch\n        print(f\"Epoch {epoch + 1} starting...\")\n        \n        # Check CPU utilization\n        cpu_info = os.popen(\"lscpu\").read()\n        print(f\"CPU Info:\\n{cpu_info}\")\n        \n        # Check GPU utilization if available\n        if tf.config.list_physical_devices('GPU'):\n            gpu_info = os.popen(\"nvidia-smi\").read()\n            print(f\"GPU Info:\\n{gpu_info}\")\n        else:\n            print(\"No GPU detected, using CPU.\")\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Summary of the epoch\n        print(f\"Epoch {epoch + 1} ended. Loss: {logs.get('loss')}, Validation Loss: {logs.get('val_loss')}\")\n        print(\"=\" * 50)\n\n# Ensure the dataset isn't infinite\nbatch_size = 4\nsteps_per_epoch = len(rainy_train) // batch_size  # Ensure steps per epoch is finite\n\n# Add debug prints in your data preprocessing and training loops\nprint(f\"Dataset Size: {len(rainy_train)}\")\nprint(f\"Steps per Epoch: {steps_per_epoch}\")\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Define callbacks\ncheckpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    rainy_train, clear_train,\n    validation_data=(rainy_test, clear_test),\n    epochs=50,\n    batch_size=8,  # Change this value if necessary\n    callbacks=[checkpoint, early_stop],\n    verbose=1\n)\n\n# Evaluate the model\npredicted_images = model.predict(rainy_test)\n\n# Calculate MSE\nmse = mean_squared_error(clear_test.flatten(), predicted_images.flatten())\nprint(\"Mean Squared Error on Test Set: \", mse)\n\n# Calculate PSNR\npsnr_value = tf.reduce_mean(psnr(clear_test, predicted_images, max_val=1.0))\nprint(\"PSNR on Test Set: \", psnr_value.numpy())\n\n# Calculate SSIM (with win_size <= 7, and using channel_axis instead of multichannel)\nssim_values = []\nfor i in range(len(clear_test)):\n    ssim_value = ssim(clear_test[i], predicted_images[i], win_size=5, channel_axis=-1)  # Set win_size <= 7\n    ssim_values.append(ssim_value)\n\navg_ssim_value = np.mean(ssim_values)\nprint(\"Average SSIM on Test Set: \", avg_ssim_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New concept","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, MultiHeadAttention, DepthwiseConv2D\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom skimage.metrics import structural_similarity as ssim\n\n# Optionally, force CPU for debugging\n# tf.config.set_visible_devices([], 'GPU')\n\n# Disable XLA to avoid issues\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        \n        # Use Depthwise Separable Convolutions\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)  # Pointwise Conv\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Swin Transformer Block Implementation with k-th Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, k=4, shift_size=0, mlp_ratio=4.):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.num_heads = num_heads\n        self.k = k  # Number of top-k attention weights\n        self.dim = dim\n        \n        # Layer normalization\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        \n        # Feed-forward network\n        self.mlp = self.get_mlp(dim, mlp_ratio)\n\n    def get_mlp(self, dim, mlp_ratio):\n        return tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dense(dim)\n        ])\n    \n    def call(self, x):\n        # Layer normalization\n        x_norm = self.norm1(x)\n\n        # Compute attention\n        attn_output = self.attn(x_norm, x_norm)\n\n        # Get top-k attention weights\n        attn_weights = tf.nn.softmax(tf.matmul(x_norm, x_norm, transpose_b=True))  # Computing attention weights\n        top_k_values, indices = tf.nn.top_k(attn_weights, k=self.k)\n        threshold = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n\n        # Suppress weights below the threshold\n        attn_weights = tf.where(attn_weights < threshold, tf.zeros_like(attn_weights), attn_weights)\n\n        # Normalize the weights again after thresholding\n        attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=-1, keepdims=True)\n\n        # Use the modified attention weights to compute output\n        attn_output = tf.matmul(attn_weights, attn_output)\n\n        # Skip connection\n        x = x + attn_output  \n\n        # Feed-forward network\n        x = self.norm2(x)\n        mlp_output = self.mlp(x)\n        x = x + mlp_output  # Skip connection\n        \n        return x\n\ndef swin_transformer_block(inputs):\n    # Define parameters for the Swin Transformer Block\n    dim = inputs.shape[-1]  # Input channel dimension\n    num_heads = 4  # Number of attention heads\n    window_size = 7  # Size of the window\n    shift_size = 0  # Shift size for the windowing scheme\n    \n    # Create the Swin Transformer Block\n    x = SwinTransformerBlock(dim, num_heads, window_size, k=4, shift_size=shift_size)(inputs)\n    \n    return x\n\n# Recursive Swin Transformer Block with residual connections\ndef recursive_swin_transformer_block(inputs, num_recursions=6):\n    x = inputs\n    for _ in range(num_recursions):\n        # Call the Swin Transformer block\n        x = swin_transformer_block(x)\n    \n    return x\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(inputs, num_classes=3):\n    # Normalize the input features\n    x = LayerNormalization()(inputs)\n    \n    # Using Depthwise Separable Convolutions\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n    \n    return x\n\n# Full Model Creation\ndef build_full_model(input_shape):\n    # Input Layer\n    inputs = Input(shape=input_shape)\n\n    # Dynamic Pyramid Model\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n\n    # Recursive Swin Transformer Block\n    transformer_features = recursive_swin_transformer_block(pyramid_features)\n\n    # Image Restoration and Enhancement\n    outputs = image_restoration_and_enhancement(transformer_features, num_classes=input_shape[-1])\n\n    return Model(inputs=inputs, outputs=outputs)\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Show some example images to verify data loading\nplt.imshow(rainy_train[0])\nplt.title(\"Sample Rainy Image\")\nplt.show()\n\nplt.imshow(clear_train[0])\nplt.title(\"Sample Clear Image\")\nplt.show()\n\n# Build the model\ninput_shape = rainy_train[0].shape  # Assuming your input images are all the same size\nmodel = build_full_model(input_shape)\nmodel.summary()\n\n# Enable mixed precision training\nfrom tensorflow.keras.mixed_precision import set_global_policy\nset_global_policy('mixed_float16')\n\n# Proper TensorFlow logging\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n\n# Custom callback to print CPU/GPU usage\nclass ResourceMonitorCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        # Print resource information at the beginning of each epoch\n        print(f\"Epoch {epoch + 1} starting...\")\n        \n        # Check CPU utilization\n        cpu_info = os.popen(\"lscpu\").read()\n        print(f\"CPU Info:\\n{cpu_info}\")\n        \n        # Check GPU utilization if available\n        if tf.config.list_physical_devices('GPU'):\n            gpu_info = os.popen(\"nvidia-smi\").read()\n            print(f\"GPU Info:\\n{gpu_info}\")\n        else:\n            print(\"No GPU detected, using CPU.\")\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Summary of the epoch\n        print(f\"Epoch {epoch + 1} ended. Loss: {logs.get('loss')}, Validation Loss: {logs.get('val_loss')}\")\n        print(\"=\" * 50)\n\n# Ensure the dataset isn't infinite\nbatch_size = 4\nsteps_per_epoch = len(rainy_train) // batch_size  # Ensure steps per epoch is finite\n\n# Add debug prints in your data preprocessing and training loops\nprint(f\"Dataset Size: {len(rainy_train)}\")\nprint(f\"Steps per Epoch: {steps_per_epoch}\")\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Define callbacks\ncheckpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    rainy_train, clear_train,\n    validation_data=(rainy_test, clear_test),\n    epochs=50,\n    batch_size=8,  # Change this value if necessary\n    callbacks=[checkpoint, early_stop],\n    verbose=1\n)\n\n# Evaluate the model\nmse = mean_squared_error(clear_test.flatten(), model.predict(rainy_test).flatten())\nprint(\"Mean Squared Error on Test Set: \", mse)\n\n# You can also calculate SSIM\nssim_value = ssim(clear_test[0], model.predict(rainy_test)[0], multichannel=True)\nprint(\"SSIM Value on Test Set: \", ssim_value)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Without Swin","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, DepthwiseConv2D\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom skimage.metrics import structural_similarity as ssim\nfrom tensorflow.image import psnr  # Importing TensorFlow's PSNR function\n\n# Optionally, force CPU for debugging\n# tf.config.set_visible_devices([], 'GPU')\n\n# Disable XLA to avoid issues\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        \n        # Use Depthwise Separable Convolutions\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)  # Pointwise Conv\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(inputs, num_classes=3):\n    # Normalize the input features\n    x = LayerNormalization()(inputs)\n    \n    # Using Depthwise Separable Convolutions\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n    \n    return x\n\n# Full Model Creation\ndef build_full_model(input_shape):\n    # Input Layer\n    inputs = Input(shape=input_shape)\n\n    # Dynamic Pyramid Model\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n\n    # Image Restoration and Enhancement\n    outputs = image_restoration_and_enhancement(pyramid_features, num_classes=input_shape[-1])\n\n    return Model(inputs=inputs, outputs=outputs)\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Show some example images to verify data loading\nplt.imshow(rainy_train[0])\nplt.title(\"Sample Rainy Image\")\nplt.show()\n\nplt.imshow(clear_train[0])\nplt.title(\"Sample Clear Image\")\nplt.show()\n\n# Build the model\ninput_shape = rainy_train[0].shape  # Assuming your input images are all the same size\nmodel = build_full_model(input_shape)\nmodel.summary()\n\n# Enable mixed precision training\nfrom tensorflow.keras.mixed_precision import set_global_policy\nset_global_policy('mixed_float16')\n\n# Proper TensorFlow logging\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n\n# Custom callback to print CPU/GPU usage\nclass ResourceMonitorCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        # Print resource information at the beginning of each epoch\n        print(f\"Epoch {epoch + 1} starting...\")\n        \n        # Check CPU utilization\n        cpu_info = os.popen(\"lscpu\").read()\n        print(f\"CPU Info:\\n{cpu_info}\")\n        \n        # Check GPU utilization if available\n        if tf.config.list_physical_devices('GPU'):\n            gpu_info = os.popen(\"nvidia-smi\").read()\n            print(f\"GPU Info:\\n{gpu_info}\")\n        else:\n            print(\"No GPU detected, using CPU.\")\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Summary of the epoch\n        print(f\"Epoch {epoch + 1} ended. Loss: {logs.get('loss')}, Validation Loss: {logs.get('val_loss')}\")\n        print(\"=\" * 50)\n\n# Ensure the dataset isn't infinite\nbatch_size = 4\nsteps_per_epoch = len(rainy_train) // batch_size  # Ensure steps per epoch is finite\n\n# Add debug prints in your data preprocessing and training loops\nprint(f\"Dataset Size: {len(rainy_train)}\")\nprint(f\"Steps per Epoch: {steps_per_epoch}\")\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Define callbacks\ncheckpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    rainy_train, clear_train,\n    validation_data=(rainy_test, clear_test),\n    epochs=50,\n    batch_size=8,  # Change this value if necessary\n    callbacks=[checkpoint, early_stop],\n    verbose=1\n)\n\n# Evaluate the model\npredicted_images = model.predict(rainy_test)\n\n# Calculate MSE\nmse = mean_squared_error(clear_test.flatten(), predicted_images.flatten())\nprint(\"Mean Squared Error on Test Set: \", mse)\n\n# Calculate PSNR\npsnr_value = tf.reduce_mean(psnr(clear_test, predicted_images, max_val=1.0))\nprint(\"PSNR on Test Set: \", psnr_value.numpy())\n\n# Calculate SSIM (with win_size <= 7, and using channel_axis instead of multichannel)\nssim_values = []\nfor i in range(len(clear_test)):\n    ssim_value = ssim(clear_test[i], predicted_images[i], win_size=5, channel_axis=-1)  # Set win_size <= 7\n    ssim_values.append(ssim_value)\n\navg_ssim_value = np.mean(ssim_values)\nprint(\"Average SSIM on Test Set: \", avg_ssim_value)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pruned model","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow-model-optimization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, MultiHeadAttention, DepthwiseConv2D, Dropout\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow_model_optimization as tfmot\nfrom tensorflow.keras.losses import KLDivergence\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    \"\"\"\n    Loads and resizes images from a specified folder.\n\n    Args:\n        folder (str): Path to the folder containing images.\n        size (tuple): Desired size for resizing the images (width, height).\n\n    Returns:\n        np.array: Array of loaded and resized images.\n    \"\"\"\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    \"\"\"\n    Preprocess and augment image data.\n\n    Args:\n        images (np.array): Array of images to preprocess.\n        augment (bool): If True, applies augmentation to the images.\n\n    Returns:\n        np.array: Preprocessed (and possibly augmented) images.\n    \"\"\"\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    \"\"\"\n    Loads and preprocesses the dataset for training and testing.\n\n    Args:\n        base_folder (str): Path to the base folder containing datasets.\n        size (tuple): Desired size for resizing the images.\n        augment (bool): If True, applies data augmentation to training data.\n\n    Returns:\n        tuple: Training and testing datasets for rainy and clear images.\n    \"\"\"\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    \"\"\"\n    Builds an advanced dynamic pyramid model.\n\n    Args:\n        input_shape (tuple): Shape of the input image (height, width, channels).\n        num_scales (int): Number of scales for the pyramid model.\n\n    Returns:\n        tuple: Model input and concatenated pyramid features.\n    \"\"\"\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        kernel_size = 3 + scale\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        pyramid_features.append(x_resized)\n\n    fused_features = Lambda(lambda x: tf.concat(x, axis=-1))(pyramid_features)\n    return inputs, fused_features\n\n# Swin Transformer Block\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, k=4, shift_size=0, mlp_ratio=4.):\n        \"\"\"\n        Swin Transformer Block with top-k sparsity applied.\n\n        Args:\n            dim (int): Input dimension size.\n            num_heads (int): Number of attention heads.\n            window_size (int): Size of the local attention window.\n            k (int): Top-k sparsity parameter.\n            shift_size (int): Shift size for shifted windowing.\n            mlp_ratio (float): Ratio for the MLP hidden size.\n        \"\"\"\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.num_heads = num_heads\n        self.k = k\n        self.dim = dim\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.mlp = self.get_mlp(dim, mlp_ratio)\n\n    def get_mlp(self, dim, mlp_ratio):\n        return tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dense(dim)\n        ])\n\n    def call(self, x):\n        \"\"\"\n        Applies Swin transformer block with top-k sparsity attention.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Tensor after applying attention and MLP.\n        \"\"\"\n        x_norm = self.norm1(x)\n        attn_output = self.attn(x_norm, x_norm)\n        attn_weights = tf.nn.softmax(tf.matmul(x_norm, x_norm, transpose_b=True))\n        top_k_values, indices = tf.nn.top_k(attn_weights, k=self.k)\n        threshold = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n        attn_weights = tf.where(attn_weights < threshold, tf.zeros_like(attn_weights), attn_weights)\n        attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=-1, keepdims=True)\n        attn_output = tf.matmul(attn_weights, attn_output)\n        x = x + attn_output  \n        x = self.norm2(x)\n        mlp_output = self.mlp(x)\n        x = x + mlp_output  \n        return x\n\n# Swin Transformer block helper functions\ndef swin_transformer_block(inputs):\n    dim = inputs.shape[-1]\n    num_heads = 4\n    window_size = 7\n    x = SwinTransformerBlock(dim, num_heads, window_size, k=4)(inputs)\n    return x\n\ndef recursive_swin_transformer_block(inputs, num_recursions=6):\n    \"\"\"\n    Builds a recursive Swin transformer block for multiple recursions.\n\n    Args:\n        inputs: Input tensor.\n        num_recursions (int): Number of recursive blocks to apply.\n\n    Returns:\n        Output tensor after multiple recursions.\n    \"\"\"\n    x = inputs\n    for _ in range(num_recursions):\n        x = swin_transformer_block(x)\n    return x\n\n# Image Restoration and Enhancement Block with Dropout\ndef image_restoration_and_enhancement_with_dropout(inputs, num_classes=3, dropout_rate=0.4):\n    \"\"\"\n    Image restoration and enhancement block with dropout layers.\n\n    Args:\n        inputs: Input tensor.\n        num_classes (int): Number of output channels.\n        dropout_rate (float): Dropout rate to apply.\n\n    Returns:\n        Output tensor for image restoration and enhancement.\n    \"\"\"\n    x = LayerNormalization()(inputs)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(num_classes, (3, 3), padding='same')(x)\n    return x\n\n# Build the complete model with pyramid, Swin transformer, and enhancement block\ndef build_full_model_with_dropout(input_shape=(128, 128, 3), num_classes=3, dropout_rate=0.4):\n    \"\"\"\n    Builds the complete image restoration model with dropout, dynamic pyramid, and Swin transformer.\n\n    Args:\n        input_shape (tuple): Shape of the input image (height, width, channels).\n        num_classes (int): Number of output channels.\n        dropout_rate (float): Dropout rate to apply in the enhancement block.\n\n    Returns:\n        Model: Keras Model object.\n    \"\"\"\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n    swin_output = recursive_swin_transformer_block(pyramid_features)\n    output = image_restoration_and_enhancement_with_dropout(swin_output, num_classes=num_classes, dropout_rate=dropout_rate)\n    model = Model(inputs=inputs, outputs=output)\n\n    # Model summary\n    model.summary()\n    return model\n\n# Knowledge Distillation Loss Function\ndef knowledge_distillation_loss(student_logits, teacher_logits, alpha=0.5, temperature=3):\n    \"\"\"\n    Custom loss function for knowledge distillation.\n\n    Args:\n        student_logits: Output logits from the student model.\n        teacher_logits: Output logits from the teacher model.\n        alpha (float): Weight for balancing between hard target loss and soft target loss.\n        temperature (float): Temperature for scaling logits during distillation.\n\n    Returns:\n        Loss value.\n    \"\"\"\n    distillation_loss = KLDivergence()(tf.nn.softmax(teacher_logits / temperature),\n                                       tf.nn.softmax(student_logits / temperature)) * (temperature ** 2)\n    return distillation_loss\n\n# Build a smaller student model\ndef build_student_model(input_shape=(128, 128, 3), dropout_rate=0.4):\n    \"\"\"\n    Builds a smaller student model for knowledge distillation.\n\n    Args:\n        input_shape (tuple): Shape of the input image (height, width, channels).\n        dropout_rate (float): Dropout rate to apply in the enhancement block.\n\n    Returns:\n        Model: Keras Model object.\n    \"\"\"\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n    swin_output = recursive_swin_transformer_block(pyramid_features, num_recursions=3)  # Fewer recursions for student\n    output = image_restoration_and_enhancement_with_dropout(swin_output, dropout_rate=dropout_rate)\n    student_model = Model(inputs=inputs, outputs=output)\n\n    # Model summary\n    student_model.summary()\n    return student_model\n\n# Pruning with TensorFlow Model Optimization Toolkit\ndef apply_pruning(model):\n    \"\"\"\n    Applies pruning to the given model to reduce its size.\n\n    Args:\n        model: Keras Model to prune.\n\n    Returns:\n        pruned_model: Pruned Keras model.\n    \"\"\"\n    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n    pruning_params = {\n        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n            initial_sparsity=0.0, final_sparsity=0.5,\n            begin_step=2000, end_step=10000\n        )\n    }\n    pruned_model = prune_low_magnitude(model, **pruning_params)\n    pruned_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n    # Model summary\n    pruned_model.summary()\n    return pruned_model\n\n# Custom training loop for Knowledge Distillation\ndef train_student_with_distillation(student_model, teacher_model, rainy_train, clear_train, epochs=50, batch_size=32):\n    \"\"\"\n    Trains the student model using knowledge distillation from the teacher model.\n\n    Args:\n        student_model: Keras model for the student.\n        teacher_model: Trained teacher model.\n        rainy_train: Training set inputs (rainy images).\n        clear_train: Training set targets (clear images).\n        epochs (int): Number of epochs to train.\n        batch_size (int): Batch size for training.\n    \"\"\"\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        for batch in range(0, len(rainy_train), batch_size):\n            x_batch = rainy_train[batch:batch + batch_size]\n            y_batch = clear_train[batch:batch + batch_size]\n            teacher_logits = teacher_model.predict(x_batch)\n            with tf.GradientTape() as tape:\n                student_logits = student_model(x_batch)\n                loss = knowledge_distillation_loss(student_logits, teacher_logits)\n            gradients = tape.gradient(loss, student_model.trainable_variables)\n            student_model.optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n\n# Example usage:\n\n# Load data\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Train Teacher Model\nteacher_model = build_full_model_with_dropout(input_shape=(128, 128, 3), dropout_rate=0.4)\nteacher_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\nteacher_model.fit(rainy_train, clear_train, epochs=50, batch_size=32, validation_split=0.1)\n\n# Apply Pruning\npruned_teacher_model = apply_pruning(teacher_model)\n\n# Train Student Model using Knowledge Distillation\nstudent_model = build_student_model(input_shape=(128, 128, 3), dropout_rate=0.4)\nstudent_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\ntrain_student_with_distillation(student_model, pruned_teacher_model, rainy_train, clear_train, epochs=50, batch_size=32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model with feature aggregation in swin transformer","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, MultiHeadAttention, DepthwiseConv2D, Dropout\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        kernel_size = 3 + scale\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        pyramid_features.append(x_resized)\n\n    fused_features = Lambda(lambda x: tf.concat(x, axis=-1))(pyramid_features)\n    return inputs, fused_features\n\n# Swin Transformer Block\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, k=4, shift_size=0, mlp_ratio=4.):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.num_heads = num_heads\n        self.k = k\n        self.dim = dim\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.mlp = self.get_mlp(dim, mlp_ratio)\n\n    def get_mlp(self, dim, mlp_ratio):\n        return tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dense(dim)\n        ])\n\n    def call(self, x):\n        x_norm = self.norm1(x)\n        attn_output = self.attn(x_norm, x_norm)\n        attn_weights = tf.nn.softmax(tf.matmul(x_norm, x_norm, transpose_b=True))\n        top_k_values, indices = tf.nn.top_k(attn_weights, k=self.k)\n        threshold = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n        attn_weights = tf.where(attn_weights < threshold, tf.zeros_like(attn_weights), attn_weights)\n        attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=-1, keepdims=True)\n        attn_output = tf.matmul(attn_weights, attn_output)\n        x = x + attn_output  \n        x = self.norm2(x)\n        mlp_output = self.mlp(x)\n        x = x + mlp_output  \n        return x\n\n# Swin Transformer block helper functions\ndef swin_transformer_block(inputs):\n    dim = inputs.shape[-1]\n    num_heads = 4\n    window_size = 7\n    x = SwinTransformerBlock(dim, num_heads, window_size, k=4)(inputs)\n    return x\n\n# Recursive Swin Transformer Block with Aggregation and Residual Connections\ndef recursive_swin_transformer_block(inputs, num_recursions=6):\n    x = inputs\n    for i in range(num_recursions):\n        swin_output = swin_transformer_block(x)\n        x = Add()([x, swin_output])  # Residual connection across blocks\n    return x\n\n# Image Restoration and Enhancement Block with Dropout\ndef image_restoration_and_enhancement_with_dropout(inputs, num_classes=3, dropout_rate=0.4):\n    x = LayerNormalization()(inputs)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(num_classes, (3, 3), padding='same')(x)\n    return x\n\n# Build the complete model with pyramid, Swin transformer, and enhancement block\ndef build_full_model_with_dropout(input_shape=(128, 128, 3), num_classes=3, dropout_rate=0.4):\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n    swin_output = recursive_swin_transformer_block(pyramid_features)\n    output = image_restoration_and_enhancement_with_dropout(swin_output, num_classes=num_classes, dropout_rate=dropout_rate)\n    model = Model(inputs=inputs, outputs=output)\n\n    # Model summary\n    model.summary()\n    return model\n\n# Example usage:\n\n# Load data\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Build and compile the full model\nmodel = build_full_model_with_dropout(input_shape=(128, 128, 3), dropout_rate=0.4)\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n# Train the model\nmodel.fit(rainy_train, clear_train, epochs=50, batch_size=32, validation_split=0.1)\n\n# Evaluate the model on the test data\nmodel.evaluate(rainy_test, clear_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []  # Corrected initialization\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\nprint(f\"Training Rainy Images Shape: {train_rainy.shape}\")\nprint(f\"Training Clear Images Shape: {train_clear.shape}\")\nprint(f\"Testing Rainy Images Shape: {test_rainy.shape}\")\nprint(f\"Testing Clear Images Shape: {test_clear.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Last implementation","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torch.nn import functional as F\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom sklearn.model_selection import train_test_split\n\n# Function to load images from a specified folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)  # Read image using skimage\n        img = img_as_float(img)  # Convert to float\n        img_resized = transforms.functional.resize(torch.tensor(img), size)  # Resize image\n        images.append(img_resized.numpy())  # Append to list\n    return np.array(images)  # Convert list to numpy array\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        # Apply augmentation (example: flipping, rotation)\n        transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(),  # Random horizontal flip\n            transforms.RandomRotation(15),  # Random rotation within 15 degrees\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Color jitter\n            transforms.ToTensor()  # Convert image to tensor\n        ])\n        images = np.array([transform(img) for img in images])  # Apply transformations\n    return images  # Return preprocessed images\n\n# Dataset class for PyTorch\nclass ImageDataset(data.Dataset):\n    def __init__(self, rainy_images, clear_images, transform=None):\n        self.rainy_images = rainy_images  # Rainy images\n        self.clear_images = clear_images  # Corresponding clear images\n        self.transform = transform  # Transformation to apply\n\n    def __len__(self):\n        return len(self.rainy_images)  # Return the total number of samples\n\n    def __getitem__(self, idx):\n        rainy_image = self.rainy_images[idx]  # Get rainy image\n        clear_image = self.clear_images[idx]  # Get corresponding clear image\n        if self.transform:\n            rainy_image = self.transform(rainy_image)  # Apply transformation\n            clear_image = self.transform(clear_image)  # Apply transformation\n        return rainy_image, clear_image  # Return pair of images\n\n# Function to load datasets from the base folder\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []  # Lists for training images\n    rainy_images_test, clear_images_test = [], []  # Lists for testing images\n\n    datasets = ['Rain200L', 'Rain200H']  # Example dataset folders\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')  # Path to training folder\n        test_folder = os.path.join(base_folder, dataset, 'test')  # Path to testing folder\n\n        # Load training images\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))  # Load rainy images\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))  # Load clear images\n\n        # Load testing images\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))  # Load rainy test images\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))  # Load clear test images\n\n    rainy_images_train = np.array(rainy_images_train)  # Convert to numpy array\n    clear_images_train = np.array(clear_images_train)  # Convert to numpy array\n    rainy_images_test = np.array(rainy_images_test)  # Convert to numpy array\n    clear_images_test = np.array(clear_images_test)  # Convert to numpy array\n\n    if augment:  # Check if augmentation is enabled\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)  # Augment training images\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)  # Augment training images\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)  # Return datasets\n\n# Fully connected multilayer perceptron class\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features  # If out_features is None, use in_features\n        hidden_features = hidden_features or in_features  # If hidden_features is None, use in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)  # First fully connected layer\n        self.act = act_layer()  # Activation layer\n        self.fc2 = nn.Linear(hidden_features, out_features)  # Second fully connected layer\n        self.drop = nn.Dropout(drop)  # Dropout layer\n\n    def forward(self, x):\n        x = self.fc1(x)  # Pass through first layer\n        x = self.act(x)  # Activation\n        x = self.drop(x)  # Dropout\n        x = self.fc2(x)  # Pass through second layer\n        x = self.drop(x)  # Dropout\n        return x  # Return output\n\n# Function to partition the input into windows\ndef window_partition(x, window_size):\n    B, H, W, C = x.shape  # Extract dimensions\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)  # Reshape\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)  # Permute dimensions\n    return windows  # Return windows\n\n# Function to reverse the partitioning process\ndef window_reverse(windows, window_size, H, W):\n    B = int(windows.shape[0] / (H * W / window_size / window_size))  # Calculate batch size\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)  # Reshape\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)  # Permute dimensions\n    return x  # Return original shape\n\n# Window attention class\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0., top_k=10):\n        super().__init__()\n        self.dim = dim  # Dimension of input\n        self.window_size = window_size  # Size of the attention window\n        self.num_heads = num_heads  # Number of attention heads\n        self.top_k = top_k  # Retain top-k attention scores\n        head_dim = dim // num_heads  # Dimension per head\n        self.scale = qk_scale or head_dim ** -0.5  # Scaling factor\n\n        # Relative position bias table\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n\n        coords_h = torch.arange(self.window_size[0])  # Height coordinates\n        coords_w = torch.arange(self.window_size[1])  # Width coordinates\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # Mesh grid\n        coords_flatten = torch.flatten(coords, 1)  # Flatten coordinates\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # Calculate relative coordinates\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Permute dimensions\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # Adjust coordinates\n        relative_coords[:, :, 1] += self.window_size[1] - 1  # Adjust coordinates\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1  # Calculate index\n        relative_position_index = relative_coords.sum(-1)  # Sum to get index\n        self.register_buffer(\"relative_position_index\", relative_position_index)  # Register buffer\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Linear layer for Q, K, V\n        self.attn_drop = nn.Dropout(attn_drop)  # Dropout layer for attention\n        self.proj = nn.Linear(dim, dim)  # Projection layer\n        self.proj_drop = nn.Dropout(proj_drop)  # Dropout layer for projection\n\n    def forward(self, x):\n        B, N, C = x.shape  # Extract dimensions\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3, 4)  # Calculate Q, K, V\n        q, k, v = qkv.unbind(2)  # Unbind into Q, K, V\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # Calculate attention scores\n        attn = attn.softmax(dim=-1)  # Softmax to get attention weights\n        attn = self.attn_drop(attn)  # Apply dropout\n        x = (attn @ v)  # Apply attention to V\n        x = x.transpose(1, 2).reshape(B, N, C)  # Reshape output\n        x = self.proj(x)  # Project output\n        x = self.proj_drop(x)  # Apply dropout\n        return x  # Return output\n\n# Swin Transformer block class\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self, dim, input_resolution, num_heads, window_size, shift_size=0, mlp_ratio=4., qkv_bias=True,\n                 qk_scale=None, drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution  # Input resolution\n        self.dim = dim  # Dimension of input\n        self.window_size = window_size  # Size of the window\n        self.shift_size = shift_size  # Shift size\n        self.norm1 = norm_layer(dim)  # Normalization layer\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.norm2 = norm_layer(dim)  # Second normalization layer\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), drop=drop)  # MLP\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()  # DropPath for residual connections\n\n    def forward(self, x):\n        H, W = self.input_resolution  # Get input resolution\n        shortcut = x  # Store shortcut for residual connection\n        x = self.norm1(x)  # Apply normalization\n        x = window_partition(x, self.window_size)  # Partition into windows\n        x = self.attn(x)  # Apply attention\n        x = window_reverse(x, self.window_size, H, W)  # Reverse partitioning\n        x = shortcut + self.drop_path(x)  # Residual connection\n\n        shortcut = x  # Store shortcut again\n        x = self.norm2(x)  # Apply second normalization\n        x = self.mlp(x)  # Apply MLP\n        x = shortcut + self.drop_path(x)  # Second residual connection\n\n        return x  # Return output\n\n# Patch embedding class\nclass PatchEmbed(nn.Module):\n    def __init__(self, img_size=128, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        self.img_size = img_size  # Image size\n        self.patch_size = patch_size  # Patch size\n        self.num_patches = (img_size // patch_size) ** 2  # Number of patches\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)  # Convolutional layer\n        self.norm = norm_layer(embed_dim) if norm_layer else None  # Normalization layer\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)  # Project and flatten patches\n        if self.norm is not None:\n            x = self.norm(x)  # Apply normalization if defined\n        return x  # Return output\n\n# Hierarchical Swin Transformer class\nclass HierarchicalSwinTransformer(nn.Module):\n    def __init__(self, img_size=128, patch_size=4, in_chans=3, embed_dim=96, num_heads=4, window_size=7, mlp_ratio=4., \n                 depths=[2, 2, 2, 2], drop_rate=0., drop_path_rate=0.1, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)  # Patch embedding\n        \n        self.num_layers = len(depths)  # Number of layers\n        self.layers = nn.ModuleList()  # List of layers\n        for i in range(self.num_layers):\n            layer = nn.ModuleList([\n                SwinTransformerBlock(\n                    dim=embed_dim * (2 ** i),  # Update dimension for each layer\n                    input_resolution=(img_size // (2 ** i), img_size // (2 ** i)),  # Update input resolution\n                    num_heads=num_heads,  # Number of heads\n                    window_size=window_size,\n                    drop_path=drop_path_rate * (i + 1) / self.num_layers,  # Drop path rate\n                    norm_layer=norm_layer  # Normalization layer\n                ) for _ in range(depths[i])  # Create multiple blocks per layer\n            ])\n            self.layers.append(layer)  # Add layer to list\n\n        self.norm = norm_layer(embed_dim * (2 ** (self.num_layers - 1)))  # Final normalization layer\n\n    def forward(self, x):\n        x = self.patch_embed(x)  # Patch embedding\n\n        for layer in self.layers:  # Iterate over layers\n            for block in layer:  # Iterate over blocks\n                x = block(x)  # Forward pass through block\n\n        x = self.norm(x)  # Apply final normalization\n        return x  # Return output\n\n# Loss function for image restoration: Mean Squared Error\nclass RestorationLoss(nn.Module):\n    def __init__(self):\n        super(RestorationLoss, self).__init__()\n\n    def forward(self, output, target):\n        return F.mse_loss(output, target)  # Calculate and return MSE loss\n\n# Main training function\ndef train_model(model, train_loader, optimizer, criterion, num_epochs=10, device='cpu'):\n    model.to(device)  # Move model to device\n    for epoch in range(num_epochs):  # Iterate over epochs\n        model.train()  # Set model to training mode\n        running_loss = 0.0  # Initialize running loss\n        for i, (rainy_images, clear_images) in enumerate(train_loader):  # Iterate over batches\n            rainy_images, clear_images = rainy_images.to(device), clear_images.to(device)  # Move data to device\n            optimizer.zero_grad()  # Zero the gradients\n            outputs = model(rainy_images)  # Forward pass\n            loss = criterion(outputs, clear_images)  # Calculate loss\n            loss.backward()  # Backward pass\n            optimizer.step()  # Optimize model\n            running_loss += loss.item()  # Accumulate loss\n        \n        epoch_loss = running_loss / len(train_loader)  # Average loss for the epoch\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')  # Print epoch loss\n\n# Main script\nif __name__ == '__main__':\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n    (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test) = load_and_preprocess_datasets(base_folder)\n\n    # Create dataset and data loader\n    train_dataset = ImageDataset(rainy_images_train, clear_images_train, transform=None)  # Create dataset\n    train_loader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create data loader\n\n    # Initialize model, optimizer, and loss function\n    model = HierarchicalSwinTransformer()  # Create model\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)  # Adam optimizer\n    criterion = RestorationLoss()  # Loss function\n\n    # Train the model\n    train_model(model, train_loader, optimizer, criterion, num_epochs=20, device='cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'swin_transformer_model.pth')  # Save model state\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Advanced Dynamic Pyramid Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp  # Import TensorFlow Probability for percentile calculation\nfrom keras.layers import Input, Conv2D, Add, Lambda, LayerNormalization, MultiHeadAttention, Dense, Layer, UpSampling2D\nfrom tensorflow.keras.models import Model\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        x_conv = Conv2D(64 * scale_factor, (kernel_size, kernel_size), activation='relu', padding='same')(x_scaled)\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Custom Layer: Window Partition Layer\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        windowed = tf.image.extract_patches(\n            inputs,\n            sizes=[1, self.window_size, self.window_size, 1],\n            strides=[1, self.window_size, self.window_size, 1],\n            rates=[1, 1, 1, 1],\n            padding='VALID'\n        )\n        windowed = tf.reshape(windowed, (batch_size, -1, self.window_size * self.window_size, channels))\n        return windowed\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], (input_shape[1] // self.window_size) * (input_shape[2] // self.window_size), self.window_size * self.window_size, input_shape[3])\n\n# Custom Layer: Patch Merge Layer\nclass PatchMergeLayer(Layer):\n    def __init__(self, window_size):\n        super(PatchMergeLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, num_windows, flattened_window_size, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        merged = tf.reshape(inputs, (batch_size, int(num_windows**0.5), int(num_windows**0.5), self.window_size, self.window_size, channels))\n        merged = tf.transpose(merged, perm=[0, 1, 3, 2, 4, 5])\n        merged = tf.reshape(merged, (batch_size, merged.shape[1] * self.window_size, merged.shape[3] * self.window_size, channels))\n        return merged, self.window_size * 2\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1] * 2, input_shape[2] * 2, input_shape[3])\n\n# Modified Swin Transformer Block with dynamic attention filtering\nclass ModifiedSwinTransformerBlock(Layer):\n    def __init__(self, initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3):\n        super(ModifiedSwinTransformerBlock, self).__init__()\n        self.initial_window_size = initial_window_size\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.num_recursions = num_recursions\n\n    def call(self, fused_features):\n        original_shape = tf.shape(fused_features)\n        return self.recursive_block(fused_features, self.initial_window_size, 2, self.num_recursions, original_shape)\n\n    def recursive_block(self, x, window_size, iteration, recursion, original_shape):\n        if recursion == 0:\n            return x\n        \n        for _ in range(iteration):\n            # Partition the window\n            partition_layer = WindowPartitionLayer(window_size)\n            windows = partition_layer(x)\n\n            window_shape = (tf.shape(windows)[-2], tf.shape(windows)[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], tf.shape(x)[-1]))\n\n            # Apply LayerNormalization and MultiHeadAttention\n            x_norm = LayerNormalization(axis=-1)(windows_reshaped)\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n\n            # Dynamically filter 60-80% attention values based on a threshold\n            attention_scores = tf.reduce_mean(tf.abs(attention), axis=-1, keepdims=True)\n            threshold = tfp.stats.percentile(attention_scores, tf.random.uniform([], 60, 80))\n            attention = tf.where(attention_scores > threshold, attention, tf.zeros_like(attention))\n\n            # Residual connection\n            x_add = Add()([windows_reshaped, attention])\n            x_reconstructed = self.window_reverse(x_add, original_shape)\n\n            # Apply LayerNormalization and FFN\n            x_norm_ffn = LayerNormalization(axis=-1)(x_reconstructed)\n            x_ffn = Dense(128, activation='relu')(x_norm_ffn)\n            x_ffn_out = Dense(tf.shape(x)[-1])(x_ffn)\n\n            x_out = Add()([x_reconstructed, x_ffn_out])\n\n            # Merge the patches and enlarge window size for next iteration\n            patch_merge_layer = PatchMergeLayer(window_size)\n            x_out, window_size = patch_merge_layer(x_out)\n\n        return self.recursive_block(x_out, window_size, iteration, recursion - 1, original_shape)\n\n    def window_reverse(self, x, original_shape):\n        batch_size = tf.shape(x)[0]\n        num_windows = tf.shape(x)[1] // (self.initial_window_size * self.initial_window_size)\n        x = tf.reshape(x, (batch_size, num_windows, self.initial_window_size, self.initial_window_size, -1))\n        return tf.reshape(x, (batch_size, original_shape[1], original_shape[2], -1))\n\n    def compute_output_shape(self, input_shape):\n        # The output shape changes based on recursion and merging, so return the computed shape\n        return input_shape  # You may need to adjust this if shape transforms across layers\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n\n# Full Model Construction\ndef build_full_model(input_shape):\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_block = ModifiedSwinTransformerBlock(initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3)\n    transformer_output = transformer_block(pyramid_features)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\n# Compile the model\ninput_shape = (256, 256, 3)  # Example input shape\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Window Partition and Reverse Functions","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        patches = tf.image.extract_patches(images=inputs,\n                                           sizes=[1, self.window_size, self.window_size, 1],\n                                           strides=[1, self.window_size, self.window_size, 1],\n                                           rates=[1, 1, 1, 1],\n                                           padding='VALID')\n        return patches\n\ndef window_partition(x, window_size):\n    partition_layer = WindowPartitionLayer(window_size)\n    return partition_layer(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patch Merging Function","metadata":{}},{"cell_type":"code","source":"# Patch Merging Function\ndef patch_merge(x, window_size):\n    \"\"\"Merge adjacent patches to create a larger feature window.\"\"\"\n    partition_layer = WindowPartitionLayer(window_size)\n    patches = partition_layer(x)\n\n    # Calculate the new merged patch (by averaging or pooling)\n    merged_patches = tf.reduce_mean(patches, axis=-1, keepdims=True)\n\n    # Reshape into the original window size\n    new_window_size = window_size * 2\n    merged = window_reverse(merged_patches, new_window_size, x.shape)\n\n    return merged, new_window_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modified Swin Transformer Block with Recursion and Iteration","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\n\ndef modified_swin_transformer_block(fused_features, initial_window_size, num_heads, key_dim, num_recursions):\n    # Define a recursive block as a Keras layer\n    class RecursiveBlock(Layer):\n        def __init__(self, window_size, num_heads, key_dim, num_recursions):\n            super().__init__()\n            self.window_size = window_size\n            self.num_heads = num_heads\n            self.key_dim = key_dim\n            self.num_recursions = num_recursions\n\n        def call(self, x):\n            if self.num_recursions <= 0:\n                return x\n            \n            # Partition the input into windows\n            windows = window_partition(x, self.window_size)\n            window_shape = (windows.shape[-2], windows.shape[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], x.shape[-1]))\n\n            # Layer Normalization before Attention\n            x_norm = LayerNormalization()(windows_reshaped)\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n\n            # Apply reverse window operation\n            x_out = window_reverse(attention, window_shape, x.shape)\n            x_out, window_size = patch_merge(x_out, window_size)\n\n            # Recur on the output\n            return RecursiveBlock(window_size, self.num_heads, self.key_dim, self.num_recursions - 1)(x_out)\n\n    # Instantiate and call the recursive block\n    recursive_layer = RecursiveBlock(initial_window_size, num_heads, key_dim, num_recursions)\n    return recursive_layer(fused_features)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, Conv2D, UpSampling2D\n\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Construct and Compile the Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n\ndef build_full_model(input_shape):\n    inputs = Input(shape=input_shape)\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_output = modified_swin_transformer_block(fused_features=pyramid_features, \n                                                         initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for training\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Load and preprocess datasets, resizing images to (256, 256)\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(256, 256), augment=True)\n\n# Assign training and validation data\ntrain_data = train_rainy  # Input for training\ntrain_labels = train_clear  # Target/Label for training\n\nval_data = test_rainy  # Input for validation\nval_labels = test_clear  # Target/Label for validation\n\n# Set up input shape and define the model\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\n\n# Compile the model with Adam optimizer and MSE loss\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Define callbacks for saving the best model and early stopping\ncheckpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Define training parameters\nepochs = 50\nbatch_size = 16\n\n# Train the model\nhistory = model.fit(train_data, train_labels,\n                    validation_data=(val_data, val_labels),\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[checkpoint, early_stopping])\n\n# Save the final model\nmodel.save('final_model.keras')\n\n# Optionally, plot the training and validation loss over epochs\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for evaluation\nimport tensorflow as tf\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Evaluate the model on the test data\ntest_loss, test_accuracy = model.evaluate(test_data, test_labels)\n\n# Print the test results\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Calculation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for PSNR and SSIM calculations\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport numpy as np\n\n# Function to calculate PSNR\ndef calculate_psnr(ground_truth, prediction):\n    return peak_signal_noise_ratio(ground_truth, prediction, data_range=1.0)\n\n# Function to calculate SSIM\ndef calculate_ssim(ground_truth, prediction):\n    return structural_similarity(ground_truth, prediction, multichannel=True, data_range=1.0)\n\n# Use the trained model to predict on the test data\npredictions = model.predict(test_data)\n\n# Initialize lists to store PSNR and SSIM values\npsnr_list = []\nssim_list = []\n\n# Loop through each test image and calculate PSNR and SSIM\nfor i in range(len(test_data)):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    psnr = calculate_psnr(gt_image, pred_image)\n    ssim = calculate_ssim(gt_image, pred_image)\n    \n    psnr_list.append(psnr)\n    ssim_list.append(ssim)\n\n# Calculate average PSNR and SSIM\naverage_psnr = np.mean(psnr_list)\naverage_ssim = np.mean(ssim_list)\n\n# Print the results\nprint(f\"Average PSNR: {average_psnr:.4f}\")\nprint(f\"Average SSIM: {average_ssim:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Evaluation on Test Images","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for visualization\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Predict on test data\npredictions = model.predict(test_data)\n\n# Visualize the results\nnum_images_to_display = 3\n\nfor i in range(num_images_to_display):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    # Display ground truth and prediction side by side\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Ground Truth\")\n    plt.imshow((gt_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Predicted\")\n    plt.imshow((pred_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Evaluation Across Test Set","metadata":{}},{"cell_type":"code","source":"# PSNR and SSIM Evaluation Across the Test Set\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\n# Function to evaluate PSNR and SSIM for the entire test set\ndef evaluate_test_set_psnr_ssim(test_data, test_labels, predictions):\n    psnr_values = []\n    ssim_values = []\n\n    for i in range(len(test_data)):\n        gt_image = test_labels[i]\n        pred_image = predictions[i]\n        \n        psnr = peak_signal_noise_ratio(gt_image, pred_image, data_range=1.0)\n        ssim = structural_similarity(gt_image, pred_image, multichannel=True, data_range=1.0)\n        \n        psnr_values.append(psnr)\n        ssim_values.append(ssim)\n\n    # Compute the average PSNR and SSIM\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    print(f\"Average PSNR: {avg_psnr:.4f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    \n    return avg_psnr, avg_ssim\n\n# Get predictions from the model\npredictions = model.predict(test_data)\n\n# Evaluate PSNR and SSIM on the test set\navg_psnr, avg_ssim = evaluate_test_set_psnr_ssim(test_data, test_labels, predictions)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of Training History","metadata":{}},{"cell_type":"code","source":"# Import libraries for visualization\nimport matplotlib.pyplot as plt\n\n# Plot the training and validation loss\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot the training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Show the plots\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}