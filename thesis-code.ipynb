{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9457906,"sourceType":"datasetVersion","datasetId":5749623}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# First Five Notebooks for Thesis Work. Learn Code and Integrate them","metadata":{}},{"cell_type":"markdown","source":"# More advance with attention and postprocessing","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization, Conv2DTranspose, ReLU, Multiply, Add, BatchNormalization, Dropout, GlobalAveragePooling2D, Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob\nimport cv2  # Ensure you have opencv installed\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Conv2D, Multiply\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  \n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\ndef channel_attention(x, reduction_ratio=16):\n    # Apply global average and max pooling\n    channel_avg = GlobalAveragePooling2D()(x)\n    channel_max = GlobalMaxPooling2D()(x)\n\n    # Reshape to match the number of channels\n    channel_avg = Reshape((1, 1, x.shape[-1]))(channel_avg)\n    channel_max = Reshape((1, 1, x.shape[-1]))(channel_max)\n\n    # Concatenate the pooled features\n    concat = Concatenate(axis=-1)([channel_avg, channel_max])\n\n    # Pass through a convolutional layer to generate attention weights\n    channel_weights = Conv2D(filters=x.shape[-1] // reduction_ratio, kernel_size=1, activation='relu')(concat)\n    channel_weights = Conv2D(filters=x.shape[-1], kernel_size=1, activation='sigmoid')(channel_weights)\n\n    return Multiply()([x, channel_weights])\n\n\ndef spatial_attention(x):\n    # Apply global average and max pooling\n    avg_pool = GlobalAveragePooling2D()(x)  # Use Keras layer\n    max_pool = GlobalMaxPooling2D()(x)      # Use Keras layer\n\n    # Reshape to match the input dimensions\n    avg_pool = Reshape((1, 1, x.shape[-1]))(avg_pool)  # Reshape for broadcasting\n    max_pool = Reshape((1, 1, x.shape[-1]))(max_pool)  # Reshape for broadcasting\n\n    # Concatenate the pooled features\n    concat = Concatenate(axis=-1)([avg_pool, max_pool])\n\n    # Use convolution to get the attention weights\n    attention = Conv2D(filters=1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n\n    # Apply the attention weights to the input\n    return Multiply()([x, attention])\n\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=1, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n# Building a U-Net with one less layer\ndef build_simplified_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder Block 1\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)\n    conv1 = channel_attention(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    swin1 = SwinTransformerBlock(dim=64, num_heads=4, shift_size=1)(pool1)\n    swin1_residual = Add()([pool1, swin1])  # Residual connection after Swin Transformer\n\n    # Encoder Block 2\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1_residual)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    conv2 = channel_attention(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    swin2 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(pool2)\n    swin2_residual = Add()([pool2, swin2])  # Residual connection after Swin Transformer\n\n    # Encoder Block 3 (Deeper Layer)\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2_residual)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n    conv3 = channel_attention(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    swin3 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(pool3)\n    swin3_residual = Add()([pool3, swin3])  # Residual connection after Swin Transformer\n\n    # Bottleneck (Deepest Layer)\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(swin3_residual)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Conv2D(512, 1, activation='relu', padding='same')(conv4)\n\n    swin_bottleneck = SwinTransformerBlock(dim=512, num_heads=8, shift_size=1)(conv4)\n    swin_bottleneck_residual = Add()([conv4, swin_bottleneck])  # Residual connection after Swin Transformer\n\n    # Decoder Block 1\n    up1 = Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin_bottleneck_residual)\n    up1 = Concatenate()([up1, conv3])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv5 = BatchNormalization()(conv5)\n    conv5 = Conv2D(256, 1, activation='relu', padding='same')(conv5)\n    conv5 = channel_attention(conv5)\n    conv5 = spatial_attention(conv5)\n\n    swin4 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(conv5)\n    swin4_residual = Add()([conv5, swin4])  # Residual connection after Swin Transformer\n\n    # Decoder Block 2\n    up2 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin4_residual)\n    up2 = Concatenate()([up2, conv2])\n    conv6 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv6 = BatchNormalization()(conv6)\n    conv6 = Conv2D(128, 1, activation='relu', padding='same')(conv6)\n    conv6 = channel_attention(conv6)\n    conv6 = spatial_attention(conv6)\n\n    swin5 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(conv6)\n    swin5_residual = Add()([conv6, swin5])  # Residual connection after Swin Transformer\n\n    # Decoder Block 3 (Shallowest Layer)\n    up3 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin5_residual)\n    up3 = Concatenate()([up3, conv1])\n    conv7 = DepthwiseConv2D(3, padding='same', activation='relu')(up3)\n    conv7 = BatchNormalization()(conv7)\n    conv7 = Conv2D(64, 1, activation='relu', padding='same')(conv7)\n    conv7 = channel_attention(conv7)\n    conv7 = spatial_attention(conv7)\n    \n    swin6 = SwinTransformerBlock(dim=64, num_heads=4, shift_size=1)(conv7)\n    swin6_residual = Add()([conv7, swin6])  # Residual connection after Swin Transformer\n\n    # Output Layer\n    output = Conv2D(3, kernel_size=(1, 1), padding='same', activation='sigmoid')(swin6_residual)\n\n    model = Model(inputs=inputs, outputs=output)\n    return model\n\n# Build and compile the model\nmodel = build_simplified_unet_swin(input_shape=(128, 128, 3))\n\n# Load and preprocess datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Adjust to your folder path\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Build and compile the model\nmodel = build_deeper_unet_swin(input_shape=(128, 128, 3))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_absolute_error', metrics=['accuracy'])\n\n# Model Summary\nmodel.summary()\n\n# Callbacks for early stopping and learning rate reduction\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n]\n\n# Training the model\nhistory = model.fit(\n    train_rainy, train_clear,\n    validation_split=0.2,\n    batch_size=1,  # Experiment with batch sizes\n    epochs=1,\n    callbacks=callbacks\n)\n\n# Evaluate model performance on the test set\npredictions = model.predict(test_rainy)\n\n# Calculate PSNR\ndef PSNR(target, prediction):\n    mse = np.mean((target - prediction) ** 2)\n    if mse == 0:  # MSE is zero means no noise is present in the signal.\n        return 100  # Return a high value for PSNR\n    max_pixel = 1.0  # Assuming the pixel value range is [0, 1]\n    return 20 * np.log10(max_pixel / np.sqrt(mse))\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    win_size = 3  \n    data_range = 1.0  \n    \n    for i in range(predictions.shape[0]):\n        psnr_val = PSNR(ground_truth[i], predictions[i])\n        \n        if ground_truth[i].shape[0] < 7 or ground_truth[i].shape[1] < 7:\n            ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, data_range=data_range)\n        else:\n            ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Apply post-processing on the predictions\ndef post_process_images(predictions):\n    processed_images = []\n    for img in predictions:\n        # Bilateral filter for noise reduction\n        img_bilateral = cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n        \n        # Non-local means denoising\n        img_denoised = cv2.fastNlMeansDenoisingColored((img_bilateral * 255).astype(np.uint8), None, 10, 10, 7, 21)\n        \n        processed_images.append(img_denoised / 255.0)  # Normalize back to [0, 1]\n    return np.array(processed_images)\n\n# Post-process the predicted images\nprocessed_images = post_process_images(predictions)\n\n# Display the results after post-processing\ndef display_processed_results(test_rainy, test_clear, predictions):\n    for i in range(5):\n        plt.figure(figsize=(15, 5))\n        plt.subplot(1, 3, 1)\n        plt.title(\"Rainy Image\")\n        plt.imshow(test_rainy[i])\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 2)\n        plt.title(\"Predicted Clear Image (After Post-Processing)\")\n        plt.imshow(predictions[i])  # After post-processing\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 3)\n        plt.title(\"Ground Truth Clear Image\")\n        plt.imshow(test_clear[i])\n        plt.axis('off')\n        \n        plt.show()\n\n# Run the display function after post-processing\ndisplay_processed_results(test_rainy, test_clear, processed_images)\n\n# Calculate PSNR and SSIM using post-processed images\npsnr, ssim = calculate_metrics(processed_images, test_clear)\n\nprint(f'Test PSNR: {psnr:.2f} dB')\nprint(f'Test SSIM: {ssim:.4f}')\n\n# Save the model if needed\nmodel.save('hybrid_unet_swin_deraining_model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-10-04T17:24:03.786925Z","iopub.execute_input":"2024-10-04T17:24:03.787424Z","iopub.status.idle":"2024-10-04T17:27:34.258329Z","shell.execute_reply.started":"2024-10-04T17:24:03.787375Z","shell.execute_reply":"2024-10-04T17:27:34.256412Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 226\u001b[0m\n\u001b[1;32m    223\u001b[0m (train_rainy, train_clear), (test_rainy, test_clear) \u001b[38;5;241m=\u001b[39m load_and_preprocess_datasets(base_folder, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Build and compile the model\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_deeper_unet_swin\u001b[49m(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    227\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Model Summary\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'build_deeper_unet_swin' is not defined"],"ename":"NameError","evalue":"name 'build_deeper_unet_swin' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# More depth unet + swin","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization, Conv2DTranspose, ReLU, Multiply, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, GlobalAveragePooling2D, Reshape\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=5,\n            width_shift_range=0.05,\n            height_shift_range=0.05,\n            shear_range=0.05,\n            zoom_range=[0.95, 1.05],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')  # Add this line to define clear_test_folder\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n\n# Define Channel Attention Mechanism\ndef channel_attention(x, reduction_ratio=16):\n    channel_avg = GlobalAveragePooling2D()(x)\n    channel_avg = Reshape((1, 1, channel_avg.shape[-1]))(channel_avg)\n    channel_avg = Dense(x.shape[-1] // reduction_ratio, activation='relu')(channel_avg)\n    channel_avg = Dense(x.shape[-1], activation='sigmoid')(channel_avg)\n    \n    return Multiply()([x, channel_avg])\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=2, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n# Build the efficient hybrid U-Net with Swin Transformer using Depthwise Convolution\ndef build_efficient_hybrid_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder with Depthwise Convolutions and residual connections\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)\n    conv1 = channel_attention(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    swin1 = SwinTransformerBlock(dim=64, num_heads=4, shift_size=1)(pool1)\n\n    # Add residual connection to pass the feature map forward\n    swin1_residual = Add()([pool1, swin1])\n\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1_residual)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    conv2 = channel_attention(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    swin2 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(pool2)\n\n    # Pass feature map forward with residual connection\n    swin2_residual = Add()([pool2, swin2])\n\n    # Bottleneck with Swin Transformer blocks and residual connections\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2_residual)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n\n    swin3 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(conv3)\n    swin4 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(swin3)\n\n    # Pass feature map forward with residual connection\n    swin4_residual = Add()([conv3, swin4])\n\n    # Decoder with Skip Connections, Depthwise Convolutions, and residual connections\n    up1 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin4_residual)\n    up1 = Concatenate()([up1, conv2])\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv4 = Conv2D(128, 1, activation='relu', padding='same')(conv4)\n    conv4 = channel_attention(conv4)\n\n    swin5 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(conv4)\n\n    # Pass feature map forward with residual connection\n    swin5_residual = Add()([conv4, swin5])\n\n    up2 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin5_residual)\n    up2 = Concatenate()([up2, conv1])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv5 = Conv2D(64, 1, activation='relu', padding='same')(conv5)\n    conv5 = channel_attention(conv5)\n\n    swin6 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=1)(conv5)\n\n    # Final residual connection before output\n    swin6_residual = Add()([conv5, swin6])\n\n    # Output layer with Sigmoid activation\n    outputs = Conv2D(3, 1, activation='sigmoid')(swin6_residual)\n\n    return Model(inputs, outputs)\n\n# Instantiate and compile the model\nmodel = build_efficient_hybrid_unet_swin()\nmodel.summary()\n\n# Using L1 Loss\nmodel.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    win_size = 3  \n    data_range = 1.0  \n    \n    for i in range(predictions.shape[0]):\n        psnr_val = PSNR(ground_truth[i], predictions[i])\n        ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Function to train the model\ndef train_model(model, rainy_images_train, clear_images_train, rainy_images_test, clear_images_test, epochs=5, batch_size=2):\n    # Compile the model using Adam optimizer and L1 Loss\n    model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(\n        rainy_images_train, clear_images_train,\n        validation_data=(rainy_images_test, clear_images_test),\n        epochs=epochs,\n        batch_size=batch_size\n    )\n    \n    # Plot training history (loss and accuracy)\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n    return model, history\n\n# Visualize predictions and ground truth images\ndef visualize_predictions(model, rainy_images_test, clear_images_test, num_samples=5):\n    predictions = model.predict(rainy_images_test[:num_samples])\n\n    for i in range(num_samples):\n        plt.figure(figsize=(15, 5))\n        \n        plt.subplot(1, 3, 1)\n        plt.title('Rainy Image')\n        plt.imshow((rainy_images_test[i] + 1) / 2)  # Scaling back from [-1,1] to [0,1]\n        \n        plt.subplot(1, 3, 2)\n        plt.title('Ground Truth (Clear)')\n        plt.imshow((clear_images_test[i] + 1) / 2)  # Scaling back from [-1,1] to [0,1]\n        \n        plt.subplot(1, 3, 3)\n        plt.title('Predicted Clear Image')\n        plt.imshow((predictions[i] + 1) / 2)  # Scaling back from [-1,1] to [0,1]\n        \n        plt.show()\n\n# Load and preprocess datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Update this path to your dataset location\n(rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Train the model\nmodel, history = train_model(model, rainy_images_train, clear_images_train, rainy_images_test, clear_images_test, epochs=5, batch_size=2)\n\n# Visualize predictions on test data\nvisualize_predictions(model, rainy_images_test, clear_images_test, num_samples=5)\n\n# Calculate PSNR and SSIM metrics for the test dataset\npredictions = model.predict(rainy_images_test)\npsnr, ssim = calculate_metrics(predictions, clear_images_test)\nprint(f\"Average PSNR: {psnr}, Average SSIM: {ssim}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:32:10.635325Z","iopub.execute_input":"2024-10-02T09:32:10.636065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unet with attenton + skip connection + dropout with swin","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, GlobalAveragePooling2D, Reshape, Multiply\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=10,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.1,\n            zoom_range=[0.9, 1.1],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False, batch_size=1):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Define Channel Attention Mechanism\ndef channel_attention(x, reduction_ratio=16):\n    channel_avg = GlobalAveragePooling2D()(x)\n    channel_avg = Reshape((1, 1, channel_avg.shape[-1]))(channel_avg)\n    channel_avg = Dense(x.shape[-1] // reduction_ratio, activation='relu')(channel_avg)\n    channel_avg = Dense(x.shape[-1], activation='sigmoid')(channel_avg)\n    \n    return Multiply()([x, channel_avg])\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=2, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n# Build the efficient hybrid U-Net with Swin Transformer using Depthwise Convolution\ndef build_efficient_hybrid_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder with Depthwise Convolutions\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)\n    conv1 = channel_attention(conv1)  # Channel attention\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    swin1 = SwinTransformerBlock(dim=64, num_heads=4, shift_size=1)(pool1)\n\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    conv2 = channel_attention(conv2)  # Channel attention\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    swin2 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(pool2)\n\n    # Bottleneck\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n\n    # Third Swin Transformer Block in bottleneck\n    swin3 = SwinTransformerBlock(dim=256, num_heads=4, shift_size=1)(conv3)\n\n    # Decoder with Skip Connections and Depthwise Convolutions\n    up1 = Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin3)  # Learned upsampling\n    up1 = Concatenate()([up1, conv2])\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv4 = Conv2D(128, 1, activation='relu', padding='same')(conv4)\n    conv4 = channel_attention(conv4)  # Channel attention\n\n    swin4 = SwinTransformerBlock(dim=128, num_heads=4, shift_size=1)(conv4)\n\n    up2 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')(swin4)  # Learned upsampling\n    up2 = Concatenate()([up2, conv1])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv5 = Conv2D(64, 1, activation='relu', padding='same')(conv5)\n    conv5 = channel_attention(conv5)  # Channel attention\n\n    swin5 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=1)(conv5)\n\n    # Output layer with Tanh activation\n    outputs = Conv2D(3, 1, activation='tanh')(swin5)\n\n    return Model(inputs, outputs)\n\n# Instantiate and compile the model\nmodel = build_efficient_hybrid_unet_swin()\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    win_size = 3  \n    data_range = 1.0  \n    \n    for i in range(predictions.shape[0]):\n        psnr_val = PSNR(ground_truth[i], predictions[i])\n        \n        if ground_truth[i].shape[0] < 7 or ground_truth[i].shape[1] < 7:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        else:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Function to train the model\ndef train_model(model, rainy_images_train, clear_images_train, epochs=5, batch_size=1):\n    history = model.fit(rainy_images_train, clear_images_train, \n                        epochs=epochs, \n                        batch_size=batch_size, \n                        validation_split=0.1)\n    return history\n\n# Function to visualize predictions\ndef visualize_predictions(model, rainy_images_test, clear_images_test):\n    predictions = model.predict(rainy_images_test, batch_size = 1)\n    psnr, ssim = calculate_metrics(predictions, clear_images_test)\n\n    print(f'Test PSNR: {psnr:.2f} dB')\n    print(f'Test SSIM: {ssim:.4f}')\n\n    # Plotting results\n    plt.figure(figsize=(15, 5))\n    for i in range(5):\n        plt.subplot(3, 5, i + 1)\n        plt.imshow((rainy_images_test[i] + 1) / 2)  # Rescale back to [0, 1]\n        plt.title('Rainy Image')\n        plt.axis('off')\n\n        plt.subplot(3, 5, i + 6)\n        plt.imshow((clear_images_test[i] + 1) / 2)  # Rescale back to [0, 1]\n        plt.title('Ground Truth')\n        plt.axis('off')\n\n        plt.subplot(3, 5, i + 11)\n        plt.imshow((predictions[i] + 1) / 2)  # Rescale back to [0, 1]\n        plt.title('Predicted Image')\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Load and preprocess the datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Update with the actual path\n(rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Train the model\nhistory = train_model(model, rainy_images_train, clear_images_train, epochs=5, batch_size=1)\n\n# Visualize predictions on test data\nvisualize_predictions(model, rainy_images_test, clear_images_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T05:23:12.780970Z","iopub.execute_input":"2024-10-02T05:23:12.781250Z","iopub.status.idle":"2024-10-02T07:17:37.382257Z","shell.execute_reply.started":"2024-10-02T05:23:12.781217Z","shell.execute_reply":"2024-10-02T07:17:37.381372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Updated mix model (unet + swin)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as PSNR\nfrom skimage.metrics import structural_similarity as SSIM\nfrom skimage import io, img_as_float\nimport glob  # Import glob to handle file loading\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Function to load and preprocess images from a folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data augmentation and preprocessing pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=10,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.1,\n            zoom_range=[0.9, 1.1],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Optimized dataset loading using tf.data API\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False, batch_size=2):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Define Swin Transformer Block with Shift Size and Multi-Head Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size=8, shift_size=2, mlp_ratio=4.0, dropout_rate=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dropout(dropout_rate),\n            Dense(dim)\n        ])\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n    def call(self, x):\n        # Apply attention with shift size\n        attn_out = self.attn(x, x)\n        x = self.norm1(attn_out + x)\n        mlp_out = self.mlp(x)\n        return self.norm2(mlp_out + x)\n\n\n# Build the efficient hybrid U-Net with Swin Transformer using Depthwise Convolution\ndef build_efficient_hybrid_unet_swin(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Encoder with Depthwise Convolutions (Parameter reduction)\n    conv1 = DepthwiseConv2D(3, padding='same', activation='relu')(inputs)\n    conv1 = Conv2D(64, 1, activation='relu', padding='same')(conv1)  # Pointwise convolution\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    # Swin Transformer Block for Global Feature Extraction\n    swin1 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=2)(pool1)\n\n    # Encoder continued with Depthwise Convolutions\n    conv2 = DepthwiseConv2D(3, padding='same', activation='relu')(swin1)\n    conv2 = Conv2D(128, 1, activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Second Swin Transformer Block\n    swin2 = SwinTransformerBlock(dim=128, num_heads=2, shift_size=2)(pool2)\n\n    # Bottleneck\n    conv3 = DepthwiseConv2D(3, padding='same', activation='relu')(swin2)\n    conv3 = Conv2D(256, 1, activation='relu', padding='same')(conv3)\n\n    # Third Swin Transformer Block in bottleneck\n    swin3 = SwinTransformerBlock(dim=256, num_heads=2, shift_size=2)(conv3)\n\n    # Decoder with Skip Connections and Depthwise Convolutions\n    up1 = UpSampling2D(size=(2, 2))(swin3)\n    up1 = Concatenate()([up1, conv2])\n    conv4 = DepthwiseConv2D(3, padding='same', activation='relu')(up1)\n    conv4 = Conv2D(128, 1, activation='relu', padding='same')(conv4)\n\n    # Fourth Swin Transformer Block\n    swin4 = SwinTransformerBlock(dim=128, num_heads=2, shift_size=2)(conv4)\n\n    up2 = UpSampling2D(size=(2, 2))(swin4)\n    up2 = Concatenate()([up2, conv1])\n    conv5 = DepthwiseConv2D(3, padding='same', activation='relu')(up2)\n    conv5 = Conv2D(64, 1, activation='relu', padding='same')(conv5)\n\n    # Fifth Swin Transformer Block\n    swin5 = SwinTransformerBlock(dim=64, num_heads=2, shift_size=2)(conv5)\n\n    # Output layer with Tanh activation\n    outputs = Conv2D(3, 1, activation='tanh')(swin5)\n\n    return Model(inputs, outputs)\n\n# Instantiate and compile the model\nmodel = build_efficient_hybrid_unet_swin()\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    # Define a smaller window size if images are less than 7x7\n    win_size = 3  # Use a smaller window size if necessary\n    \n    # Determine the data range\n    data_range = 1.0  # Assuming images are normalized between [0, 1]\n    \n    for i in range(predictions.shape[0]):\n        psnr_val = PSNR(ground_truth[i], predictions[i])  # Use PSNR instead of psnr\n        \n        # Ensure the window size is appropriate for the image dimensions\n        if ground_truth[i].shape[0] < 7 or ground_truth[i].shape[1] < 7:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)  # Use SSIM instead of ssim\n        else:\n            ssim_val = SSIM(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n\n# Training function\ndef train_model(model, train_data, train_labels, epochs=1, batch_size=2):\n    history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n    return history\n\n# Load the dataset\nbase_folder = \"/kaggle/input/derainingdata/RainData\"  # Update with your actual dataset path\n(train_data, train_labels), (test_data, test_labels) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Train the model\nhistory = train_model(model, train_data, train_labels, epochs=1)\n\n# Predict and display results\npredictions = model.predict(test_data, batch_size = 1)\npredictions = (predictions + 1) / 2  # Scale predictions back to [0, 1]\n\n# Show prediction vs ground truth\ndef display_predictions(test_data, predictions, ground_truth, index=0):\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.title('Input')\n    plt.imshow((test_data[index] + 1) / 2)  # Scale back to [0, 1]\n    plt.subplot(1, 3, 2)\n    plt.title('Prediction')\n    plt.imshow(predictions[index])\n    plt.subplot(1, 3, 3)\n    plt.title('Ground Truth')\n    plt.imshow((ground_truth[index] + 1) / 2)  # Scale back to [0, 1]\n    plt.show()\n\n# Test and display metrics for an example image\nfor i in range(5):  # Test and display first 5 images\n    display_predictions(test_data, predictions, test_labels, index=i)\n\n # Calculate PSNR and SSIM\nmean_psnr, mean_ssim = calculate_metrics(predictions, test_labels)\nprint(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")    \n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:20:49.930207Z","iopub.execute_input":"2024-10-01T14:20:49.930617Z","iopub.status.idle":"2024-10-01T14:33:19.966347Z","shell.execute_reply.started":"2024-10-01T14:20:49.930579Z","shell.execute_reply":"2024-10-01T14:33:19.965111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unet + official swin","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage import io, img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = io.imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, shift_size):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        \n        # Layers for the transformer block\n        self.attn = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.dim)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        \n        # Feedforward network (FFN)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.dim * 2, activation='gelu'),  # Reduced dimensionality\n            tf.keras.layers.Dense(self.dim)\n        ])\n\n    def call(self, x):\n        attn_output = self.attn(x, x)\n        x = self.layernorm1(x + attn_output)\n        ffn_output = self.ffn(x)\n        return self.layernorm2(x + ffn_output)\n\nclass AggregationLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(AggregationLayer, self).__init__()\n\n    def call(self, skip_connections):\n        return tf.reduce_mean(tf.stack(skip_connections), axis=0)\n\ndef build_swin_transformer(input_shape=(128, 128, 3), num_blocks=3):\n    inputs = Input(shape=input_shape)\n    x = Conv2D(32, kernel_size=3, padding='same')(inputs)  # Reduced channels\n    skip_connections = []  # To store skip connection outputs\n\n    # Using the Swin Transformer block specified number of times\n    for _ in range(num_blocks):\n        x = SwinTransformerBlock(dim=32, num_heads=2, window_size=8, shift_size=0)(x)\n        skip_connections.append(x)\n\n    # Use Aggregation Layer to combine skip connections\n    aggregated_features = AggregationLayer()(skip_connections)\n    \n    # Print shape for debugging\n    print(f\"Shape after aggregation: {aggregated_features.shape}\")  # Debug print\n    \n    # Add a Conv2D layer to reduce channels from 32 to 3\n    x = Conv2D(3, kernel_size=1, padding='same')(aggregated_features)\n    \n    # Print shape for debugging\n    print(f\"Shape before returning model: {x.shape}\")  # Debug print\n    \n    return Model(inputs, x)\n\n# Define the U-Net model\ndef build_unet(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n    \n    # Encoder\n    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)  # Reduced channels\n    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    # Bottleneck\n    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n\n    # Decoder\n    up6 = UpSampling2D(size=(2, 2))(conv5)\n    up6 = Concatenate()([up6, conv4])\n    conv6 = Conv2D(256, 3, activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n\n    up7 = UpSampling2D(size=(2, 2))(conv6)\n    up7 = Concatenate()([up7, conv3])\n    conv7 = Conv2D(128, 3, activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n\n    up8 = UpSampling2D(size=(2, 2))(conv7)\n    up8 = Concatenate()([up8, conv2])\n    conv8 = Conv2D(64, 3, activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n\n    up9 = UpSampling2D(size=(2, 2))(conv8)\n    up9 = Concatenate()([up9, conv1])\n    conv9 = Conv2D(32, 3, activation='relu', padding='same')(up9)  # Reduced channels\n    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n\n    outputs = Conv2D(3, 1, activation='tanh')(conv9)  # Adjust activation if necessary\n    \n    return Model(inputs, outputs)\n\n# Main function\ndef main():\n    # Load datasets\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n    (rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n    # Print shapes of the training data\n    print(f\"Rainy train shape: {rainy_train.shape}\")\n    print(f\"Clear train shape: {clear_train.shape}\")\n\n    # Define models\n    swin_model = build_swin_transformer()\n    unet_model = build_unet()\n\n    # Combine models\n    inputs = Input(shape=(128, 128, 3))\n    swin_output = swin_model(inputs)\n    outputs = unet_model(swin_output)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])\n\n    # Model summary\n    model.summary()\n\n    # Define callbacks\n    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n    early_stopping = EarlyStopping(patience=10, monitor='val_loss')\n\n    # Train the model\n    history = model.fit(rainy_train, clear_train,\n                        validation_split=0.1,\n                        epochs=5,\n                        batch_size=1,  # Adjust batch size if necessary\n                        callbacks=[checkpoint, early_stopping])\n\n    # Evaluate the model on test data\n    test_loss, test_accuracy = model.evaluate(rainy_test, clear_test)\n    print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n\n    # Visualize some predictions\n    predictions = model.predict(test_rainy, batch_size = 1)\n    \n    plt.figure(figsize=(15, 5))\n    for i in range(5):\n        plt.subplot(2, 5, i + 1)\n        plt.imshow((rainy_test[i] + 1) / 2)  # Convert back to [0, 1]\n        plt.axis('off')\n        plt.title('Rainy Image')\n\n        plt.subplot(2, 5, i + 6)\n        plt.imshow((predictions[i] + 1) / 2)  # Convert back to [0, 1]\n        plt.axis('off')\n        plt.title('Predicted Image')\n        \n    plt.show()\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:14:56.618332Z","iopub.execute_input":"2024-09-30T21:14:56.618814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unet + window based swin (not working)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG19\nimport os\nimport glob\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer, Conv2D, LayerNormalization, Dense, Dropout, MultiHeadAttention, Input\n\nK.clear_session()\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n\n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Global variable for VGG model\nvgg = VGG19(include_top=False, input_shape=(128, 128, 3))\nvgg.trainable = False\nvgg_feat_extractor = models.Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n\n# Perceptual Loss Function\ndef perceptual_loss(y_true, y_pred):\n    true_features = vgg_feat_extractor(y_true)\n    pred_features = vgg_feat_extractor(y_pred)\n    return tf.reduce_mean(tf.square(true_features - pred_features))\n\n# SSIM Loss\ndef ssim_loss(y_true, y_pred):\n    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n\n# Total Variation Loss (for sharpness)\ndef total_variation_loss(y_pred):\n    return tf.image.total_variation(y_pred)\n\n# Combined loss function to incorporate perceptual, SSIM, and total variation losses\ndef combined_loss(y_true, y_pred):\n    perceptual = perceptual_loss(y_true, y_pred)\n    ssim_loss_val = ssim_loss(y_true, y_pred)\n    tv_loss = total_variation_loss(y_pred)\n\n    # Combine losses with different weights\n    return 0.8 * perceptual + 0.1 * ssim_loss_val + 0.1 * tv_loss\n\n# Define the MultiHeadSelfAttention Layer if not already defined\nclass MultiHeadSelfAttention(Layer):\n    def __init__(self, num_heads, key_dim):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n\n    def call(self, queries, keys):\n        return self.attention(queries, keys)\n\n# Define the MLP Layer if not already defined\nclass MLP(Layer):\n    def __init__(self, dim):\n        super(MLP, self).__init__()\n        self.fc1 = Dense(dim * 4, activation='gelu')\n        self.fc2 = Dense(dim)\n\n    def call(self, x):\n        return self.fc2(Dropout(0.1)(self.fc1(x)))\n\n# Define window partition and reverse functions\ndef window_partition(x, window_size):\n    B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n    x = tf.reshape(x, (B, H // window_size, window_size, W // window_size, window_size, C))\n    windows = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n    return tf.reshape(windows, (-1, window_size, window_size, C)), H, W, 0, 0  # Padding values as 0 for now\n\ndef window_reverse(windows, H_padded, W_padded, pad_h, pad_w, window_size):\n    B = tf.shape(windows)[0]\n    x = tf.reshape(windows, (B, H_padded // window_size, W_padded // window_size, window_size, window_size, -1))\n    x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n    return tf.reshape(x, (B, H_padded, W_padded, -1))[:, :H_padded - pad_h, :W_padded - pad_w, :]\n\n# Define the SwinTransformerBlock\nclass SwinTransformerBlock(Layer):\n    def __init__(self, dim, num_heads, window_size):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        \n        self.attn = MultiHeadSelfAttention(num_heads=num_heads, key_dim=dim)\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        self.mlp = MLP(dim)\n\n    def build(self, input_shape):\n        # Build the attention layer to create variables based on input shape\n        self.attn.build(input_shape)\n        super(SwinTransformerBlock, self).build(input_shape)\n\n    def call(self, x):\n        # Print input shape for debugging\n        print(\"Input shape before window partition:\", x.shape)\n\n        windows, H_padded, W_padded, pad_h, pad_w = window_partition(x, self.window_size)\n\n        # Print shape after window partition\n        print(\"Shape after window partitioning:\", windows.shape)\n\n        # Ensure windows shape is correct before applying attention\n        if tf.shape(windows)[2] is None or tf.shape(windows)[3] is None:\n            raise ValueError(\"Window dimensions must be defined\")\n\n        # Apply attention\n        windows_attn = self.attn(windows, windows)\n        windows_attn = self.norm1(windows_attn + windows)  # Add & Norm\n\n        # Print shape after attention\n        print(\"Shape after attention:\", windows_attn.shape)\n\n        # Reverse window partition\n        x = window_reverse(windows_attn, H_padded, W_padded, pad_h, pad_w, self.window_size)\n\n        # Print shape after window reversing\n        print(\"Shape after window reversing:\", x.shape)\n\n        x = self.norm2(x)  # Add & Norm\n        return x\n\n# Example U-Net + Swin Transformer Hybrid Model\ndef build_hybrid_model(input_shape=(128, 128, 3)):\n    inputs = Input(shape=input_shape)\n\n    # Initial Conv Layer\n    x = Conv2D(64, (3, 3), padding='same')(inputs)\n    print(f\"After initial Conv, shape: {x.shape}\")\n\n    # Add 12 Swin Transformer blocks with alternating window sizes\n    for i in range(12):\n        window_size = 8 if i % 2 == 0 else 7  # Alternate window size\n        x = SwinTransformerBlock(dim=64, num_heads=4, window_size=window_size)(x)\n        print(f\"After Swin Transformer Block {i + 1}, shape: {x.shape}\")  # Debugging output\n\n\n    # U-Net Encoder\n    encoder_output = []\n    for filters in [64, 128, 256]:\n        x = layers.Conv2D(filters, kernel_size=3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        encoder_output.append(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        print(f\"After Encoder Conv, shape: {x.shape}\")  # Debugging output\n\n    # Bottleneck\n    x = layers.Conv2D(512, kernel_size=3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    print(f\"After Bottleneck, shape: {x.shape}\")  # Debugging output\n\n    # U-Net Decoder\n    for filters in [256, 128, 64]:\n        x = layers.Conv2DTranspose(filters, kernel_size=2, strides=2, padding='same')(x)\n        x = layers.concatenate([x, encoder_output.pop()])\n        x = layers.Conv2D(filters, kernel_size=3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        print(f\"After Decoder Conv, shape: {x.shape}\")  # Debugging output\n\n    outputs = layers.Conv2D(3, kernel_size=1, activation='sigmoid')(x)  # Output layer\n    model = models.Model(inputs, outputs)\n    return model\n\n# Load datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Set your dataset path here\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Build model\nmodel = build_hybrid_model()\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss=combined_loss, metrics=[psnr])\n\n# Train the model\nhistory = model.fit(train_rainy, train_clear, validation_data=(test_rainy, test_clear), epochs=50, batch_size=8)\n\n# Evaluate PSNR and SSIM metrics\ndef evaluate_model(model, test_rainy, test_clear):\n    predictions = model.predict(test_rainy)\n    psnr_values = []\n    ssim_values = []\n    \n    for pred, true in zip(predictions, test_clear):\n        psnr_val = psnr(true, pred)\n        ssim_val = ssim(true, pred, multichannel=True)\n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n\n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Evaluate the model\naverage_psnr, average_ssim = evaluate_model(model, test_rainy, test_clear)\nprint(f'Average PSNR: {average_psnr:.2f} dB, Average SSIM: {average_ssim:.4f}')\n\n# Show output images\ndef show_images(original, predicted, ground_truth, num=5):\n    plt.figure(figsize=(15, 5))\n    for i in range(num):\n        plt.subplot(3, num, i + 1)\n        plt.imshow(original[i] * 0.5 + 0.5)  # Rescale to [0, 1]\n        plt.title('Original (Rainy)')\n        plt.axis('off')\n        \n        plt.subplot(3, num, i + 1 + num)\n        plt.imshow(predicted[i] * 0.5 + 0.5)\n        plt.title('Predicted')\n        plt.axis('off')\n\n        plt.subplot(3, num, i + 1 + 2 * num)\n        plt.imshow(ground_truth[i] * 0.5 + 0.5)\n        plt.title('Ground Truth')\n        plt.axis('off')\n\n    plt.show()\n\n# Display output images\npredicted_images = model.predict(test_rainy)\nshow_images(test_rainy, predicted_images, test_clear)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More Revised Unet + swin","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG19\nimport os\nimport glob\nfrom tensorflow.keras import backend as K\n\nK.clear_session()\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        \n        # Scale the images to [-1, 1]\n        img_resized = img_resized * 2 - 1\n        images.append(img_resized)\n    return np.array(images)\n\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L']  # , 'Rain200H' Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Global variable for VGG model\nvgg = VGG19(include_top=False, input_shape=(128, 128, 3))\nvgg.trainable = False\nvgg_feat_extractor = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n\n# Perceptual Loss Function\ndef perceptual_loss(y_true, y_pred):\n    true_features = vgg_feat_extractor(y_true)\n    pred_features = vgg_feat_extractor(y_pred)\n    return tf.reduce_mean(tf.square(true_features - pred_features))\n\n# SSIM Loss\ndef ssim_loss(y_true, y_pred):\n    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n\n# Total Variation Loss (for sharpness)\ndef total_variation_loss(y_pred):\n    return tf.image.total_variation(y_pred)\n\n# Combined loss function to incorporate perceptual, SSIM, and total variation losses\ndef combined_loss(y_true, y_pred):\n    perceptual = perceptual_loss(y_true, y_pred)\n    ssim_loss_val = ssim_loss(y_true, y_pred)\n    tv_loss = total_variation_loss(y_pred)\n    \n    # Combine losses with different weights\n    return 0.8 * perceptual + 0.1 * ssim_loss_val + 0.1 * tv_loss\n\n\n# Swin Transformer block implementation\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.1):\n        super(SwinTransformerBlock, self).__init__()\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim, dropout=dropout)\n\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='relu'),\n            layers.Dense(dim)\n        ])\n\n    def call(self, x):\n        h = self.norm1(x)\n        attn_output = self.attn(h, h)\n        x = x + attn_output\n        h = self.norm2(x)\n        x = x + self.mlp(h)\n        return x\n\n# Swin Transformer Layer\nclass SwinTransformerLayer(layers.Layer):\n    def __init__(self, dim, depth, num_heads, mlp_ratio=4., dropout=0.1):\n        super(SwinTransformerLayer, self).__init__()\n        self.blocks = [SwinTransformerBlock(dim, num_heads, mlp_ratio, dropout) for _ in range(depth)]\n\n    def call(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n# Swin Transformer Block (with variable depths)\ndef swin_transformer_block(input_tensor, num_heads, embed_dim, depth):\n    height, width = input_tensor.shape[1], input_tensor.shape[2]  \n    embed_layer = layers.Conv2D(embed_dim, kernel_size=4, strides=2, padding='same')(input_tensor)\n    reshaped_layer = layers.Reshape((height // 2 * width // 2, embed_dim))(embed_layer)\n\n    swin_layer = SwinTransformerLayer(embed_dim, depth, num_heads)\n    transformed_layer = swin_layer(reshaped_layer)\n\n    output_layer = layers.Reshape((height // 2, width // 2, embed_dim))(transformed_layer)\n    return output_layer\n\n# U-Net Encoder Block\ndef unet_encoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    return x\n\n# U-Net Decoder Block\ndef unet_decoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    return x\n\n# Hybrid U-Net + Swin Transformer Model\ndef unet_swin_transformer(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder\n    enc1 = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n    swin1 = swin_transformer_block(enc1, num_heads=4, embed_dim=64, depth=6)  # Swin1: 4 times\n    pool1 = layers.Conv2D(128, (3, 3), strides=2, padding='same')(swin1)\n\n    enc2 = unet_encoder_block(pool1, 128)\n    swin2 = swin_transformer_block(enc2, num_heads=4, embed_dim=128, depth=4)  # Swin2: 2 times\n    pool2 = layers.Conv2D(256, (3, 3), strides=2, padding='same')(swin2)\n\n    enc3 = unet_encoder_block(pool2, 256)\n    swin3 = swin_transformer_block(enc3, num_heads=4, embed_dim=256, depth=2)  # Swin3: 1 time\n    pool3 = layers.Conv2D(512, (3, 3), strides=2, padding='same')(swin3)\n\n    # Bottleneck\n    bottleneck = unet_encoder_block(pool3, 512)\n\n    # Decoder with Skip Connections\n    dec3 = unet_decoder_block(bottleneck, 256)\n    dec3 = layers.UpSampling2D(size=(2, 2))(dec3)  # Upsample to match swin3 (16x16)\n    dec3 = layers.Concatenate()([dec3, swin3])\n\n    dec2 = unet_decoder_block(dec3, 128)\n    dec2 = layers.UpSampling2D(size=(2, 2))(dec2)  # Upsample to match swin2 (32x32)\n    dec2 = layers.UpSampling2D(size=(2, 2))(dec2)\n    dec2 = layers.Concatenate()([dec2, swin2])\n\n    dec1 = unet_decoder_block(dec2, 64)\n    dec1 = layers.UpSampling2D(size=(2, 2))(dec1)  # Upsample to match swin1 (64x64)\n    dec1 = layers.UpSampling2D(size=(2, 2))(dec1)\n    dec1 = layers.Concatenate()([dec1, swin1])\n    \n    print(\"Shape of dec1:\", dec1.shape)\n    \n    outputs = layers.UpSampling2D(size=(2, 2))(dec1)  # Upsample from (64, 64) to (128, 128)\n    outputs = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(outputs)  # Optional Conv2D for refinement\n\n    outputs = layers.Conv2D(3, (1, 1), activation='tanh')(outputs)\n    print(\"Shape of outputs:\", outputs.shape)\n    \n    return Model(inputs, outputs)\n\n\n# Compile the model\ninput_shape = (128, 128, 3)\nmodel = unet_swin_transformer(input_shape)\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss=combined_loss, metrics=['mae'])\n\n# Model summary to check the layer dimensions\nmodel.summary()\n\n# Loading datasets\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets('/kaggle/input/derainingdata/RainData', augment=True)\n\n# Train the model\nmodel.fit(train_rainy, train_clear, batch_size=2, epochs=1, validation_data=(test_rainy, test_clear))\n\n# Evaluate model performance on testing set\npredictions = model.predict(test_rainy, batch_size = 1)\nfor i in range(3):  # Display 3 examples\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow((test_rainy[i] + 1) / 2)  # Convert back to [0, 1]\n    plt.title(\"Input\")\n    plt.subplot(1, 3, 2)\n    plt.imshow((predictions[i] + 1) / 2)  # Convert back to [0, 1]\n    plt.title(\"Prediction\")\n    plt.subplot(1, 3, 3)\n    plt.imshow((test_clear[i] + 1) / 2)  # Convert back to [0, 1]\n    plt.title(\"Ground Truth\")\n    plt.show()\n\n# Calculate PSNR and SSIM metrics for the evaluation\nfor i in range(len(test_rainy)):\n    pred_img = (predictions[i] + 1) / 2  # Convert to [0, 1]\n    gt_img = (test_clear[i] + 1) / 2  # Convert to [0, 1]\n    \n    psnr_value = psnr(gt_img, pred_img)\n    ssim_value = ssim(gt_img, pred_img, multichannel=True)\n    print(f\"Image {i+1}: PSNR = {psnr_value}, SSIM = {ssim_value}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# Revised Unet + swin ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport glob\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)  # Convert image to float in [0, 1]\n        img_resized = tf.image.resize(img, size).numpy()\n        # Remove normalization if img_as_float is already in [0, 1]\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Swin Transformer block implementation\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n\n        # Layer normalization\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        # Multi-head self-attention layer\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim, dropout=dropout)\n\n        # MLP layer\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='relu'),\n            layers.Dense(dim)\n        ])\n\n    def call(self, x):\n        h = self.norm1(x)  # Apply normalization for attention\n        attn_output = self.attn(h, h)  # Self-attention\n\n        # Residual connection\n        x = x + attn_output\n\n        h = self.norm2(x)  # Apply normalization for MLP\n        x = x + self.mlp(h)  # Residual connection for MLP\n        return x\n\n# Window-based Swin Transformer Layer (basic building block)\nclass SwinTransformerLayer(layers.Layer):\n    def __init__(self, dim, depth, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerLayer, self).__init__()\n        self.blocks = [SwinTransformerBlock(dim, num_heads, mlp_ratio, dropout) for _ in range(depth)]\n\n    def call(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n# Window-based Swin Transformer block applied after each downsampling\ndef swin_transformer_block(input_tensor, num_heads, embed_dim, depths):\n    height, width = input_tensor.shape[1], input_tensor.shape[2]  # Keras tensor shape\n    embed_layer = layers.Conv2D(embed_dim, kernel_size=4, strides=2, padding='same')(input_tensor)  # Reduce spatial size\n    print(f\"After embedding: {embed_layer.shape}\")  # Print dimension after embedding\n\n    # Reshape for Swin Transformer: Batch, Height * Width, Channels\n    reshaped_layer = layers.Reshape((height // 2 * width // 2, embed_dim))(embed_layer)\n\n    # Apply Swin Transformer layers (recursive depth)\n    swin_layer = SwinTransformerLayer(embed_dim, depths[0], num_heads)\n    transformed_layer = swin_layer(reshaped_layer)\n\n    # Reshape back to original spatial dimensions\n    output_layer = layers.Reshape((height // 2, width // 2, embed_dim))(transformed_layer)\n    print(f\"Output of Swin block: {output_layer.shape}\")  # Print dimension after Swin Transformer block\n\n    return output_layer\n\n# U-Net Encoder Block\ndef unet_encoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    print(f\"After encoder block with {filters} filters: {x.shape}\")  # Print dimension after encoder block\n    return x\n\n# U-Net Decoder Block\ndef unet_decoder_block(input_tensor, filters):\n    x = layers.Conv2D(filters, (3, 3), padding='same')(input_tensor)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.ReLU()(x)\n    print(f\"After decoder block with {filters} filters: {x.shape}\")  # Print dimension after decoder block\n    return x\n\n# Hybrid U-Net + Swin Transformer Model\ndef unet_swin_transformer(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder (with multi-scale feature extraction)\n    enc1 = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n    swin1 = swin_transformer_block(enc1, num_heads=4, embed_dim=64, depths=[2])\n    pool1 = layers.Conv2D(128, (3, 3), strides=2, padding='same')(swin1)\n    print(f\"After first pooling: {pool1.shape}\")  # Print dimension after first pooling\n\n    enc2 = unet_encoder_block(pool1, 128)\n    swin2 = swin_transformer_block(enc2, num_heads=4, embed_dim=128, depths=[2])\n    pool2 = layers.Conv2D(256, (3, 3), strides=2, padding='same')(swin2)\n    print(f\"After second pooling: {pool2.shape}\")  # Print dimension after second pooling\n\n    enc3 = unet_encoder_block(pool2, 256)\n    swin3 = swin_transformer_block(enc3, num_heads=4, embed_dim=256, depths=[2])\n    pool3 = layers.Conv2D(512, (3, 3), strides=2, padding='same')(swin3)\n    print(f\"After third pooling: {pool3.shape}\")  # Print dimension after third pooling\n\n    # Bottleneck\n    bottleneck = unet_encoder_block(pool3, 512)\n    print(f\"After bottleneck: {bottleneck.shape}\")  # Print dimension after bottleneck\n\n    # Decoder\n    dec3 = unet_decoder_block(bottleneck, 256)\n    dec3 = layers.UpSampling2D(size=(2, 2))(dec3)  # Upsample to match swin3\n    print(f\"After upsampling dec3 to match swin3: {dec3.shape}\")  # Print upsampled dimension\n    dec3 = layers.Concatenate()([dec3, swin3])  # Concatenate with skip connection from swin3\n    print(f\"After concatenation with skip connection from swin3: {dec3.shape}\")  # Print dimension after concatenation\n\n    dec2 = unet_decoder_block(dec3, 128)\n    dec2 = layers.UpSampling2D(size=(2, 2))(dec2)  # Upsample to match swin2\n    print(f\"After upsampling dec2 to match swin2: {dec2.shape}\")  # Print upsampled dimension\n\n    # Downsample swin2 to match dec2's shape\n    swin2_downsampled = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(swin2)\n    print(f\"After downsampling swin2: {swin2_downsampled.shape}\")  # Print downsampled swin2 shape\n    dec2 = layers.Concatenate()([dec2, swin2_downsampled])  # Concatenate with skip connection from downsampled swin2\n    print(f\"After concatenation with skip connection from swin2: {dec2.shape}\")  # Print dimension after concatenation\n\n    dec1 = unet_decoder_block(dec2, 64)\n    dec1 = layers.UpSampling2D(size=(2, 2))(dec1)  # Upsample to match swin1\n    print(f\"After upsampling dec1 to match swin1: {dec1.shape}\")  # Print upsampled dimension\n\n    # Upsample swin1 to match dec1's shape\n    swin1_downsampled = layers.Conv2D(64, kernel_size=3, strides=(4, 4), padding='same')(swin1)\n    #swin1_upsampled = layers.UpSampling2D(size=(2, 2))(swin1_upsampled)  # Ensure the dimensions match after upsampling\n    print(f\"After upsampling swin1: {swin1_downsampled.shape}\")  # Print upsampled swin1 shape\n    dec1 = layers.Concatenate()([dec1, swin1_downsampled])  # Concatenate with skip connection from upsampled swin1\n    print(f\"After concatenation with skip connection from swin1: {dec1.shape}\")  # Print dimension after concatenation\n    \n    upsampled = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same')(dec1)\n    upsampled = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same')(upsampled)\n    upsampled = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same')(upsampled)\n\n\n    # Output layer (assuming 3-channel RGB output)\n    outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(upsampled)\n    print(f\"Output layer shape: {outputs.shape}\")  # Print output shape\n\n    return Model(inputs, outputs)\n\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Replace with your dataset path\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Instantiate and compile model\ninput_shape = (128, 128, 3)  # Assuming input images are RGB\nmodel = unet_swin_transformer(input_shape)\nmodel.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])\n\n# Model summary\nmodel.summary()\n\n# Training\nhistory = model.fit(train_rainy, train_clear, validation_split=0.1, epochs=20, batch_size=4)\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    \n    # Define a smaller window size if images are less than 7x7\n    win_size = 3  # Use a smaller window size if necessary\n    \n    # Determine the data range\n    data_range = 1.0  # Assuming images are normalized between [0, 1]\n    \n    for i in range(predictions.shape[0]):\n        psnr_val = psnr(ground_truth[i], predictions[i])\n        \n        # Ensure the window size is appropriate for the image dimensions\n        if ground_truth[i].shape[0] < 7 or ground_truth[i].shape[1] < 7:\n            ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, win_size=3, data_range=data_range)\n        else:\n            ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True, win_size=win_size, data_range=data_range)\n        \n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\n# Evaluation\ntry:\n    predictions = model.predict(test_rainy, batch_size=1)  # Use smaller batch size to avoid memory issues\n\n    # Display predictions\n    for i in range(min(5, predictions.shape[0])):\n        plt.subplot(1, 3, 1)\n        plt.imshow(test_rainy[i])\n        plt.title('Rainy Image')\n        plt.axis('off')\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(predictions[i])\n        plt.title('Predicted Image')\n        plt.axis('off')\n\n        plt.subplot(1, 3, 3)\n        plt.imshow(test_clear[i])\n        plt.title('Ground Truth Image')\n        plt.axis('off')\n\n        plt.show()\n\n    # Calculate PSNR and SSIM\n    mean_psnr, mean_ssim = calculate_metrics(predictions, test_clear)\n    print(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")\n\nexcept Exception as e:\n    print(\"Error during evaluation:\", e)\n\n\n\n'''# Evaluation\npredictions = model.predict(test_rainy)\n\n# Display predictions\nfor i in range(min(5, predictions.shape[0])):\n    plt.subplot(1, 3, 1)\n    plt.imshow(test_rainy[i])\n    plt.title('Rainy Image')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(predictions[i])\n    plt.title('Predicted Image')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(test_clear[i])\n    plt.title('Ground Truth Image')\n    plt.axis('off')\n\n    plt.show()\n\n# Calculate PSNR and SSIM\ndef calculate_metrics(predictions, ground_truth):\n    psnr_values = []\n    ssim_values = []\n    for i in range(predictions.shape[0]):\n        psnr_val = psnr(ground_truth[i], predictions[i])\n        ssim_val = ssim(ground_truth[i], predictions[i], multichannel=True)\n        psnr_values.append(psnr_val)\n        ssim_values.append(ssim_val)\n    return np.mean(psnr_values), np.mean(ssim_values)\n\nmean_psnr, mean_ssim = calculate_metrics(predictions, test_clear)\nprint(f\"Mean PSNR: {mean_psnr}, Mean SSIM: {mean_ssim}\")'''\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNet + swin","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nimport os\nimport glob\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Swin Transformer block implementation\nclass SwinTransformerBlock(layers.Layer):\n    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n\n        # Layer normalization\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        # Multi-head self-attention layer\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim, dropout=dropout)\n\n        # MLP layer\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(int(dim * mlp_ratio), activation='relu'),\n            layers.Dense(dim)\n        ])\n\n    def call(self, x):\n        h = self.norm1(x)  # Apply normalization for attention\n        attn_output = self.attn(h, h)  # Self-attention\n\n        # Residual connection\n        x = x + attn_output\n\n        h = self.norm2(x)  # Apply normalization for MLP\n        x = x + self.mlp(h)  # Residual connection for MLP\n        return x\n\n# Window-based Swin Transformer Layer (basic building block)\nclass SwinTransformerLayer(layers.Layer):\n    def __init__(self, dim, depth, num_heads, mlp_ratio=4., dropout=0.0):\n        super(SwinTransformerLayer, self).__init__()\n        self.blocks = [SwinTransformerBlock(dim, num_heads, mlp_ratio, dropout) for _ in range(depth)]\n\n    def call(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n# Swin transformer block applied after each downsampling\ndef swin_transformer_block(input_tensor, num_heads, embed_dim, depths):\n    # Patch partition (flatten input image into non-overlapping patches)\n    height, width = input_tensor.shape[1], input_tensor.shape[2]  # Keras tensor shape\n    embed_layer = layers.Conv2D(embed_dim, kernel_size=4, strides=2, padding='same')(input_tensor)  # Reduce spatial size\n\n    # Reshape for Swin Transformer: Batch, Height * Width, Channels\n    reshaped_layer = layers.Reshape((height // 2 * width // 2, embed_dim))(embed_layer)\n\n    # Apply Swin Transformer layers (recursive depth)\n    swin_layer = SwinTransformerLayer(embed_dim, depths[0], num_heads)\n    transformed_layer = swin_layer(reshaped_layer)\n\n    # Reshape back to original spatial dimensions\n    output_layer = layers.Reshape((height // 2, width // 2, embed_dim))(transformed_layer)\n\n    return output_layer\n\n# U-Net Encoder Block (with Strided Convolutions for downsampling)\ndef unet_encoder_block(input_tensor, filters, kernel_size=3, padding='same', strides=1):\n    x = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(input_tensor)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x\n\n# U-Net Decoder Block\ndef unet_decoder_block(input_tensor, skip_tensor, filters, kernel_size=3, padding='same', strides=1):\n    x = layers.Conv2DTranspose(filters, kernel_size, padding=padding, strides=2)(input_tensor)\n    # Ensure dimensions are correct before concatenation\n    if skip_tensor.shape[1] != x.shape[1] or skip_tensor.shape[2] != x.shape[2]:\n        x = layers.Resizing(skip_tensor.shape[1], skip_tensor.shape[2])(x)  # Resize if needed\n    print(f\"Decoding: {x.shape}, Skip: {skip_tensor.shape}\")\n    x = layers.Concatenate()([x, skip_tensor])\n    x = layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x\n\n# Hybrid U-Net + Swin Transformer Model\ndef unet_swin_transformer(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # U-Net Encoder with Strided Convolutions and Swin Transformer at each downsampling step\n    enc1 = unet_encoder_block(inputs, 64)\n    swin1 = swin_transformer_block(enc1, num_heads=4, embed_dim=64, depths=[2])\n    pool1 = layers.Conv2D(128, (3, 3), strides=2, padding='same')(swin1)  # Strided convolution for downsampling\n\n    enc2 = unet_encoder_block(pool1, 128)\n    swin2 = swin_transformer_block(enc2, num_heads=4, embed_dim=128, depths=[2])\n    pool2 = layers.Conv2D(256, (3, 3), strides=2, padding='same')(swin2)  # Strided convolution for downsampling\n\n    enc3 = unet_encoder_block(pool2, 256)\n    swin3 = swin_transformer_block(enc3, num_heads=4, embed_dim=256, depths=[2])\n    pool3 = layers.Conv2D(512, (3, 3), strides=2, padding='same')(swin3)  # Strided convolution for downsampling\n\n    enc4 = unet_encoder_block(pool3, 512)\n    swin4 = swin_transformer_block(enc4, num_heads=4, embed_dim=512, depths=[2])\n\n    # Bottleneck\n    bottleneck = layers.Conv2D(1024, (3, 3), strides=2, padding='same')(swin4)\n\n    # Decoder with Skip Connections\n    dec4 = unet_decoder_block(bottleneck, enc4, 512)\n    dec3 = unet_decoder_block(dec4, enc3, 256)\n    dec2 = unet_decoder_block(dec3, enc2, 128)\n    dec1 = unet_decoder_block(dec2, enc1, 64)\n\n    # Final output layer\n    outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(dec1)\n\n    # Model definition\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Instantiate and compile the model\ninput_shape = (128, 128, 3)  # Input image dimensions\nmodel = unet_swin_transformer(input_shape)\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n\n# Print model summary\nmodel.summary()\n# Function to calculate PSNR and SSIM\ndef calculate_metrics(original, generated):\n    psnr_value = psnr(original, generated)\n    ssim_value = ssim(original, generated, multichannel=True)\n    return psnr_value, ssim_value\n\n# Training function\ndef train_model(model, train_data, epochs=100, batch_size=16):\n    rainy_images, clear_images = train_data\n    model.fit(rainy_images, clear_images, epochs=epochs, batch_size=batch_size)\n\n# Evaluation function on the test set\ndef evaluate_model(model, test_data):\n    rainy_images_test, clear_images_test = test_data\n    predictions = model.predict(rainy_images_test)\n\n    # Calculate metrics for each image\n    psnr_values = []\n    ssim_values = []\n    for i in range(len(predictions)):\n        psnr_value, ssim_value = calculate_metrics(clear_images_test[i], predictions[i])\n        psnr_values.append(psnr_value)\n        ssim_values.append(ssim_value)\n\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    return avg_psnr, avg_ssim\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_data, test_data) = load_and_preprocess_datasets(base_folder, augment=True)\ntrain_model(model, train_data, epochs=10, batch_size=16)\navg_psnr, avg_ssim = evaluate_model(model, test_data)\n\nprint(f'Average PSNR: {avg_psnr}, Average SSIM: {avg_ssim}')\n\n# Display predicted images (for the first 5 test images)\npredictions = model.predict(test_data[0])\nfor i in range(5):\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(test_data[0][i])  # Display rainy image\n    plt.axis('off')\n    plt.subplot(3, 5, i + 6)\n    plt.imshow(predictions[i])  # Display predicted clear image\n    plt.axis('off')\n    plt.subplot(3, 5, i + 11)\n    plt.imshow(test_data[1][i])  # Display ground truth image\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []  # Corrected initialization\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\nprint(f\"Training Rainy Images Shape: {train_rainy.shape}\")\nprint(f\"Training Clear Images Shape: {train_clear.shape}\")\nprint(f\"Testing Rainy Images Shape: {test_rainy.shape}\")\nprint(f\"Testing Clear Images Shape: {test_clear.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pyramid Model","metadata":{}},{"cell_type":"code","source":"# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        \n        # Use Depthwise Separable Convolutions\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)  # Pointwise Conv\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Swin transformer for image Restoration","metadata":{}},{"cell_type":"code","source":"# -----------------------------------------------------------------------------------\n# SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257\n# Originally Written by Ze Liu, Modified by Jingyun Liang.\n# -----------------------------------------------------------------------------------\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n    def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n        # qkv = self.qkv(x)\n        flops += N * self.dim * 3 * self.dim\n        # attn = (q @ k.transpose(-2, -1))\n        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n        #  x = (attn @ v)\n        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            attn_mask = self.calculate_mask(self.input_resolution)\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def calculate_mask(self, x_size):\n        # calculate attention mask for SW-MSA\n        H, W = x_size\n        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n        h_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        w_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n\n        return attn_mask\n\n    def forward(self, x, x_size):\n        H, W = x_size\n        B, L, C = x.shape\n        # assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n        if self.input_resolution == x_size:\n            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n        else:\n            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        # W-MSA/SW-MSA\n        nW = H * W / self.window_size / self.window_size\n        flops += nW * self.attn.flops(self.window_size * self.window_size)\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = H * W * self.dim\n        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n        return flops\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x, x_size):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, x_size)\n            else:\n                x = blk(x, x_size)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        if self.downsample is not None:\n            flops += self.downsample.flops()\n        return flops\n\n\nclass RSTB(nn.Module):\n    \"\"\"Residual Swin Transformer Block (RSTB).\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        img_size: Input image size.\n        patch_size: Patch size.\n        resi_connection: The convolutional block before residual connection.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n                 img_size=224, patch_size=4, resi_connection='1conv'):\n        super(RSTB, self).__init__()\n\n        self.dim = dim\n        self.input_resolution = input_resolution\n\n        self.residual_group = BasicLayer(dim=dim,\n                                         input_resolution=input_resolution,\n                                         depth=depth,\n                                         num_heads=num_heads,\n                                         window_size=window_size,\n                                         mlp_ratio=mlp_ratio,\n                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                         drop=drop, attn_drop=attn_drop,\n                                         drop_path=drop_path,\n                                         norm_layer=norm_layer,\n                                         downsample=downsample,\n                                         use_checkpoint=use_checkpoint)\n\n        if resi_connection == '1conv':\n            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n        elif resi_connection == '3conv':\n            # to save parameters and memory\n            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n            norm_layer=None)\n\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n            norm_layer=None)\n\n    def forward(self, x, x_size):\n        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n\n    def flops(self):\n        flops = 0\n        flops += self.residual_group.flops()\n        H, W = self.input_resolution\n        flops += H * W * self.dim * self.dim * 9\n        flops += self.patch_embed.flops()\n        flops += self.patch_unembed.flops()\n\n        return flops\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        flops = 0\n        H, W = self.img_size\n        if self.norm is not None:\n            flops += H * W * self.embed_dim\n        return flops\n\n\nclass PatchUnEmbed(nn.Module):\n    r\"\"\" Image to Patch Unembedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n    def forward(self, x, x_size):\n        B, HW, C = x.shape\n        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n        return x\n\n    def flops(self):\n        flops = 0\n        return flops\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n        super(Upsample, self).__init__(*m)\n\n\nclass UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n\n    \"\"\"\n\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution\n        m = []\n        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n        m.append(nn.PixelShuffle(scale))\n        super(UpsampleOneStep, self).__init__(*m)\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = H * W * self.num_feat * 3 * 9\n        return flops\n\n\nclass SwinIR(nn.Module):\n    r\"\"\" SwinIR\n        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n        img_range: Image range. 1. or 255.\n        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n    \"\"\"\n\n    def __init__(self, img_size=64, patch_size=1, in_chans=3,\n                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],\n                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n                 use_checkpoint=False, upscale=2, img_range=1., upsampler='', resi_connection='1conv',\n                 **kwargs):\n        super(SwinIR, self).__init__()\n        num_in_ch = in_chans\n        num_out_ch = in_chans\n        num_feat = 64\n        self.img_range = img_range\n        if in_chans == 3:\n            rgb_mean = (0.4488, 0.4371, 0.4040)\n            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n        else:\n            self.mean = torch.zeros(1, 1, 1, 1)\n        self.upscale = upscale\n        self.upsampler = upsampler\n        self.window_size = window_size\n\n        #####################################################################################################\n        ################################### 1, shallow feature extraction ###################################\n        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n\n        #####################################################################################################\n        ################################### 2, deep feature extraction ######################################\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = embed_dim\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # merge non-overlapping patches into image\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n            trunc_normal_(self.absolute_pos_embed, std=.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build Residual Swin Transformer blocks (RSTB)\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = RSTB(dim=embed_dim,\n                         input_resolution=(patches_resolution[0],\n                                           patches_resolution[1]),\n                         depth=depths[i_layer],\n                         num_heads=num_heads[i_layer],\n                         window_size=window_size,\n                         mlp_ratio=self.mlp_ratio,\n                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n                         drop=drop_rate, attn_drop=attn_drop_rate,\n                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n                         norm_layer=norm_layer,\n                         downsample=None,\n                         use_checkpoint=use_checkpoint,\n                         img_size=img_size,\n                         patch_size=patch_size,\n                         resi_connection=resi_connection\n\n                         )\n            self.layers.append(layer)\n        self.norm = norm_layer(self.num_features)\n\n        # build the last conv layer in deep feature extraction\n        if resi_connection == '1conv':\n            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n        elif resi_connection == '3conv':\n            # to save parameters and memory\n            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n\n        #####################################################################################################\n        ################################ 3, high quality image reconstruction ################################\n        if self.upsampler == 'pixelshuffle':\n            # for classical SR\n            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n                                                      nn.LeakyReLU(inplace=True))\n            self.upsample = Upsample(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n        elif self.upsampler == 'pixelshuffledirect':\n            # for lightweight SR (to save parameters)\n            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n                                            (patches_resolution[0], patches_resolution[1]))\n        elif self.upsampler == 'nearest+conv':\n            # for real-world SR (less artifacts)\n            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n                                                      nn.LeakyReLU(inplace=True))\n            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            if self.upscale == 4:\n                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'absolute_pos_embed'}\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {'relative_position_bias_table'}\n\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n        return x\n\n    def forward_features(self, x):\n        x_size = (x.shape[2], x.shape[3])\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x, x_size)\n\n        x = self.norm(x)  # B L C\n        x = self.patch_unembed(x, x_size)\n\n        return x\n\n    def forward(self, x):\n        H, W = x.shape[2:]\n        x = self.check_image_size(x)\n        \n        self.mean = self.mean.type_as(x)\n        x = (x - self.mean) * self.img_range\n\n        if self.upsampler == 'pixelshuffle':\n            # for classical SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.conv_last(self.upsample(x))\n        elif self.upsampler == 'pixelshuffledirect':\n            # for lightweight SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.upsample(x)\n        elif self.upsampler == 'nearest+conv':\n            # for real-world SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n            if self.upscale == 4:\n                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            x_first = self.conv_first(x)\n            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n            x = x + self.conv_last(res)\n\n        x = x / self.img_range + self.mean\n\n        return x[:, :, :H*self.upscale, :W*self.upscale]\n\n    def flops(self):\n        flops = 0\n        H, W = self.patches_resolution\n        flops += H * W * 3 * self.embed_dim * 9\n        flops += self.patch_embed.flops()\n        for i, layer in enumerate(self.layers):\n            flops += layer.flops()\n        flops += H * W * 3 * self.embed_dim * self.embed_dim\n        flops += self.upsample.flops()\n        return flops\n\n\nif __name__ == '__main__':\n    upscale = 4\n    window_size = 8\n    height = (1024 // upscale // window_size + 1) * window_size\n    width = (720 // upscale // window_size + 1) * window_size\n    model = SwinIR(upscale=2, img_size=(height, width),\n                   window_size=window_size, img_range=1., depths=[6, 6, 6, 6],\n                   embed_dim=60, num_heads=[6, 6, 6, 6], mlp_ratio=2, upsampler='pixelshuffledirect')\n    print(model)\n    print(height, width, model.flops() / 1e9)\n\n    x = torch.randn((1, 3, height, width))\n    x = model(x)\n    print(x.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(inputs, num_classes=3):\n    # Normalize the input features\n    x = LayerNormalization()(inputs)\n    \n    # Using Depthwise Separable Convolutions\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n    \n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compile Model ","metadata":{}},{"cell_type":"code","source":"# Full Model Creation\ndef build_full_model(input_shape):\n    # Input Layer\n    inputs = Input(shape=input_shape)\n\n    # Dynamic Pyramid Model\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n\n    # Image Restoration and Enhancement\n    outputs = image_restoration_and_enhancement(pyramid_features, num_classes=input_shape[-1])\n\n    return Model(inputs=inputs, outputs=outputs)\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Show some example images to verify data loading\nplt.imshow(rainy_train[0])\nplt.title(\"Sample Rainy Image\")\nplt.show()\n\nplt.imshow(clear_train[0])\nplt.title(\"Sample Clear Image\")\nplt.show()\n\n# Build the model\ninput_shape = rainy_train[0].shape  # Assuming your input images are all the same size\nmodel = build_full_model(input_shape)\nmodel.summary()\n\n# Enable mixed precision training\nfrom tensorflow.keras.mixed_precision import set_global_policy\nset_global_policy('mixed_float16')\n\n# Proper TensorFlow logging\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n\n# Custom callback to print CPU/GPU usage\nclass ResourceMonitorCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        # Print resource information at the beginning of each epoch\n        print(f\"Epoch {epoch + 1} starting...\")\n        \n        # Check CPU utilization\n        cpu_info = os.popen(\"lscpu\").read()\n        print(f\"CPU Info:\\n{cpu_info}\")\n        \n        # Check GPU utilization if available\n        if tf.config.list_physical_devices('GPU'):\n            gpu_info = os.popen(\"nvidia-smi\").read()\n            print(f\"GPU Info:\\n{gpu_info}\")\n        else:\n            print(\"No GPU detected, using CPU.\")\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Summary of the epoch\n        print(f\"Epoch {epoch + 1} ended. Loss: {logs.get('loss')}, Validation Loss: {logs.get('val_loss')}\")\n        print(\"=\" * 50)\n\n# Ensure the dataset isn't infinite\nbatch_size = 4\nsteps_per_epoch = len(rainy_train) // batch_size  # Ensure steps per epoch is finite\n\n# Add debug prints in your data preprocessing and training loops\nprint(f\"Dataset Size: {len(rainy_train)}\")\nprint(f\"Steps per Epoch: {steps_per_epoch}\")\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Define callbacks\ncheckpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    rainy_train, clear_train,\n    validation_data=(rainy_test, clear_test),\n    epochs=50,\n    batch_size=8,  # Change this value if necessary\n    callbacks=[checkpoint, early_stop],\n    verbose=1\n)\n\n# Evaluate the model\npredicted_images = model.predict(rainy_test)\n\n# Calculate MSE\nmse = mean_squared_error(clear_test.flatten(), predicted_images.flatten())\nprint(\"Mean Squared Error on Test Set: \", mse)\n\n# Calculate PSNR\npsnr_value = tf.reduce_mean(psnr(clear_test, predicted_images, max_val=1.0))\nprint(\"PSNR on Test Set: \", psnr_value.numpy())\n\n# Calculate SSIM (with win_size <= 7, and using channel_axis instead of multichannel)\nssim_values = []\nfor i in range(len(clear_test)):\n    ssim_value = ssim(clear_test[i], predicted_images[i], win_size=5, channel_axis=-1)  # Set win_size <= 7\n    ssim_values.append(ssim_value)\n\navg_ssim_value = np.mean(ssim_values)\nprint(\"Average SSIM on Test Set: \", avg_ssim_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New concept","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, MultiHeadAttention, DepthwiseConv2D\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom skimage.metrics import structural_similarity as ssim\n\n# Optionally, force CPU for debugging\n# tf.config.set_visible_devices([], 'GPU')\n\n# Disable XLA to avoid issues\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        \n        # Use Depthwise Separable Convolutions\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)  # Pointwise Conv\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Swin Transformer Block Implementation with k-th Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, k=4, shift_size=0, mlp_ratio=4.):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.num_heads = num_heads\n        self.k = k  # Number of top-k attention weights\n        self.dim = dim\n        \n        # Layer normalization\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        \n        # Feed-forward network\n        self.mlp = self.get_mlp(dim, mlp_ratio)\n\n    def get_mlp(self, dim, mlp_ratio):\n        return tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dense(dim)\n        ])\n    \n    def call(self, x):\n        # Layer normalization\n        x_norm = self.norm1(x)\n\n        # Compute attention\n        attn_output = self.attn(x_norm, x_norm)\n\n        # Get top-k attention weights\n        attn_weights = tf.nn.softmax(tf.matmul(x_norm, x_norm, transpose_b=True))  # Computing attention weights\n        top_k_values, indices = tf.nn.top_k(attn_weights, k=self.k)\n        threshold = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n\n        # Suppress weights below the threshold\n        attn_weights = tf.where(attn_weights < threshold, tf.zeros_like(attn_weights), attn_weights)\n\n        # Normalize the weights again after thresholding\n        attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=-1, keepdims=True)\n\n        # Use the modified attention weights to compute output\n        attn_output = tf.matmul(attn_weights, attn_output)\n\n        # Skip connection\n        x = x + attn_output  \n\n        # Feed-forward network\n        x = self.norm2(x)\n        mlp_output = self.mlp(x)\n        x = x + mlp_output  # Skip connection\n        \n        return x\n\ndef swin_transformer_block(inputs):\n    # Define parameters for the Swin Transformer Block\n    dim = inputs.shape[-1]  # Input channel dimension\n    num_heads = 4  # Number of attention heads\n    window_size = 7  # Size of the window\n    shift_size = 0  # Shift size for the windowing scheme\n    \n    # Create the Swin Transformer Block\n    x = SwinTransformerBlock(dim, num_heads, window_size, k=4, shift_size=shift_size)(inputs)\n    \n    return x\n\n# Recursive Swin Transformer Block with residual connections\ndef recursive_swin_transformer_block(inputs, num_recursions=6):\n    x = inputs\n    for _ in range(num_recursions):\n        # Call the Swin Transformer block\n        x = swin_transformer_block(x)\n    \n    return x\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(inputs, num_classes=3):\n    # Normalize the input features\n    x = LayerNormalization()(inputs)\n    \n    # Using Depthwise Separable Convolutions\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n    \n    return x\n\n# Full Model Creation\ndef build_full_model(input_shape):\n    # Input Layer\n    inputs = Input(shape=input_shape)\n\n    # Dynamic Pyramid Model\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n\n    # Recursive Swin Transformer Block\n    transformer_features = recursive_swin_transformer_block(pyramid_features)\n\n    # Image Restoration and Enhancement\n    outputs = image_restoration_and_enhancement(transformer_features, num_classes=input_shape[-1])\n\n    return Model(inputs=inputs, outputs=outputs)\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Show some example images to verify data loading\nplt.imshow(rainy_train[0])\nplt.title(\"Sample Rainy Image\")\nplt.show()\n\nplt.imshow(clear_train[0])\nplt.title(\"Sample Clear Image\")\nplt.show()\n\n# Build the model\ninput_shape = rainy_train[0].shape  # Assuming your input images are all the same size\nmodel = build_full_model(input_shape)\nmodel.summary()\n\n# Enable mixed precision training\nfrom tensorflow.keras.mixed_precision import set_global_policy\nset_global_policy('mixed_float16')\n\n# Proper TensorFlow logging\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n\n# Custom callback to print CPU/GPU usage\nclass ResourceMonitorCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        # Print resource information at the beginning of each epoch\n        print(f\"Epoch {epoch + 1} starting...\")\n        \n        # Check CPU utilization\n        cpu_info = os.popen(\"lscpu\").read()\n        print(f\"CPU Info:\\n{cpu_info}\")\n        \n        # Check GPU utilization if available\n        if tf.config.list_physical_devices('GPU'):\n            gpu_info = os.popen(\"nvidia-smi\").read()\n            print(f\"GPU Info:\\n{gpu_info}\")\n        else:\n            print(\"No GPU detected, using CPU.\")\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Summary of the epoch\n        print(f\"Epoch {epoch + 1} ended. Loss: {logs.get('loss')}, Validation Loss: {logs.get('val_loss')}\")\n        print(\"=\" * 50)\n\n# Ensure the dataset isn't infinite\nbatch_size = 4\nsteps_per_epoch = len(rainy_train) // batch_size  # Ensure steps per epoch is finite\n\n# Add debug prints in your data preprocessing and training loops\nprint(f\"Dataset Size: {len(rainy_train)}\")\nprint(f\"Steps per Epoch: {steps_per_epoch}\")\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Define callbacks\ncheckpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    rainy_train, clear_train,\n    validation_data=(rainy_test, clear_test),\n    epochs=50,\n    batch_size=8,  # Change this value if necessary\n    callbacks=[checkpoint, early_stop],\n    verbose=1\n)\n\n# Evaluate the model\nmse = mean_squared_error(clear_test.flatten(), model.predict(rainy_test).flatten())\nprint(\"Mean Squared Error on Test Set: \", mse)\n\n# You can also calculate SSIM\nssim_value = ssim(clear_test[0], model.predict(rainy_test)[0], multichannel=True)\nprint(\"SSIM Value on Test Set: \", ssim_value)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Without Swin","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, DepthwiseConv2D\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom skimage.metrics import structural_similarity as ssim\nfrom tensorflow.image import psnr  # Importing TensorFlow's PSNR function\n\n# Optionally, force CPU for debugging\n# tf.config.set_visible_devices([], 'GPU')\n\n# Disable XLA to avoid issues\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        \n        # Use Depthwise Separable Convolutions\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)  # Pointwise Conv\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(inputs, num_classes=3):\n    # Normalize the input features\n    x = LayerNormalization()(inputs)\n    \n    # Using Depthwise Separable Convolutions\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n    \n    return x\n\n# Full Model Creation\ndef build_full_model(input_shape):\n    # Input Layer\n    inputs = Input(shape=input_shape)\n\n    # Dynamic Pyramid Model\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n\n    # Image Restoration and Enhancement\n    outputs = image_restoration_and_enhancement(pyramid_features, num_classes=input_shape[-1])\n\n    return Model(inputs=inputs, outputs=outputs)\n\n# Load dataset\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Show some example images to verify data loading\nplt.imshow(rainy_train[0])\nplt.title(\"Sample Rainy Image\")\nplt.show()\n\nplt.imshow(clear_train[0])\nplt.title(\"Sample Clear Image\")\nplt.show()\n\n# Build the model\ninput_shape = rainy_train[0].shape  # Assuming your input images are all the same size\nmodel = build_full_model(input_shape)\nmodel.summary()\n\n# Enable mixed precision training\nfrom tensorflow.keras.mixed_precision import set_global_policy\nset_global_policy('mixed_float16')\n\n# Proper TensorFlow logging\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n\n# Custom callback to print CPU/GPU usage\nclass ResourceMonitorCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        # Print resource information at the beginning of each epoch\n        print(f\"Epoch {epoch + 1} starting...\")\n        \n        # Check CPU utilization\n        cpu_info = os.popen(\"lscpu\").read()\n        print(f\"CPU Info:\\n{cpu_info}\")\n        \n        # Check GPU utilization if available\n        if tf.config.list_physical_devices('GPU'):\n            gpu_info = os.popen(\"nvidia-smi\").read()\n            print(f\"GPU Info:\\n{gpu_info}\")\n        else:\n            print(\"No GPU detected, using CPU.\")\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Summary of the epoch\n        print(f\"Epoch {epoch + 1} ended. Loss: {logs.get('loss')}, Validation Loss: {logs.get('val_loss')}\")\n        print(\"=\" * 50)\n\n# Ensure the dataset isn't infinite\nbatch_size = 4\nsteps_per_epoch = len(rainy_train) // batch_size  # Ensure steps per epoch is finite\n\n# Add debug prints in your data preprocessing and training loops\nprint(f\"Dataset Size: {len(rainy_train)}\")\nprint(f\"Steps per Epoch: {steps_per_epoch}\")\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n# Define callbacks\ncheckpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    rainy_train, clear_train,\n    validation_data=(rainy_test, clear_test),\n    epochs=50,\n    batch_size=8,  # Change this value if necessary\n    callbacks=[checkpoint, early_stop],\n    verbose=1\n)\n\n# Evaluate the model\npredicted_images = model.predict(rainy_test)\n\n# Calculate MSE\nmse = mean_squared_error(clear_test.flatten(), predicted_images.flatten())\nprint(\"Mean Squared Error on Test Set: \", mse)\n\n# Calculate PSNR\npsnr_value = tf.reduce_mean(psnr(clear_test, predicted_images, max_val=1.0))\nprint(\"PSNR on Test Set: \", psnr_value.numpy())\n\n# Calculate SSIM (with win_size <= 7, and using channel_axis instead of multichannel)\nssim_values = []\nfor i in range(len(clear_test)):\n    ssim_value = ssim(clear_test[i], predicted_images[i], win_size=5, channel_axis=-1)  # Set win_size <= 7\n    ssim_values.append(ssim_value)\n\navg_ssim_value = np.mean(ssim_values)\nprint(\"Average SSIM on Test Set: \", avg_ssim_value)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pruned model","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow-model-optimization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, MultiHeadAttention, DepthwiseConv2D, Dropout\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow_model_optimization as tfmot\nfrom tensorflow.keras.losses import KLDivergence\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    \"\"\"\n    Loads and resizes images from a specified folder.\n\n    Args:\n        folder (str): Path to the folder containing images.\n        size (tuple): Desired size for resizing the images (width, height).\n\n    Returns:\n        np.array: Array of loaded and resized images.\n    \"\"\"\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    \"\"\"\n    Preprocess and augment image data.\n\n    Args:\n        images (np.array): Array of images to preprocess.\n        augment (bool): If True, applies augmentation to the images.\n\n    Returns:\n        np.array: Preprocessed (and possibly augmented) images.\n    \"\"\"\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    \"\"\"\n    Loads and preprocesses the dataset for training and testing.\n\n    Args:\n        base_folder (str): Path to the base folder containing datasets.\n        size (tuple): Desired size for resizing the images.\n        augment (bool): If True, applies data augmentation to training data.\n\n    Returns:\n        tuple: Training and testing datasets for rainy and clear images.\n    \"\"\"\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    \"\"\"\n    Builds an advanced dynamic pyramid model.\n\n    Args:\n        input_shape (tuple): Shape of the input image (height, width, channels).\n        num_scales (int): Number of scales for the pyramid model.\n\n    Returns:\n        tuple: Model input and concatenated pyramid features.\n    \"\"\"\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        kernel_size = 3 + scale\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        pyramid_features.append(x_resized)\n\n    fused_features = Lambda(lambda x: tf.concat(x, axis=-1))(pyramid_features)\n    return inputs, fused_features\n\n# Swin Transformer Block\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, k=4, shift_size=0, mlp_ratio=4.):\n        \"\"\"\n        Swin Transformer Block with top-k sparsity applied.\n\n        Args:\n            dim (int): Input dimension size.\n            num_heads (int): Number of attention heads.\n            window_size (int): Size of the local attention window.\n            k (int): Top-k sparsity parameter.\n            shift_size (int): Shift size for shifted windowing.\n            mlp_ratio (float): Ratio for the MLP hidden size.\n        \"\"\"\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.num_heads = num_heads\n        self.k = k\n        self.dim = dim\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.mlp = self.get_mlp(dim, mlp_ratio)\n\n    def get_mlp(self, dim, mlp_ratio):\n        return tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dense(dim)\n        ])\n\n    def call(self, x):\n        \"\"\"\n        Applies Swin transformer block with top-k sparsity attention.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Tensor after applying attention and MLP.\n        \"\"\"\n        x_norm = self.norm1(x)\n        attn_output = self.attn(x_norm, x_norm)\n        attn_weights = tf.nn.softmax(tf.matmul(x_norm, x_norm, transpose_b=True))\n        top_k_values, indices = tf.nn.top_k(attn_weights, k=self.k)\n        threshold = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n        attn_weights = tf.where(attn_weights < threshold, tf.zeros_like(attn_weights), attn_weights)\n        attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=-1, keepdims=True)\n        attn_output = tf.matmul(attn_weights, attn_output)\n        x = x + attn_output  \n        x = self.norm2(x)\n        mlp_output = self.mlp(x)\n        x = x + mlp_output  \n        return x\n\n# Swin Transformer block helper functions\ndef swin_transformer_block(inputs):\n    dim = inputs.shape[-1]\n    num_heads = 4\n    window_size = 7\n    x = SwinTransformerBlock(dim, num_heads, window_size, k=4)(inputs)\n    return x\n\ndef recursive_swin_transformer_block(inputs, num_recursions=6):\n    \"\"\"\n    Builds a recursive Swin transformer block for multiple recursions.\n\n    Args:\n        inputs: Input tensor.\n        num_recursions (int): Number of recursive blocks to apply.\n\n    Returns:\n        Output tensor after multiple recursions.\n    \"\"\"\n    x = inputs\n    for _ in range(num_recursions):\n        x = swin_transformer_block(x)\n    return x\n\n# Image Restoration and Enhancement Block with Dropout\ndef image_restoration_and_enhancement_with_dropout(inputs, num_classes=3, dropout_rate=0.4):\n    \"\"\"\n    Image restoration and enhancement block with dropout layers.\n\n    Args:\n        inputs: Input tensor.\n        num_classes (int): Number of output channels.\n        dropout_rate (float): Dropout rate to apply.\n\n    Returns:\n        Output tensor for image restoration and enhancement.\n    \"\"\"\n    x = LayerNormalization()(inputs)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(num_classes, (3, 3), padding='same')(x)\n    return x\n\n# Build the complete model with pyramid, Swin transformer, and enhancement block\ndef build_full_model_with_dropout(input_shape=(128, 128, 3), num_classes=3, dropout_rate=0.4):\n    \"\"\"\n    Builds the complete image restoration model with dropout, dynamic pyramid, and Swin transformer.\n\n    Args:\n        input_shape (tuple): Shape of the input image (height, width, channels).\n        num_classes (int): Number of output channels.\n        dropout_rate (float): Dropout rate to apply in the enhancement block.\n\n    Returns:\n        Model: Keras Model object.\n    \"\"\"\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n    swin_output = recursive_swin_transformer_block(pyramid_features)\n    output = image_restoration_and_enhancement_with_dropout(swin_output, num_classes=num_classes, dropout_rate=dropout_rate)\n    model = Model(inputs=inputs, outputs=output)\n\n    # Model summary\n    model.summary()\n    return model\n\n# Knowledge Distillation Loss Function\ndef knowledge_distillation_loss(student_logits, teacher_logits, alpha=0.5, temperature=3):\n    \"\"\"\n    Custom loss function for knowledge distillation.\n\n    Args:\n        student_logits: Output logits from the student model.\n        teacher_logits: Output logits from the teacher model.\n        alpha (float): Weight for balancing between hard target loss and soft target loss.\n        temperature (float): Temperature for scaling logits during distillation.\n\n    Returns:\n        Loss value.\n    \"\"\"\n    distillation_loss = KLDivergence()(tf.nn.softmax(teacher_logits / temperature),\n                                       tf.nn.softmax(student_logits / temperature)) * (temperature ** 2)\n    return distillation_loss\n\n# Build a smaller student model\ndef build_student_model(input_shape=(128, 128, 3), dropout_rate=0.4):\n    \"\"\"\n    Builds a smaller student model for knowledge distillation.\n\n    Args:\n        input_shape (tuple): Shape of the input image (height, width, channels).\n        dropout_rate (float): Dropout rate to apply in the enhancement block.\n\n    Returns:\n        Model: Keras Model object.\n    \"\"\"\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n    swin_output = recursive_swin_transformer_block(pyramid_features, num_recursions=3)  # Fewer recursions for student\n    output = image_restoration_and_enhancement_with_dropout(swin_output, dropout_rate=dropout_rate)\n    student_model = Model(inputs=inputs, outputs=output)\n\n    # Model summary\n    student_model.summary()\n    return student_model\n\n# Pruning with TensorFlow Model Optimization Toolkit\ndef apply_pruning(model):\n    \"\"\"\n    Applies pruning to the given model to reduce its size.\n\n    Args:\n        model: Keras Model to prune.\n\n    Returns:\n        pruned_model: Pruned Keras model.\n    \"\"\"\n    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n    pruning_params = {\n        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n            initial_sparsity=0.0, final_sparsity=0.5,\n            begin_step=2000, end_step=10000\n        )\n    }\n    pruned_model = prune_low_magnitude(model, **pruning_params)\n    pruned_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n    # Model summary\n    pruned_model.summary()\n    return pruned_model\n\n# Custom training loop for Knowledge Distillation\ndef train_student_with_distillation(student_model, teacher_model, rainy_train, clear_train, epochs=50, batch_size=32):\n    \"\"\"\n    Trains the student model using knowledge distillation from the teacher model.\n\n    Args:\n        student_model: Keras model for the student.\n        teacher_model: Trained teacher model.\n        rainy_train: Training set inputs (rainy images).\n        clear_train: Training set targets (clear images).\n        epochs (int): Number of epochs to train.\n        batch_size (int): Batch size for training.\n    \"\"\"\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        for batch in range(0, len(rainy_train), batch_size):\n            x_batch = rainy_train[batch:batch + batch_size]\n            y_batch = clear_train[batch:batch + batch_size]\n            teacher_logits = teacher_model.predict(x_batch)\n            with tf.GradientTape() as tape:\n                student_logits = student_model(x_batch)\n                loss = knowledge_distillation_loss(student_logits, teacher_logits)\n            gradients = tape.gradient(loss, student_model.trainable_variables)\n            student_model.optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n\n# Example usage:\n\n# Load data\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Train Teacher Model\nteacher_model = build_full_model_with_dropout(input_shape=(128, 128, 3), dropout_rate=0.4)\nteacher_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\nteacher_model.fit(rainy_train, clear_train, epochs=50, batch_size=32, validation_split=0.1)\n\n# Apply Pruning\npruned_teacher_model = apply_pruning(teacher_model)\n\n# Train Student Model using Knowledge Distillation\nstudent_model = build_student_model(input_shape=(128, 128, 3), dropout_rate=0.4)\nstudent_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\ntrain_student_with_distillation(student_model, pruned_teacher_model, rainy_train, clear_train, epochs=50, batch_size=32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model with feature aggregation in swin transformer","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, MultiHeadAttention, DepthwiseConv2D, Dropout\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        kernel_size = 3 + scale\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        pyramid_features.append(x_resized)\n\n    fused_features = Lambda(lambda x: tf.concat(x, axis=-1))(pyramid_features)\n    return inputs, fused_features\n\n# Swin Transformer Block\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, k=4, shift_size=0, mlp_ratio=4.):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.num_heads = num_heads\n        self.k = k\n        self.dim = dim\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        self.mlp = self.get_mlp(dim, mlp_ratio)\n\n    def get_mlp(self, dim, mlp_ratio):\n        return tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dense(dim)\n        ])\n\n    def call(self, x):\n        x_norm = self.norm1(x)\n        attn_output = self.attn(x_norm, x_norm)\n        attn_weights = tf.nn.softmax(tf.matmul(x_norm, x_norm, transpose_b=True))\n        top_k_values, indices = tf.nn.top_k(attn_weights, k=self.k)\n        threshold = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n        attn_weights = tf.where(attn_weights < threshold, tf.zeros_like(attn_weights), attn_weights)\n        attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=-1, keepdims=True)\n        attn_output = tf.matmul(attn_weights, attn_output)\n        x = x + attn_output  \n        x = self.norm2(x)\n        mlp_output = self.mlp(x)\n        x = x + mlp_output  \n        return x\n\n# Swin Transformer block helper functions\ndef swin_transformer_block(inputs):\n    dim = inputs.shape[-1]\n    num_heads = 4\n    window_size = 7\n    x = SwinTransformerBlock(dim, num_heads, window_size, k=4)(inputs)\n    return x\n\n# Recursive Swin Transformer Block with Aggregation and Residual Connections\ndef recursive_swin_transformer_block(inputs, num_recursions=6):\n    x = inputs\n    for i in range(num_recursions):\n        swin_output = swin_transformer_block(x)\n        x = Add()([x, swin_output])  # Residual connection across blocks\n    return x\n\n# Image Restoration and Enhancement Block with Dropout\ndef image_restoration_and_enhancement_with_dropout(inputs, num_classes=3, dropout_rate=0.4):\n    x = LayerNormalization()(inputs)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n    x = Conv2D(num_classes, (3, 3), padding='same')(x)\n    return x\n\n# Build the complete model with pyramid, Swin transformer, and enhancement block\ndef build_full_model_with_dropout(input_shape=(128, 128, 3), num_classes=3, dropout_rate=0.4):\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape)\n    swin_output = recursive_swin_transformer_block(pyramid_features)\n    output = image_restoration_and_enhancement_with_dropout(swin_output, num_classes=num_classes, dropout_rate=dropout_rate)\n    model = Model(inputs=inputs, outputs=output)\n\n    # Model summary\n    model.summary()\n    return model\n\n# Example usage:\n\n# Load data\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, augment=True)\n\n# Build and compile the full model\nmodel = build_full_model_with_dropout(input_shape=(128, 128, 3), dropout_rate=0.4)\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n# Train the model\nmodel.fit(rainy_train, clear_train, epochs=50, batch_size=32, validation_split=0.1)\n\n# Evaluate the model on the test data\nmodel.evaluate(rainy_test, clear_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []  # Corrected initialization\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\nprint(f\"Training Rainy Images Shape: {train_rainy.shape}\")\nprint(f\"Training Clear Images Shape: {train_clear.shape}\")\nprint(f\"Testing Rainy Images Shape: {test_rainy.shape}\")\nprint(f\"Testing Clear Images Shape: {test_clear.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Last implementation","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torch.nn import functional as F\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom sklearn.model_selection import train_test_split\n\n# Function to load images from a specified folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)  # Read image using skimage\n        img = img_as_float(img)  # Convert to float\n        img_resized = transforms.functional.resize(torch.tensor(img), size)  # Resize image\n        images.append(img_resized.numpy())  # Append to list\n    return np.array(images)  # Convert list to numpy array\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        # Apply augmentation (example: flipping, rotation)\n        transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(),  # Random horizontal flip\n            transforms.RandomRotation(15),  # Random rotation within 15 degrees\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Color jitter\n            transforms.ToTensor()  # Convert image to tensor\n        ])\n        images = np.array([transform(img) for img in images])  # Apply transformations\n    return images  # Return preprocessed images\n\n# Dataset class for PyTorch\nclass ImageDataset(data.Dataset):\n    def __init__(self, rainy_images, clear_images, transform=None):\n        self.rainy_images = rainy_images  # Rainy images\n        self.clear_images = clear_images  # Corresponding clear images\n        self.transform = transform  # Transformation to apply\n\n    def __len__(self):\n        return len(self.rainy_images)  # Return the total number of samples\n\n    def __getitem__(self, idx):\n        rainy_image = self.rainy_images[idx]  # Get rainy image\n        clear_image = self.clear_images[idx]  # Get corresponding clear image\n        if self.transform:\n            rainy_image = self.transform(rainy_image)  # Apply transformation\n            clear_image = self.transform(clear_image)  # Apply transformation\n        return rainy_image, clear_image  # Return pair of images\n\n# Function to load datasets from the base folder\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []  # Lists for training images\n    rainy_images_test, clear_images_test = [], []  # Lists for testing images\n\n    datasets = ['Rain200L', 'Rain200H']  # Example dataset folders\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')  # Path to training folder\n        test_folder = os.path.join(base_folder, dataset, 'test')  # Path to testing folder\n\n        # Load training images\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))  # Load rainy images\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))  # Load clear images\n\n        # Load testing images\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))  # Load rainy test images\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))  # Load clear test images\n\n    rainy_images_train = np.array(rainy_images_train)  # Convert to numpy array\n    clear_images_train = np.array(clear_images_train)  # Convert to numpy array\n    rainy_images_test = np.array(rainy_images_test)  # Convert to numpy array\n    clear_images_test = np.array(clear_images_test)  # Convert to numpy array\n\n    if augment:  # Check if augmentation is enabled\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)  # Augment training images\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)  # Augment training images\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)  # Return datasets\n\n# Fully connected multilayer perceptron class\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features  # If out_features is None, use in_features\n        hidden_features = hidden_features or in_features  # If hidden_features is None, use in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)  # First fully connected layer\n        self.act = act_layer()  # Activation layer\n        self.fc2 = nn.Linear(hidden_features, out_features)  # Second fully connected layer\n        self.drop = nn.Dropout(drop)  # Dropout layer\n\n    def forward(self, x):\n        x = self.fc1(x)  # Pass through first layer\n        x = self.act(x)  # Activation\n        x = self.drop(x)  # Dropout\n        x = self.fc2(x)  # Pass through second layer\n        x = self.drop(x)  # Dropout\n        return x  # Return output\n\n# Function to partition the input into windows\ndef window_partition(x, window_size):\n    B, H, W, C = x.shape  # Extract dimensions\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)  # Reshape\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)  # Permute dimensions\n    return windows  # Return windows\n\n# Function to reverse the partitioning process\ndef window_reverse(windows, window_size, H, W):\n    B = int(windows.shape[0] / (H * W / window_size / window_size))  # Calculate batch size\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)  # Reshape\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)  # Permute dimensions\n    return x  # Return original shape\n\n# Window attention class\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0., top_k=10):\n        super().__init__()\n        self.dim = dim  # Dimension of input\n        self.window_size = window_size  # Size of the attention window\n        self.num_heads = num_heads  # Number of attention heads\n        self.top_k = top_k  # Retain top-k attention scores\n        head_dim = dim // num_heads  # Dimension per head\n        self.scale = qk_scale or head_dim ** -0.5  # Scaling factor\n\n        # Relative position bias table\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n\n        coords_h = torch.arange(self.window_size[0])  # Height coordinates\n        coords_w = torch.arange(self.window_size[1])  # Width coordinates\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # Mesh grid\n        coords_flatten = torch.flatten(coords, 1)  # Flatten coordinates\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # Calculate relative coordinates\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Permute dimensions\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # Adjust coordinates\n        relative_coords[:, :, 1] += self.window_size[1] - 1  # Adjust coordinates\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1  # Calculate index\n        relative_position_index = relative_coords.sum(-1)  # Sum to get index\n        self.register_buffer(\"relative_position_index\", relative_position_index)  # Register buffer\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Linear layer for Q, K, V\n        self.attn_drop = nn.Dropout(attn_drop)  # Dropout layer for attention\n        self.proj = nn.Linear(dim, dim)  # Projection layer\n        self.proj_drop = nn.Dropout(proj_drop)  # Dropout layer for projection\n\n    def forward(self, x):\n        B, N, C = x.shape  # Extract dimensions\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3, 4)  # Calculate Q, K, V\n        q, k, v = qkv.unbind(2)  # Unbind into Q, K, V\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # Calculate attention scores\n        attn = attn.softmax(dim=-1)  # Softmax to get attention weights\n        attn = self.attn_drop(attn)  # Apply dropout\n        x = (attn @ v)  # Apply attention to V\n        x = x.transpose(1, 2).reshape(B, N, C)  # Reshape output\n        x = self.proj(x)  # Project output\n        x = self.proj_drop(x)  # Apply dropout\n        return x  # Return output\n\n# Swin Transformer block class\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self, dim, input_resolution, num_heads, window_size, shift_size=0, mlp_ratio=4., qkv_bias=True,\n                 qk_scale=None, drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution  # Input resolution\n        self.dim = dim  # Dimension of input\n        self.window_size = window_size  # Size of the window\n        self.shift_size = shift_size  # Shift size\n        self.norm1 = norm_layer(dim)  # Normalization layer\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.norm2 = norm_layer(dim)  # Second normalization layer\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), drop=drop)  # MLP\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()  # DropPath for residual connections\n\n    def forward(self, x):\n        H, W = self.input_resolution  # Get input resolution\n        shortcut = x  # Store shortcut for residual connection\n        x = self.norm1(x)  # Apply normalization\n        x = window_partition(x, self.window_size)  # Partition into windows\n        x = self.attn(x)  # Apply attention\n        x = window_reverse(x, self.window_size, H, W)  # Reverse partitioning\n        x = shortcut + self.drop_path(x)  # Residual connection\n\n        shortcut = x  # Store shortcut again\n        x = self.norm2(x)  # Apply second normalization\n        x = self.mlp(x)  # Apply MLP\n        x = shortcut + self.drop_path(x)  # Second residual connection\n\n        return x  # Return output\n\n# Patch embedding class\nclass PatchEmbed(nn.Module):\n    def __init__(self, img_size=128, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        self.img_size = img_size  # Image size\n        self.patch_size = patch_size  # Patch size\n        self.num_patches = (img_size // patch_size) ** 2  # Number of patches\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)  # Convolutional layer\n        self.norm = norm_layer(embed_dim) if norm_layer else None  # Normalization layer\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)  # Project and flatten patches\n        if self.norm is not None:\n            x = self.norm(x)  # Apply normalization if defined\n        return x  # Return output\n\n# Hierarchical Swin Transformer class\nclass HierarchicalSwinTransformer(nn.Module):\n    def __init__(self, img_size=128, patch_size=4, in_chans=3, embed_dim=96, num_heads=4, window_size=7, mlp_ratio=4., \n                 depths=[2, 2, 2, 2], drop_rate=0., drop_path_rate=0.1, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)  # Patch embedding\n        \n        self.num_layers = len(depths)  # Number of layers\n        self.layers = nn.ModuleList()  # List of layers\n        for i in range(self.num_layers):\n            layer = nn.ModuleList([\n                SwinTransformerBlock(\n                    dim=embed_dim * (2 ** i),  # Update dimension for each layer\n                    input_resolution=(img_size // (2 ** i), img_size // (2 ** i)),  # Update input resolution\n                    num_heads=num_heads,  # Number of heads\n                    window_size=window_size,\n                    drop_path=drop_path_rate * (i + 1) / self.num_layers,  # Drop path rate\n                    norm_layer=norm_layer  # Normalization layer\n                ) for _ in range(depths[i])  # Create multiple blocks per layer\n            ])\n            self.layers.append(layer)  # Add layer to list\n\n        self.norm = norm_layer(embed_dim * (2 ** (self.num_layers - 1)))  # Final normalization layer\n\n    def forward(self, x):\n        x = self.patch_embed(x)  # Patch embedding\n\n        for layer in self.layers:  # Iterate over layers\n            for block in layer:  # Iterate over blocks\n                x = block(x)  # Forward pass through block\n\n        x = self.norm(x)  # Apply final normalization\n        return x  # Return output\n\n# Loss function for image restoration: Mean Squared Error\nclass RestorationLoss(nn.Module):\n    def __init__(self):\n        super(RestorationLoss, self).__init__()\n\n    def forward(self, output, target):\n        return F.mse_loss(output, target)  # Calculate and return MSE loss\n\n# Main training function\ndef train_model(model, train_loader, optimizer, criterion, num_epochs=10, device='cpu'):\n    model.to(device)  # Move model to device\n    for epoch in range(num_epochs):  # Iterate over epochs\n        model.train()  # Set model to training mode\n        running_loss = 0.0  # Initialize running loss\n        for i, (rainy_images, clear_images) in enumerate(train_loader):  # Iterate over batches\n            rainy_images, clear_images = rainy_images.to(device), clear_images.to(device)  # Move data to device\n            optimizer.zero_grad()  # Zero the gradients\n            outputs = model(rainy_images)  # Forward pass\n            loss = criterion(outputs, clear_images)  # Calculate loss\n            loss.backward()  # Backward pass\n            optimizer.step()  # Optimize model\n            running_loss += loss.item()  # Accumulate loss\n        \n        epoch_loss = running_loss / len(train_loader)  # Average loss for the epoch\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')  # Print epoch loss\n\n# Main script\nif __name__ == '__main__':\n    base_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n    (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test) = load_and_preprocess_datasets(base_folder)\n\n    # Create dataset and data loader\n    train_dataset = ImageDataset(rainy_images_train, clear_images_train, transform=None)  # Create dataset\n    train_loader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create data loader\n\n    # Initialize model, optimizer, and loss function\n    model = HierarchicalSwinTransformer()  # Create model\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)  # Adam optimizer\n    criterion = RestorationLoss()  # Loss function\n\n    # Train the model\n    train_model(model, train_loader, optimizer, criterion, num_epochs=20, device='cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'swin_transformer_model.pth')  # Save model state\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Advanced Dynamic Pyramid Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp  # Import TensorFlow Probability for percentile calculation\nfrom keras.layers import Input, Conv2D, Add, Lambda, LayerNormalization, MultiHeadAttention, Dense, Layer, UpSampling2D\nfrom tensorflow.keras.models import Model\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        x_conv = Conv2D(64 * scale_factor, (kernel_size, kernel_size), activation='relu', padding='same')(x_scaled)\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Custom Layer: Window Partition Layer\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        windowed = tf.image.extract_patches(\n            inputs,\n            sizes=[1, self.window_size, self.window_size, 1],\n            strides=[1, self.window_size, self.window_size, 1],\n            rates=[1, 1, 1, 1],\n            padding='VALID'\n        )\n        windowed = tf.reshape(windowed, (batch_size, -1, self.window_size * self.window_size, channels))\n        return windowed\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], (input_shape[1] // self.window_size) * (input_shape[2] // self.window_size), self.window_size * self.window_size, input_shape[3])\n\n# Custom Layer: Patch Merge Layer\nclass PatchMergeLayer(Layer):\n    def __init__(self, window_size):\n        super(PatchMergeLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, num_windows, flattened_window_size, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        merged = tf.reshape(inputs, (batch_size, int(num_windows**0.5), int(num_windows**0.5), self.window_size, self.window_size, channels))\n        merged = tf.transpose(merged, perm=[0, 1, 3, 2, 4, 5])\n        merged = tf.reshape(merged, (batch_size, merged.shape[1] * self.window_size, merged.shape[3] * self.window_size, channels))\n        return merged, self.window_size * 2\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1] * 2, input_shape[2] * 2, input_shape[3])\n\n# Modified Swin Transformer Block with dynamic attention filtering\nclass ModifiedSwinTransformerBlock(Layer):\n    def __init__(self, initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3):\n        super(ModifiedSwinTransformerBlock, self).__init__()\n        self.initial_window_size = initial_window_size\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.num_recursions = num_recursions\n\n    def call(self, fused_features):\n        original_shape = tf.shape(fused_features)\n        return self.recursive_block(fused_features, self.initial_window_size, 2, self.num_recursions, original_shape)\n\n    def recursive_block(self, x, window_size, iteration, recursion, original_shape):\n        if recursion == 0:\n            return x\n        \n        for _ in range(iteration):\n            # Partition the window\n            partition_layer = WindowPartitionLayer(window_size)\n            windows = partition_layer(x)\n\n            window_shape = (tf.shape(windows)[-2], tf.shape(windows)[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], tf.shape(x)[-1]))\n\n            # Apply LayerNormalization and MultiHeadAttention\n            x_norm = LayerNormalization(axis=-1)(windows_reshaped)\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n\n            # Dynamically filter 60-80% attention values based on a threshold\n            attention_scores = tf.reduce_mean(tf.abs(attention), axis=-1, keepdims=True)\n            threshold = tfp.stats.percentile(attention_scores, tf.random.uniform([], 60, 80))\n            attention = tf.where(attention_scores > threshold, attention, tf.zeros_like(attention))\n\n            # Residual connection\n            x_add = Add()([windows_reshaped, attention])\n            x_reconstructed = self.window_reverse(x_add, original_shape)\n\n            # Apply LayerNormalization and FFN\n            x_norm_ffn = LayerNormalization(axis=-1)(x_reconstructed)\n            x_ffn = Dense(128, activation='relu')(x_norm_ffn)\n            x_ffn_out = Dense(tf.shape(x)[-1])(x_ffn)\n\n            x_out = Add()([x_reconstructed, x_ffn_out])\n\n            # Merge the patches and enlarge window size for next iteration\n            patch_merge_layer = PatchMergeLayer(window_size)\n            x_out, window_size = patch_merge_layer(x_out)\n\n        return self.recursive_block(x_out, window_size, iteration, recursion - 1, original_shape)\n\n    def window_reverse(self, x, original_shape):\n        batch_size = tf.shape(x)[0]\n        num_windows = tf.shape(x)[1] // (self.initial_window_size * self.initial_window_size)\n        x = tf.reshape(x, (batch_size, num_windows, self.initial_window_size, self.initial_window_size, -1))\n        return tf.reshape(x, (batch_size, original_shape[1], original_shape[2], -1))\n\n    def compute_output_shape(self, input_shape):\n        # The output shape changes based on recursion and merging, so return the computed shape\n        return input_shape  # You may need to adjust this if shape transforms across layers\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n\n# Full Model Construction\ndef build_full_model(input_shape):\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_block = ModifiedSwinTransformerBlock(initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3)\n    transformer_output = transformer_block(pyramid_features)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\n# Compile the model\ninput_shape = (256, 256, 3)  # Example input shape\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Window Partition and Reverse Functions","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        patches = tf.image.extract_patches(images=inputs,\n                                           sizes=[1, self.window_size, self.window_size, 1],\n                                           strides=[1, self.window_size, self.window_size, 1],\n                                           rates=[1, 1, 1, 1],\n                                           padding='VALID')\n        return patches\n\ndef window_partition(x, window_size):\n    partition_layer = WindowPartitionLayer(window_size)\n    return partition_layer(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patch Merging Function","metadata":{}},{"cell_type":"code","source":"# Patch Merging Function\ndef patch_merge(x, window_size):\n    \"\"\"Merge adjacent patches to create a larger feature window.\"\"\"\n    partition_layer = WindowPartitionLayer(window_size)\n    patches = partition_layer(x)\n\n    # Calculate the new merged patch (by averaging or pooling)\n    merged_patches = tf.reduce_mean(patches, axis=-1, keepdims=True)\n\n    # Reshape into the original window size\n    new_window_size = window_size * 2\n    merged = window_reverse(merged_patches, new_window_size, x.shape)\n\n    return merged, new_window_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modified Swin Transformer Block with Recursion and Iteration","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\n\ndef modified_swin_transformer_block(fused_features, initial_window_size, num_heads, key_dim, num_recursions):\n    # Define a recursive block as a Keras layer\n    class RecursiveBlock(Layer):\n        def __init__(self, window_size, num_heads, key_dim, num_recursions):\n            super().__init__()\n            self.window_size = window_size\n            self.num_heads = num_heads\n            self.key_dim = key_dim\n            self.num_recursions = num_recursions\n\n        def call(self, x):\n            if self.num_recursions <= 0:\n                return x\n            \n            # Partition the input into windows\n            windows = window_partition(x, self.window_size)\n            window_shape = (windows.shape[-2], windows.shape[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], x.shape[-1]))\n\n            # Layer Normalization before Attention\n            x_norm = LayerNormalization()(windows_reshaped)\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n\n            # Apply reverse window operation\n            x_out = window_reverse(attention, window_shape, x.shape)\n            x_out, window_size = patch_merge(x_out, window_size)\n\n            # Recur on the output\n            return RecursiveBlock(window_size, self.num_heads, self.key_dim, self.num_recursions - 1)(x_out)\n\n    # Instantiate and call the recursive block\n    recursive_layer = RecursiveBlock(initial_window_size, num_heads, key_dim, num_recursions)\n    return recursive_layer(fused_features)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, Conv2D, UpSampling2D\n\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Construct and Compile the Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n\ndef build_full_model(input_shape):\n    inputs = Input(shape=input_shape)\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_output = modified_swin_transformer_block(fused_features=pyramid_features, \n                                                         initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for training\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Load and preprocess datasets, resizing images to (256, 256)\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(256, 256), augment=True)\n\n# Assign training and validation data\ntrain_data = train_rainy  # Input for training\ntrain_labels = train_clear  # Target/Label for training\n\nval_data = test_rainy  # Input for validation\nval_labels = test_clear  # Target/Label for validation\n\n# Set up input shape and define the model\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\n\n# Compile the model with Adam optimizer and MSE loss\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Define callbacks for saving the best model and early stopping\ncheckpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Define training parameters\nepochs = 50\nbatch_size = 16\n\n# Train the model\nhistory = model.fit(train_data, train_labels,\n                    validation_data=(val_data, val_labels),\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[checkpoint, early_stopping])\n\n# Save the final model\nmodel.save('final_model.keras')\n\n# Optionally, plot the training and validation loss over epochs\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for evaluation\nimport tensorflow as tf\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Evaluate the model on the test data\ntest_loss, test_accuracy = model.evaluate(test_data, test_labels)\n\n# Print the test results\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Calculation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for PSNR and SSIM calculations\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport numpy as np\n\n# Function to calculate PSNR\ndef calculate_psnr(ground_truth, prediction):\n    return peak_signal_noise_ratio(ground_truth, prediction, data_range=1.0)\n\n# Function to calculate SSIM\ndef calculate_ssim(ground_truth, prediction):\n    return structural_similarity(ground_truth, prediction, multichannel=True, data_range=1.0)\n\n# Use the trained model to predict on the test data\npredictions = model.predict(test_data)\n\n# Initialize lists to store PSNR and SSIM values\npsnr_list = []\nssim_list = []\n\n# Loop through each test image and calculate PSNR and SSIM\nfor i in range(len(test_data)):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    psnr = calculate_psnr(gt_image, pred_image)\n    ssim = calculate_ssim(gt_image, pred_image)\n    \n    psnr_list.append(psnr)\n    ssim_list.append(ssim)\n\n# Calculate average PSNR and SSIM\naverage_psnr = np.mean(psnr_list)\naverage_ssim = np.mean(ssim_list)\n\n# Print the results\nprint(f\"Average PSNR: {average_psnr:.4f}\")\nprint(f\"Average SSIM: {average_ssim:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Evaluation on Test Images","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for visualization\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Predict on test data\npredictions = model.predict(test_data)\n\n# Visualize the results\nnum_images_to_display = 3\n\nfor i in range(num_images_to_display):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    # Display ground truth and prediction side by side\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Ground Truth\")\n    plt.imshow((gt_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Predicted\")\n    plt.imshow((pred_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Evaluation Across Test Set","metadata":{}},{"cell_type":"code","source":"# PSNR and SSIM Evaluation Across the Test Set\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\n# Function to evaluate PSNR and SSIM for the entire test set\ndef evaluate_test_set_psnr_ssim(test_data, test_labels, predictions):\n    psnr_values = []\n    ssim_values = []\n\n    for i in range(len(test_data)):\n        gt_image = test_labels[i]\n        pred_image = predictions[i]\n        \n        psnr = peak_signal_noise_ratio(gt_image, pred_image, data_range=1.0)\n        ssim = structural_similarity(gt_image, pred_image, multichannel=True, data_range=1.0)\n        \n        psnr_values.append(psnr)\n        ssim_values.append(ssim)\n\n    # Compute the average PSNR and SSIM\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    print(f\"Average PSNR: {avg_psnr:.4f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    \n    return avg_psnr, avg_ssim\n\n# Get predictions from the model\npredictions = model.predict(test_data)\n\n# Evaluate PSNR and SSIM on the test set\navg_psnr, avg_ssim = evaluate_test_set_psnr_ssim(test_data, test_labels, predictions)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of Training History","metadata":{}},{"cell_type":"code","source":"# Import libraries for visualization\nimport matplotlib.pyplot as plt\n\n# Plot the training and validation loss\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot the training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Show the plots\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}