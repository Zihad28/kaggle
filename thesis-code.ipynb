{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9457906,"sourceType":"datasetVersion","datasetId":5749623}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Conv2D, Add, Lambda, LayerNormalization, MultiHeadAttention, Dense, Layer, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        kernel_size = 3 + scale\n        x_conv = Conv2D(64 * scale_factor, (kernel_size, kernel_size), activation='relu', padding='same')(x_scaled)\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    fused_features = Lambda(lambda x: tf.concat(x, axis=-1))(pyramid_features)\n    return inputs, fused_features\n\n# Custom Layer: Window Partition Layer\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        windowed = tf.image.extract_patches(\n            inputs,\n            sizes=[1, self.window_size, self.window_size, 1],\n            strides=[1, self.window_size, self.window_size, 1],\n            rates=[1, 1, 1, 1],\n            padding='VALID'\n        )\n        windowed = tf.reshape(windowed, (batch_size, -1, self.window_size * self.window_size, tf.shape(inputs)[-1]))\n        return windowed\n\n# Custom Layer: Patch Merge Layer\nclass PatchMergeLayer(Layer):\n    def __init__(self, window_size):\n        super(PatchMergeLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, num_patches, flattened_window_size, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        merged = tf.reshape(inputs, (batch_size, int(num_patches**0.5), int(num_patches**0.5), self.window_size, self.window_size, channels))\n        merged = tf.reshape(merged, (batch_size, int(num_patches**0.5) * self.window_size, int(num_patches**0.5) * self.window_size, channels))\n        return merged\n\nclass ModifiedSwinTransformerBlock(Layer):\n    def __init__(self, initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3):\n        super(ModifiedSwinTransformerBlock, self).__init__()\n        self.initial_window_size = initial_window_size\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.num_recursions = num_recursions\n\n    def call(self, fused_features):\n        original_shape = tf.shape(fused_features)  # Shape: (batch_size, 128, 128, 448)\n        x = self.recursive_block(fused_features, self.initial_window_size, 2, self.num_recursions, original_shape)\n        return x\n\n    def recursive_block(self, x, window_size, iteration, recursion, original_shape):\n        if recursion == 0:\n            return x\n        \n        for _ in range(iteration):\n            partition_layer = WindowPartitionLayer(window_size)\n            windows = partition_layer(x)  # Shape: (batch_size, num_patches, window_size * window_size, channels)\n\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(windows, windows)  # Shape: (batch_size, num_patches, window_size * window_size, channels)\n\n            # Reshape attention output back to original window shape\n            attention_reshaped = tf.reshape(attention, tf.shape(windows))  # Shape: (batch_size, num_patches, window_size * window_size, channels)\n            x_add = Add()([windows, attention_reshaped])  # Shape: (batch_size, num_patches, window_size * window_size, channels)\n\n            # Normalization and Feed Forward Network\n            x_norm_ffn = LayerNormalization(axis=-1)(x_add)  # Shape: (batch_size, num_patches, window_size * window_size, channels)\n            x_ffn = Dense(128, activation='relu')(x_norm_ffn)  # Shape: (batch_size, num_patches, window_size * window_size, 128)\n            x_ffn_out = Dense(tf.shape(x)[-1])(x_ffn)  # Shape: (batch_size, num_patches, window_size * window_size, channels)\n\n            # Reverse window partitioning to original spatial dimensions\n            x_reconstructed = self.window_reverse(x_ffn_out, original_shape)  # Ensure shape matches original\n\n            # Update window size for the next iteration\n            window_size *= 2  # Double the window size\n\n        return self.recursive_block(x_reconstructed, window_size, iteration, recursion - 1, original_shape)\n\n    def window_reverse(self, x, original_shape):\n        # Implement reverse logic based on windowed output to reconstruct original shape\n        # Ensure that the output shape matches original shape required for LayerNormalization\n        batch_size = tf.shape(x)[0]\n        return tf.reshape(x, (batch_size, original_shape[1], original_shape[2], -1))  # Ensure compatibility\n\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    outputs = Conv2D(num_classes, (1, 1), activation='sigmoid')(x)\n    return outputs\n\ndef build_model(input_shape):\n    inputs, fused_features = advanced_dynamic_pyramid_model(input_shape)\n    \n    # Use the modified Swin Transformer block for processing\n    transformer_block = ModifiedSwinTransformerBlock(initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3)\n    x_out = transformer_block(fused_features)\n\n    # Image restoration and enhancement\n    outputs = image_restoration_and_enhancement(x_out)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Load and preprocess the datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\n(input_shape, num_classes) = (128, 128, 3), 3  # Example shape and number of classes\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Build the model\nmodel = build_model(input_shape)\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n# Callbacks for training\ncallbacks = [\n    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss'),\n    EarlyStopping(patience=5, restore_best_weights=True)\n]\n\n# Train the model\nhistory = model.fit(rainy_train, clear_train, validation_split=0.1, epochs=50, batch_size=16, callbacks=callbacks)\n\n# Evaluate the model on the test set\ntest_loss, test_mae = model.evaluate(rainy_test, clear_test)\n\n# Visualize training history\nplt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\n# Function to visualize results\ndef visualize_results(rainy_images, clear_images, model):\n    predictions = model.predict(rainy_images)\n\n    for i in range(len(rainy_images)):\n        plt.figure(figsize=(12, 6))\n\n        plt.subplot(1, 3, 1)\n        plt.title(\"Rainy Image\")\n        plt.imshow(rainy_images[i])\n\n        plt.subplot(1, 3, 2)\n        plt.title(\"Ground Truth\")\n        plt.imshow(clear_images[i])\n\n        plt.subplot(1, 3, 3)\n        plt.title(\"Model Prediction\")\n        plt.imshow(predictions[i])\n\n        plt.show()\n\n# Visualize some predictions\nvisualize_results(rainy_test[:5], clear_test[:5], model)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-23T10:01:52.052470Z","iopub.execute_input":"2024-09-23T10:01:52.053048Z","iopub.status.idle":"2024-09-23T10:03:38.918147Z","shell.execute_reply.started":"2024-09-23T10:01:52.052999Z","shell.execute_reply":"2024-09-23T10:03:38.916719Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 204\u001b[0m\n\u001b[1;32m    201\u001b[0m (rainy_train, clear_train), (rainy_test, clear_test) \u001b[38;5;241m=\u001b[39m load_and_preprocess_datasets(base_folder, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m), augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Callbacks for training\u001b[39;00m\n","Cell \u001b[0;32mIn[18], line 190\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Use the modified Swin Transformer block for processing\u001b[39;00m\n\u001b[1;32m    189\u001b[0m transformer_block \u001b[38;5;241m=\u001b[39m ModifiedSwinTransformerBlock(initial_window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, key_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_recursions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 190\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Image restoration and enhancement\u001b[39;00m\n\u001b[1;32m    193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m image_restoration_and_enhancement(x_out)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[18], line 139\u001b[0m, in \u001b[0;36mModifiedSwinTransformerBlock.call\u001b[0;34m(self, fused_features)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, fused_features):\n\u001b[1;32m    138\u001b[0m     original_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(fused_features)  \u001b[38;5;66;03m# Shape: (batch_size, 128, 128, 448)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_window_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_recursions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","Cell \u001b[0;32mIn[18], line 159\u001b[0m, in \u001b[0;36mModifiedSwinTransformerBlock.recursive_block\u001b[0;34m(self, x, window_size, iteration, recursion, original_shape)\u001b[0m\n\u001b[1;32m    157\u001b[0m x_norm_ffn \u001b[38;5;241m=\u001b[39m LayerNormalization(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(x_add)  \u001b[38;5;66;03m# Shape: (batch_size, num_patches, window_size * window_size, channels)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m x_ffn \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x_norm_ffn)  \u001b[38;5;66;03m# Shape: (batch_size, num_patches, window_size * window_size, 128)\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m x_ffn_out \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ffn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size, num_patches, window_size * window_size, channels)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Reverse window partitioning to original spatial dimensions\u001b[39;00m\n\u001b[1;32m    162\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_reverse(x_ffn_out, original_shape)  \u001b[38;5;66;03m# Ensure shape matches original\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling ModifiedSwinTransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'modified_swin_transformer_block_10' (of type ModifiedSwinTransformerBlock). Either the `ModifiedSwinTransformerBlock.call()` method is incorrect, or you need to implement the `ModifiedSwinTransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInvalid dtype: <property object at 0x791c0d366a70>\u001b[0m\n\nArguments received by ModifiedSwinTransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 128, 128, 448), dtype=float32, sparse=False, name=keras_tensor_370>',)\n  • kwargs=<class 'inspect._empty'>"],"ename":"RuntimeError","evalue":"Exception encountered when calling ModifiedSwinTransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'modified_swin_transformer_block_10' (of type ModifiedSwinTransformerBlock). Either the `ModifiedSwinTransformerBlock.call()` method is incorrect, or you need to implement the `ModifiedSwinTransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInvalid dtype: <property object at 0x791c0d366a70>\u001b[0m\n\nArguments received by ModifiedSwinTransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 128, 128, 448), dtype=float32, sparse=False, name=keras_tensor_370>',)\n  • kwargs=<class 'inspect._empty'>","output_type":"error"}]},{"cell_type":"code","source":"pip install torchsummary\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T10:30:55.815967Z","iopub.execute_input":"2024-09-23T10:30:55.816435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torchinfo\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Swin transformer block","metadata":{}},{"cell_type":"code","source":"def swin_transformer_block(inputs):\n    # Here you would implement your actual Swin Transformer logic\n    \n    \n    # --------------------------------------------------------\n    # Swin Transformer Code\n    # --------------------------------------------------------\n\n    import torch\n    import torch.nn as nn\n    import torch.utils.checkpoint as checkpoint\n    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n    try:\n        import os, sys\n\n        kernel_path = os.path.abspath(os.path.join('..'))\n        sys.path.append(kernel_path)\n        from kernels.window_process.window_process import WindowProcess, WindowProcessReverse\n\n    except:\n        WindowProcess = None\n        WindowProcessReverse = None\n        print(\"[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\")\n\n    class Mlp(nn.Module):\n        def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n            super().__init__()\n            out_features = out_features or in_features\n            hidden_features = hidden_features or in_features\n            self.fc1 = nn.Linear(in_features, hidden_features)\n            self.act = act_layer()\n            self.fc2 = nn.Linear(hidden_features, out_features)\n            self.drop = nn.Dropout(drop)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            x = self.act(x)\n            x = self.drop(x)\n            x = self.fc2(x)\n            x = self.drop(x)\n            return x\n\n    def window_partition(x, window_size):\n        B, H, W, C = x.shape\n        x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n        return windows\n\n    def window_reverse(windows, window_size, H, W):\n        B = int(windows.shape[0] / (H * W / window_size / window_size))\n        x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n        return x\n\n    class WindowAttention(nn.Module):\n        def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n            super().__init__()\n            self.dim = dim\n            self.window_size = window_size\n            self.num_heads = num_heads\n            head_dim = dim // num_heads\n            self.scale = qk_scale or head_dim ** -0.5\n\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n\n            coords_h = torch.arange(self.window_size[0])\n            coords_w = torch.arange(self.window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n            coords_flatten = torch.flatten(coords, 1)\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n            relative_coords[:, :, 0] += self.window_size[0] - 1\n            relative_coords[:, :, 1] += self.window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n            relative_position_index = relative_coords.sum(-1)\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n            self.attn_drop = nn.Dropout(attn_drop)\n            self.proj = nn.Linear(dim, dim)\n            self.proj_drop = nn.Dropout(proj_drop)\n\n            trunc_normal_(self.relative_position_bias_table, std=.02)\n            self.softmax = nn.Softmax(dim=-1)\n\n        def forward(self, x, mask=None):\n            B_, N, C = x.shape\n            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n            q = q * self.scale\n            attn = (q @ k.transpose(-2, -1))\n\n            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n            if mask is not None:\n                nW = mask.shape[0]\n                attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n                attn = attn.view(-1, self.num_heads, N, N)\n                attn = self.softmax(attn)\n            else:\n                attn = self.softmax(attn)\n\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n            return x\n\n        def extra_repr(self) -> str:\n            return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n        \n        def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n        # qkv = self.qkv(x)\n        flops += N * self.dim * 3 * self.dim\n        # attn = (q @ k.transpose(-2, -1))\n        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n        #  x = (attn @ v)\n        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 fused_window_process=False):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n        self.fused_window_process = fused_window_process\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            if not self.fused_window_process:\n                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n                # partition windows\n                x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n            else:\n                x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size)\n        else:\n            shifted_x = x\n            # partition windows\n            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            if not self.fused_window_process:\n                shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n            else:\n                x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size)\n        else:\n            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n            x = shifted_x\n        x = x.view(B, H * W, C)\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        # W-MSA/SW-MSA\n        nW = H * W / self.window_size / self.window_size\n        flops += nW * self.attn.flops(self.window_size * self.window_size)\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = H * W * self.dim\n        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n        return flops\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n                 fused_window_process=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 fused_window_process=fused_window_process)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        if self.downsample is not None:\n            flops += self.downsample.flops()\n        return flops\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\n\nclass SwinTransformer(nn.Module):\n    r\"\"\" Swin Transformer\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n                 use_checkpoint=False, fused_window_process=False, **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n            trunc_normal_(self.absolute_pos_embed, std=.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint,\n                               fused_window_process=fused_window_process)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'absolute_pos_embed'}\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {'relative_position_bias_table'}\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n    def flops(self):\n        flops = 0\n        flops += self.patch_embed.flops()\n        for i, layer in enumerate(self.layers):\n            flops += layer.flops()\n        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n        flops += self.num_features * self.num_classes\n        return flops\n    \n    # For now, let's just apply some Conv layers to simulate processing\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(inputs)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n\n    return x  # Modify this return statement based on your intended output\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:54:15.640141Z","iopub.execute_input":"2024-09-25T18:54:15.640532Z","iopub.status.idle":"2024-09-25T18:54:15.724353Z","shell.execute_reply.started":"2024-09-25T18:54:15.640493Z","shell.execute_reply":"2024-09-25T18:54:15.723103Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[2], line 121\u001b[0;36m\u001b[0m\n\u001b[0;31m    flops = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 119\n"],"ename":"IndentationError","evalue":"expected an indented block after function definition on line 119 (804458131.py, line 121)","output_type":"error"}]},{"cell_type":"markdown","source":"# New Concept","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Add, Lambda, LayerNormalization, Dense, MultiHeadAttention, DepthwiseConv2D\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        \n        # Use Depthwise Separable Convolutions\n        x_conv = DepthwiseConv2D((kernel_size, kernel_size), padding='same', activation='relu')(x_scaled)\n        x_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_conv)  # Pointwise Conv\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Swin Transformer Block Implementation with k-th Attention\nclass SwinTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads, window_size, k=4, shift_size=0, mlp_ratio=4.):\n        super(SwinTransformerBlock, self).__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.num_heads = num_heads\n        self.k = k  # Number of top-k attention weights\n        self.dim = dim\n        \n        # Layer normalization\n        self.norm1 = LayerNormalization(epsilon=1e-5)\n        self.norm2 = LayerNormalization(epsilon=1e-5)\n        \n        # Multi-head self-attention\n        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n        \n        # Feed-forward network\n        self.mlp = self.get_mlp(dim, mlp_ratio)\n\n    def get_mlp(self, dim, mlp_ratio):\n        return tf.keras.Sequential([\n            Dense(int(dim * mlp_ratio), activation='relu'),\n            Dense(dim)\n        ])\n    \n    def build(self, input_shape):\n        pass  # You can initialize any additional weights here if necessary\n\n    def call(self, x):\n        # Layer normalization\n        x_norm = self.norm1(x)\n\n        # Compute attention\n        attn_output = self.attn(x_norm, x_norm)\n\n        # Get top-k attention weights\n        attn_weights = tf.nn.softmax(tf.matmul(x_norm, x_norm, transpose_b=True))  # Computing attention weights\n        top_k_values, indices = tf.nn.top_k(attn_weights, k=self.k)\n        threshold = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n\n        # Suppress weights below the threshold\n        attn_weights = tf.where(attn_weights < threshold, tf.zeros_like(attn_weights), attn_weights)\n\n        # Normalize the weights again after thresholding\n        attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=-1, keepdims=True)\n\n        # Use the modified attention weights to compute output\n        attn_output = tf.matmul(attn_weights, attn_output)\n\n        # Skip connection\n        x = x + attn_output  \n\n        # Feed-forward network\n        x = self.norm2(x)\n        mlp_output = self.mlp(x)\n        x = x + mlp_output  # Skip connection\n        \n        return x\n\ndef swin_transformer_block(inputs):\n    # Define parameters for the Swin Transformer Block\n    dim = inputs.shape[-1]  # Input channel dimension\n    num_heads = 4  # Number of attention heads\n    window_size = 7  # Size of the window\n    shift_size = 0  # Shift size for the windowing scheme\n    \n    # Create the Swin Transformer Block\n    x = SwinTransformerBlock(dim, num_heads, window_size, k=4, shift_size=shift_size)(inputs)\n    \n    return x\n\n# Recursive Swin Transformer Block with residual connections\ndef recursive_swin_transformer_block(inputs, num_recursions=6):\n    x = inputs\n    for _ in range(num_recursions):\n        # Call the Swin Transformer block\n        x = swin_transformer_block(x)\n    \n    return x\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(inputs, num_classes=3):\n    # Normalize the input features\n    x = LayerNormalization()(inputs)\n    \n    # Using Depthwise Separable Convolutions\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(32, (1, 1), padding='same', activation='relu')(x)  # Pointwise Conv\n\n    x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(num_classes, (1, 1), padding='same', activation='sigmoid')(x)  # Final output layer\n\n    return x\n\n# Build the full model\ndef build_full_model(input_shape=(128, 128, 3)):\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n\n    # Create the recursive model for feature extraction\n    transformer_output = recursive_swin_transformer_block(pyramid_features, num_recursions=6)\n\n    # Image restoration and enhancement\n    outputs = image_restoration_and_enhancement(transformer_output, num_classes=input_shape[2])  # Keeping the output channels same as input channels\n\n    # Create and return the model\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Set parameters\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Update with your dataset path\ninput_shape = (128, 128, 3)\n\n# Load and preprocess the dataset\n(rainy_train, clear_train), (rainy_test, clear_test) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\n# Build and compile the model\nmodel = build_full_model(input_shape=input_shape)\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n# Print the model summary\nmodel.summary()\n\n# Optionally, you can train the model\n# model.fit(rainy_train, clear_train, epochs=50, batch_size=32, validation_split=0.1)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:29:14.631976Z","iopub.execute_input":"2024-09-25T19:29:14.632333Z","iopub.status.idle":"2024-09-25T19:31:02.117017Z","shell.execute_reply.started":"2024-09-25T19:29:14.632300Z","shell.execute_reply":"2024-09-25T19:31:02.116171Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_21\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_16      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_28 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_30 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_32 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_6  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │         \u001b[38;5;34m30\u001b[0m │ lambda_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_7  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m) │         \u001b[38;5;34m51\u001b[0m │ lambda_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_8  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m) │         \u001b[38;5;34m78\u001b[0m │ lambda_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m256\u001b[0m │ depthwise_conv2d… │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m256\u001b[0m │ lambda_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ depthwise_conv2d… │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ lambda_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ depthwise_conv2d… │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ lambda_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_18 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │ conv2d_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_19 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │ conv2d_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_20 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │ conv2d_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_29 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_31 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_33 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_34 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ lambda_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m448\u001b[0m)              │            │ lambda_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │                   │            │ lambda_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │  \u001b[38;5;34m4,826,752\u001b[0m │ lambda_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mSwinTransformerBl…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │  \u001b[38;5;34m4,826,752\u001b[0m │ swin_transformer… │\n│ (\u001b[38;5;33mSwinTransformerBl…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │  \u001b[38;5;34m4,826,752\u001b[0m │ swin_transformer… │\n│ (\u001b[38;5;33mSwinTransformerBl…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │  \u001b[38;5;34m4,826,752\u001b[0m │ swin_transformer… │\n│ (\u001b[38;5;33mSwinTransformerBl…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │  \u001b[38;5;34m4,826,752\u001b[0m │ swin_transformer… │\n│ (\u001b[38;5;33mSwinTransformerBl…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │  \u001b[38;5;34m4,826,752\u001b[0m │ swin_transformer… │\n│ (\u001b[38;5;33mSwinTransformerBl…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ swin_transformer… │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_9  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m4,480\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m14,368\u001b[0m │ depthwise_conv2d… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_10 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m320\u001b[0m │ conv2d_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_36 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,056\u001b[0m │ depthwise_conv2d… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_11 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m320\u001b[0m │ conv2d_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_37 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │         \u001b[38;5;34m99\u001b[0m │ depthwise_conv2d… │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_16      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_6  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ lambda_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_7  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │ lambda_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_8  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span> │ lambda_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ depthwise_conv2d… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lambda_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ depthwise_conv2d… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ lambda_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ depthwise_conv2d… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ lambda_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │ conv2d_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ conv2d_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ conv2d_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │ lambda_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ lambda_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,826,752</span> │ lambda_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformerBl…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,826,752</span> │ swin_transformer… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformerBl…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,826,752</span> │ swin_transformer… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformerBl…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,826,752</span> │ swin_transformer… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformerBl…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,826,752</span> │ swin_transformer… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformerBl…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ swin_transformer_b… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,826,752</span> │ swin_transformer… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformerBl…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ swin_transformer… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_9  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,368</span> │ depthwise_conv2d… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_10 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ conv2d_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ depthwise_conv2d… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ depthwise_conv2d_11 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ conv2d_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │ depthwise_conv2d… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,985,794\u001b[0m (110.57 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,985,794</span> (110.57 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,985,794\u001b[0m (110.57 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,985,794</span> (110.57 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Training the model\nhistory = model.fit(rainy_train, clear_train, epochs=50, batch_size=32, validation_split=0.1)\n\n# Function to calculate PSNR\ndef calculate_psnr(target, ref):\n    return 20 * np.log10(1.0 / np.sqrt(mean_squared_error(target.flatten(), ref.flatten())))\n\n# Evaluate the model\ndef evaluate_model(model, rainy_test, clear_test):\n    predictions = model.predict(rainy_test)\n\n    psnr_values = []\n    ssim_values = []\n    \n    for i in range(len(predictions)):\n        psnr = calculate_psnr(clear_test[i], predictions[i])\n        ssim_value = ssim(clear_test[i], predictions[i], multichannel=True)\n        psnr_values.append(psnr)\n        ssim_values.append(ssim_value)\n\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    return predictions, avg_psnr, avg_ssim\n\n# Evaluating the model\npredictions, avg_psnr, avg_ssim = evaluate_model(model, rainy_test, clear_test)\nprint(f\"Average PSNR: {avg_psnr:.2f} dB\")\nprint(f\"Average SSIM: {avg_ssim:.4f}\")\n\n# Plotting results\ndef plot_results(rainy_images, clear_images, predicted_images, n=5):\n    plt.figure(figsize=(15, 5))\n    for i in range(n):\n        plt.subplot(3, n, i + 1)\n        plt.imshow(rainy_images[i])\n        plt.title(\"Rainy Image\")\n        plt.axis(\"off\")\n\n        plt.subplot(3, n, i + 1 + n)\n        plt.imshow(clear_images[i])\n        plt.title(\"Clear Image\")\n        plt.axis(\"off\")\n\n        plt.subplot(3, n, i + 1 + 2 * n)\n        plt.imshow(predicted_images[i])\n        plt.title(\"Predicted Image\")\n        plt.axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plotting some results\nplot_results(rainy_test, clear_test, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:37:21.327689Z","iopub.execute_input":"2024-09-25T19:37:21.328075Z","iopub.status.idle":"2024-09-25T19:38:00.184599Z","shell.execute_reply.started":"2024-09-25T19:37:21.328042Z","shell.execute_reply":"2024-09-25T19:38:00.183337Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1727293061.240601      98 service.cc:145] XLA service 0x7b91e4093f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1727293061.240658      98 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n2024-09-25 19:37:53.202426: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng18{k11=0} for conv (f32[448,1,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,448,128,128]{3,2,1,0}, f32[32,448,128,128]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=448, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-09-25 19:37:53.618573: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.416237209s\nTrying algorithm eng18{k11=0} for conv (f32[448,1,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,448,128,128]{3,2,1,0}, f32[32,448,128,128]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=448, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrainy_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to calculate PSNR\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_psnr\u001b[39m(target, ref):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_36/1290293688.py\", line 2, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 314, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 117, in one_step_on_iterator\n\nOut of memory while trying to allocate 137455730688 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_224272]"],"ename":"ResourceExhaustedError","evalue":"Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_36/1290293688.py\", line 2, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 314, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 117, in one_step_on_iterator\n\nOut of memory while trying to allocate 137455730688 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_224272]","output_type":"error"}]},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"# Torch imports\nimport torch\nimport torch.nn as nn\nimport torch.cuda as cuda\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom timm.models.layers import trunc_normal_\nfrom torch.utils.data import DataLoader\nfrom torchsummary import summary\n\n# Other imports\nimport numpy as np\nimport glob\nimport os\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom skimage.io import imread\nfrom skimage import img_as_float\nimport warnings\nimport matplotlib.pyplot as plt\n\n# Own files import\nfrom transformer_block import TransformerBlock, PatchEmbed, PatchUnEmbed\nfrom Data.data_loader import Rain800TrainData, Rain800ValData\nfrom my_utils import batch_PSNR, batch_SSIM, output_to_image\nfrom my_utils import save_ckp, load_ckp, base_path\n\n# Hyperparameters\ntraining_image_size = 56\ndtype = torch.cuda.FloatTensor\nbatch_size = 5\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed_all(1234)\nepochs = 4600\nlr = 0.0001\nerror_plot_freq = 20\nINT_MAX = 2147483647\nerror_tolerence = 10\npatch_size = 1\n\n# Paths\nbase_pth = base_path()\nckp_pth = base_pth + \"/CheckPoints\"\n\n# Miscellaneous\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (16, 9)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []\n\n    datasets = ['Rain200L', 'Rain200H']\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Basic Block for Transformer\nclass BasicBlock(nn.Module):\n    def __init__(self, dim, input_resolution, num_heads, patch_size):\n        super().__init__()\n        self.dim = dim\n        self.patch_size = patch_size\n        self.input_resolution = input_resolution\n        H, W = self.input_resolution\n        self.num_heads = num_heads\n        self.transformer = TransformerBlock(self.dim, (H // self.patch_size, W // self.patch_size), num_heads)\n\n    def forward(self, x):\n        x = self.transformer(x)\n        return x\n\n# Residual Layer\nclass ResidualLayer(nn.Module):\n    def __init__(self, dim, input_resolution, residual_depth, patch_size):\n        super().__init__()\n        self.dim = dim\n        self.patch_size = patch_size\n        self.residual_depth = residual_depth\n        self.input_resolution = input_resolution\n        self.block1 = BasicBlock(self.dim, self.input_resolution, 2, self.patch_size)\n        self.block2 = BasicBlock(self.dim, self.input_resolution, 2, self.patch_size)\n        self.conv_out = nn.Conv2d(self.dim, self.dim, 3, padding=1)\n\n    def forward(self, x):\n        B, HW, C = x.shape\n        H, W = self.input_resolution\n        shortcut = x\n        for _ in range(self.residual_depth):\n            x = self.block1(self.block2(x))\n            x = torch.add(x, shortcut)\n        x = x.transpose(1, 2).view(B, C, H // self.patch_size, W // self.patch_size)\n        x = self.conv_out(x).flatten(2).transpose(1, 2)  # B L C\n        return x\n\n# Deep Recursive Transformer Model\nclass DeepRecursiveTransformer(nn.Module):\n    def __init__(self, dim, input_resolution, patch_size, residual_depth, recursive_depth):\n        super().__init__()\n        self.dim = dim\n        self.patch_size = patch_size\n        self.recursive_depth = recursive_depth\n        self.input_resolution = input_resolution\n        self.H, self.W = self.input_resolution\n        assert self.H == self.W, \"Input height and width should be the same\"\n        self.input_conv1 = nn.Conv2d(3, self.dim, 3, padding=1)\n        self.patch_embed = PatchEmbed(img_size=self.H, patch_size=self.patch_size, in_chans=3, embed_dim=self.dim)\n        self.patch_unembed = PatchUnEmbed(img_size=self.H, patch_size=self.patch_size, in_chans=self.dim, unembed_dim=3)\n        self.recursive_layers = nn.ModuleList()\n        for i in range(self.recursive_depth):\n            layer = ResidualLayer(self.dim, self.input_resolution, self.residual_depth, self.patch_size)\n            self.recursive_layers.append(layer)\n        self.output_conv1 = nn.Conv2d(self.dim, 3, 3, padding=1)\n        self.normalise_layer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.denormalise_layer = transforms.Normalize((-0.485, -0.456, -0.406), (1. / 0.229, 1. / 0.224, 1. / 0.225))\n        self.apply(self._init_weights)\n        self.activation = nn.LeakyReLU()\n\n    def _init_weights(self, l):\n        if isinstance(l, nn.Linear):\n            trunc_normal_(l.weight, std=.02)\n            if isinstance(l, nn.Linear) and l.bias is not None:\n                nn.init.constant_(l.bias, 0)\n        elif isinstance(l, nn.LayerNorm):\n            nn.init.constant_(l.bias, 0)\n            nn.init.constant_(l.weight, 1.0)\n\n    def forward(self, x):\n        x = self.normalise_layer(x)\n        outer_shortcut = x\n        x = self.patch_embed(x)\n        inner_shortcut = x\n\n        for i in range(len(self.recursive_layers)):\n            x = self.recursive_layers[i](x)\n\n        x = torch.add(x, inner_shortcut)\n        x = self.patch_unembed(x, (self.H // self.patch_size, self.W // self.patch_size))\n        x = torch.add(x, outer_shortcut)\n        x = self.denormalise_layer(x)\n        return x  # output shape (B, C, H, W)\n\n# Load datasets\nbase_folder = '/kaggle/input/derainingdata/RainData'  # Update this path\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(training_image_size, training_image_size), augment=True)\n\n# Initialize the model\nnet = DeepRecursiveTransformer(96, (training_image_size, training_image_size), patch_size, 3, 6)\nsummary(net.cuda(), (3, training_image_size, training_image_size))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T10:26:49.296787Z","iopub.execute_input":"2024-09-23T10:26:49.297266Z","iopub.status.idle":"2024-09-23T10:26:56.074412Z","shell.execute_reply.started":"2024-09-23T10:26:49.297224Z","shell.execute_reply":"2024-09-23T10:26:56.073039Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trunc_normal_\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Other imports\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torchsummary'","output_type":"error"}]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []  # Corrected initialization\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\nprint(f\"Training Rainy Images Shape: {train_rainy.shape}\")\nprint(f\"Training Clear Images Shape: {train_clear.shape}\")\nprint(f\"Testing Rainy Images Shape: {test_rainy.shape}\")\nprint(f\"Testing Clear Images Shape: {test_clear.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:40:19.839046Z","iopub.execute_input":"2024-09-23T07:40:19.840026Z","iopub.status.idle":"2024-09-23T07:43:08.485820Z","shell.execute_reply.started":"2024-09-23T07:40:19.839967Z","shell.execute_reply":"2024-09-23T07:43:08.484733Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Training Rainy Images Shape: (3600, 128, 128, 3)\nTraining Clear Images Shape: (3600, 128, 128, 3)\nTesting Rainy Images Shape: (400, 128, 128, 3)\nTesting Clear Images Shape: (400, 128, 128, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Advanced Dynamic Pyramid Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp  # Import TensorFlow Probability for percentile calculation\nfrom keras.layers import Input, Conv2D, Add, Lambda, LayerNormalization, MultiHeadAttention, Dense, Layer, UpSampling2D\nfrom tensorflow.keras.models import Model\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        x_conv = Conv2D(64 * scale_factor, (kernel_size, kernel_size), activation='relu', padding='same')(x_scaled)\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Custom Layer: Window Partition Layer\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        windowed = tf.image.extract_patches(\n            inputs,\n            sizes=[1, self.window_size, self.window_size, 1],\n            strides=[1, self.window_size, self.window_size, 1],\n            rates=[1, 1, 1, 1],\n            padding='VALID'\n        )\n        windowed = tf.reshape(windowed, (batch_size, -1, self.window_size * self.window_size, channels))\n        return windowed\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], (input_shape[1] // self.window_size) * (input_shape[2] // self.window_size), self.window_size * self.window_size, input_shape[3])\n\n# Custom Layer: Patch Merge Layer\nclass PatchMergeLayer(Layer):\n    def __init__(self, window_size):\n        super(PatchMergeLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, num_windows, flattened_window_size, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        merged = tf.reshape(inputs, (batch_size, int(num_windows**0.5), int(num_windows**0.5), self.window_size, self.window_size, channels))\n        merged = tf.transpose(merged, perm=[0, 1, 3, 2, 4, 5])\n        merged = tf.reshape(merged, (batch_size, merged.shape[1] * self.window_size, merged.shape[3] * self.window_size, channels))\n        return merged, self.window_size * 2\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1] * 2, input_shape[2] * 2, input_shape[3])\n\n# Modified Swin Transformer Block with dynamic attention filtering\nclass ModifiedSwinTransformerBlock(Layer):\n    def __init__(self, initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3):\n        super(ModifiedSwinTransformerBlock, self).__init__()\n        self.initial_window_size = initial_window_size\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.num_recursions = num_recursions\n\n    def call(self, fused_features):\n        original_shape = tf.shape(fused_features)\n        return self.recursive_block(fused_features, self.initial_window_size, 2, self.num_recursions, original_shape)\n\n    def recursive_block(self, x, window_size, iteration, recursion, original_shape):\n        if recursion == 0:\n            return x\n        \n        for _ in range(iteration):\n            # Partition the window\n            partition_layer = WindowPartitionLayer(window_size)\n            windows = partition_layer(x)\n\n            window_shape = (tf.shape(windows)[-2], tf.shape(windows)[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], tf.shape(x)[-1]))\n\n            # Apply LayerNormalization and MultiHeadAttention\n            x_norm = LayerNormalization(axis=-1)(windows_reshaped)\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n\n            # Dynamically filter 60-80% attention values based on a threshold\n            attention_scores = tf.reduce_mean(tf.abs(attention), axis=-1, keepdims=True)\n            threshold = tfp.stats.percentile(attention_scores, tf.random.uniform([], 60, 80))\n            attention = tf.where(attention_scores > threshold, attention, tf.zeros_like(attention))\n\n            # Residual connection\n            x_add = Add()([windows_reshaped, attention])\n            x_reconstructed = self.window_reverse(x_add, original_shape)\n\n            # Apply LayerNormalization and FFN\n            x_norm_ffn = LayerNormalization(axis=-1)(x_reconstructed)\n            x_ffn = Dense(128, activation='relu')(x_norm_ffn)\n            x_ffn_out = Dense(tf.shape(x)[-1])(x_ffn)\n\n            x_out = Add()([x_reconstructed, x_ffn_out])\n\n            # Merge the patches and enlarge window size for next iteration\n            patch_merge_layer = PatchMergeLayer(window_size)\n            x_out, window_size = patch_merge_layer(x_out)\n\n        return self.recursive_block(x_out, window_size, iteration, recursion - 1, original_shape)\n\n    def window_reverse(self, x, original_shape):\n        batch_size = tf.shape(x)[0]\n        num_windows = tf.shape(x)[1] // (self.initial_window_size * self.initial_window_size)\n        x = tf.reshape(x, (batch_size, num_windows, self.initial_window_size, self.initial_window_size, -1))\n        return tf.reshape(x, (batch_size, original_shape[1], original_shape[2], -1))\n\n    def compute_output_shape(self, input_shape):\n        # The output shape changes based on recursion and merging, so return the computed shape\n        return input_shape  # You may need to adjust this if shape transforms across layers\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n\n# Full Model Construction\ndef build_full_model(input_shape):\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_block = ModifiedSwinTransformerBlock(initial_window_size=4, num_heads=4, key_dim=64, num_recursions=3)\n    transformer_output = transformer_block(pyramid_features)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\n# Compile the model\ninput_shape = (256, 256, 3)  # Example input shape\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T06:59:30.951039Z","iopub.execute_input":"2024-09-23T06:59:30.951986Z","iopub.status.idle":"2024-09-23T06:59:34.144282Z","shell.execute_reply.started":"2024-09-23T06:59:30.951934Z","shell.execute_reply":"2024-09-23T06:59:34.143392Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_14 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_16 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_18 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │      \u001b[38;5;34m1,792\u001b[0m │ lambda_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │        \u001b[38;5;34m256\u001b[0m │ lambda_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m6,272\u001b[0m │ lambda_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m512\u001b[0m │ lambda_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │     \u001b[38;5;34m19,456\u001b[0m │ lambda_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ lambda_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │ conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_15 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_17 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_19 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_20 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ lambda_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │ \u001b[38;5;34m448\u001b[0m)              │            │ lambda_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │                   │            │ lambda_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ modified_swin_tran… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ lambda_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mModifiedSwinTrans…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ modified_swin_tr… │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │ \u001b[38;5;34m448\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │    \u001b[38;5;34m258,112\u001b[0m │ layer_normalizat… │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ up_sampling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mUpSampling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │     \u001b[38;5;34m73,856\u001b[0m │ up_sampling2d[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │        \u001b[38;5;34m256\u001b[0m │ conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │      \u001b[38;5;34m3,459\u001b[0m │ layer_normalizat… │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ lambda_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lambda_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │ lambda_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ lambda_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,456</span> │ lambda_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ lambda_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │ lambda_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ lambda_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ modified_swin_tran… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ModifiedSwinTrans…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ modified_swin_tr… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">258,112</span> │ layer_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ up_sampling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ up_sampling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,459</span> │ layer_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m402,819\u001b[0m (1.54 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">402,819</span> (1.54 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m402,819\u001b[0m (1.54 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">402,819</span> (1.54 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Window Partition and Reverse Functions","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        patches = tf.image.extract_patches(images=inputs,\n                                           sizes=[1, self.window_size, self.window_size, 1],\n                                           strides=[1, self.window_size, self.window_size, 1],\n                                           rates=[1, 1, 1, 1],\n                                           padding='VALID')\n        return patches\n\ndef window_partition(x, window_size):\n    partition_layer = WindowPartitionLayer(window_size)\n    return partition_layer(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:37:06.398262Z","iopub.execute_input":"2024-09-22T21:37:06.399081Z","iopub.status.idle":"2024-09-22T21:37:06.406498Z","shell.execute_reply.started":"2024-09-22T21:37:06.399041Z","shell.execute_reply":"2024-09-22T21:37:06.405448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patch Merging Function","metadata":{}},{"cell_type":"code","source":"# Patch Merging Function\ndef patch_merge(x, window_size):\n    \"\"\"Merge adjacent patches to create a larger feature window.\"\"\"\n    partition_layer = WindowPartitionLayer(window_size)\n    patches = partition_layer(x)\n\n    # Calculate the new merged patch (by averaging or pooling)\n    merged_patches = tf.reduce_mean(patches, axis=-1, keepdims=True)\n\n    # Reshape into the original window size\n    new_window_size = window_size * 2\n    merged = window_reverse(merged_patches, new_window_size, x.shape)\n\n    return merged, new_window_size","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:30:48.687892Z","iopub.execute_input":"2024-09-22T21:30:48.688248Z","iopub.status.idle":"2024-09-22T21:30:48.694236Z","shell.execute_reply.started":"2024-09-22T21:30:48.688214Z","shell.execute_reply":"2024-09-22T21:30:48.693189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modified Swin Transformer Block with Recursion and Iteration","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\n\ndef modified_swin_transformer_block(fused_features, initial_window_size, num_heads, key_dim, num_recursions):\n    # Define a recursive block as a Keras layer\n    class RecursiveBlock(Layer):\n        def __init__(self, window_size, num_heads, key_dim, num_recursions):\n            super().__init__()\n            self.window_size = window_size\n            self.num_heads = num_heads\n            self.key_dim = key_dim\n            self.num_recursions = num_recursions\n\n        def call(self, x):\n            if self.num_recursions <= 0:\n                return x\n            \n            # Partition the input into windows\n            windows = window_partition(x, self.window_size)\n            window_shape = (windows.shape[-2], windows.shape[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], x.shape[-1]))\n\n            # Layer Normalization before Attention\n            x_norm = LayerNormalization()(windows_reshaped)\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n\n            # Apply reverse window operation\n            x_out = window_reverse(attention, window_shape, x.shape)\n            x_out, window_size = patch_merge(x_out, window_size)\n\n            # Recur on the output\n            return RecursiveBlock(window_size, self.num_heads, self.key_dim, self.num_recursions - 1)(x_out)\n\n    # Instantiate and call the recursive block\n    recursive_layer = RecursiveBlock(initial_window_size, num_heads, key_dim, num_recursions)\n    return recursive_layer(fused_features)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:38:27.279997Z","iopub.execute_input":"2024-09-22T21:38:27.280623Z","iopub.status.idle":"2024-09-22T21:38:27.290579Z","shell.execute_reply.started":"2024-09-22T21:38:27.280583Z","shell.execute_reply":"2024-09-22T21:38:27.289577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, Conv2D, UpSampling2D\n\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:37:13.163540Z","iopub.execute_input":"2024-09-22T21:37:13.164274Z","iopub.status.idle":"2024-09-22T21:37:13.171030Z","shell.execute_reply.started":"2024-09-22T21:37:13.164236Z","shell.execute_reply":"2024-09-22T21:37:13.170142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Construct and Compile the Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n\ndef build_full_model(input_shape):\n    inputs = Input(shape=input_shape)\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_output = modified_swin_transformer_block(fused_features=pyramid_features, \n                                                         initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:38:31.546294Z","iopub.execute_input":"2024-09-22T21:38:31.546687Z","iopub.status.idle":"2024-09-22T21:38:31.852519Z","shell.execute_reply.started":"2024-09-22T21:38:31.546651Z","shell.execute_reply":"2024-09-22T21:38:31.851174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for training\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Load and preprocess datasets, resizing images to (256, 256)\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(256, 256), augment=True)\n\n# Assign training and validation data\ntrain_data = train_rainy  # Input for training\ntrain_labels = train_clear  # Target/Label for training\n\nval_data = test_rainy  # Input for validation\nval_labels = test_clear  # Target/Label for validation\n\n# Set up input shape and define the model\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\n\n# Compile the model with Adam optimizer and MSE loss\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Define callbacks for saving the best model and early stopping\ncheckpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Define training parameters\nepochs = 50\nbatch_size = 16\n\n# Train the model\nhistory = model.fit(train_data, train_labels,\n                    validation_data=(val_data, val_labels),\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[checkpoint, early_stopping])\n\n# Save the final model\nmodel.save('final_model.keras')\n\n# Optionally, plot the training and validation loss over epochs\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:39:51.146817Z","iopub.execute_input":"2024-09-23T07:39:51.147701Z","iopub.status.idle":"2024-09-23T07:40:01.040313Z","shell.execute_reply.started":"2024-09-23T07:39:51.147657Z","shell.execute_reply":"2024-09-23T07:40:01.039040Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load and preprocess datasets, resizing images to (256, 256)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m (train_rainy, train_clear), (test_rainy, test_clear) \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_datasets\u001b[49m(base_folder, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assign training and validation data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_rainy  \u001b[38;5;66;03m# Input for training\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'load_and_preprocess_datasets' is not defined"],"ename":"NameError","evalue":"name 'load_and_preprocess_datasets' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for evaluation\nimport tensorflow as tf\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Evaluate the model on the test data\ntest_loss, test_accuracy = model.evaluate(test_data, test_labels)\n\n# Print the test results\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Calculation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for PSNR and SSIM calculations\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport numpy as np\n\n# Function to calculate PSNR\ndef calculate_psnr(ground_truth, prediction):\n    return peak_signal_noise_ratio(ground_truth, prediction, data_range=1.0)\n\n# Function to calculate SSIM\ndef calculate_ssim(ground_truth, prediction):\n    return structural_similarity(ground_truth, prediction, multichannel=True, data_range=1.0)\n\n# Use the trained model to predict on the test data\npredictions = model.predict(test_data)\n\n# Initialize lists to store PSNR and SSIM values\npsnr_list = []\nssim_list = []\n\n# Loop through each test image and calculate PSNR and SSIM\nfor i in range(len(test_data)):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    psnr = calculate_psnr(gt_image, pred_image)\n    ssim = calculate_ssim(gt_image, pred_image)\n    \n    psnr_list.append(psnr)\n    ssim_list.append(ssim)\n\n# Calculate average PSNR and SSIM\naverage_psnr = np.mean(psnr_list)\naverage_ssim = np.mean(ssim_list)\n\n# Print the results\nprint(f\"Average PSNR: {average_psnr:.4f}\")\nprint(f\"Average SSIM: {average_ssim:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Evaluation on Test Images","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for visualization\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Predict on test data\npredictions = model.predict(test_data)\n\n# Visualize the results\nnum_images_to_display = 3\n\nfor i in range(num_images_to_display):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    # Display ground truth and prediction side by side\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Ground Truth\")\n    plt.imshow((gt_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Predicted\")\n    plt.imshow((pred_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Evaluation Across Test Set","metadata":{}},{"cell_type":"code","source":"# PSNR and SSIM Evaluation Across the Test Set\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\n# Function to evaluate PSNR and SSIM for the entire test set\ndef evaluate_test_set_psnr_ssim(test_data, test_labels, predictions):\n    psnr_values = []\n    ssim_values = []\n\n    for i in range(len(test_data)):\n        gt_image = test_labels[i]\n        pred_image = predictions[i]\n        \n        psnr = peak_signal_noise_ratio(gt_image, pred_image, data_range=1.0)\n        ssim = structural_similarity(gt_image, pred_image, multichannel=True, data_range=1.0)\n        \n        psnr_values.append(psnr)\n        ssim_values.append(ssim)\n\n    # Compute the average PSNR and SSIM\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    print(f\"Average PSNR: {avg_psnr:.4f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    \n    return avg_psnr, avg_ssim\n\n# Get predictions from the model\npredictions = model.predict(test_data)\n\n# Evaluate PSNR and SSIM on the test set\navg_psnr, avg_ssim = evaluate_test_set_psnr_ssim(test_data, test_labels, predictions)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of Training History","metadata":{}},{"cell_type":"code","source":"# Import libraries for visualization\nimport matplotlib.pyplot as plt\n\n# Plot the training and validation loss\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot the training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Show the plots\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}