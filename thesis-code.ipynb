{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9406778,"sourceType":"datasetVersion","datasetId":5711376}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dense, Dropout, Add, \n                                     MultiHeadAttention, LayerNormalization, GlobalAveragePooling2D)\nfrom tensorflow.keras import Model\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T20:26:28.196502Z","iopub.execute_input":"2024-09-15T20:26:28.197336Z","iopub.status.idle":"2024-09-15T20:26:42.996724Z","shell.execute_reply.started":"2024-09-15T20:26:28.197290Z","shell.execute_reply":"2024-09-15T20:26:42.995900Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\ndef load_and_preprocess_datasets(base_folder):\n    rainy_images = []\n    clear_images = []\n    for i in range(6):  # Assuming 6 datasets\n        dataset_folder = os.path.join(base_folder, f'dataset_{i}')\n        rainy_folder = os.path.join(dataset_folder, 'rainy')\n        clear_folder = os.path.join(dataset_folder, 'clear')\n        \n        rainy_images.extend(load_images_from_folder(rainy_folder))\n        clear_images.extend(load_images_from_folder(clear_folder))\n        \n    return np.array(rainy_images), np.array(clear_images)\n\nbase_folder = 'path_to_your_dataset_folder'\nrainy_images, clear_images = load_and_preprocess_datasets(base_folder)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T20:27:52.733940Z","iopub.execute_input":"2024-09-15T20:27:52.734539Z","iopub.status.idle":"2024-09-15T20:27:53.668672Z","shell.execute_reply.started":"2024-09-15T20:27:52.734504Z","shell.execute_reply":"2024-09-15T20:27:53.667289Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     55\u001b[0m main_dataset_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/rainydata\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with actual path to main dataset folder\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m rain_images, ground_truth_images \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_dataset_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Rain Images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrain_images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Ground Truth Images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mground_truth_images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[2], line 43\u001b[0m, in \u001b[0;36mload_all_datasets\u001b[0;34m(main_dataset_folder, image_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m rain_streak_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(main_dataset_folder, dataset_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrain_streak\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m ground_truth_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(main_dataset_folder, dataset_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m rain_images, ground_truth_images \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrain_streak_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m all_rain_images\u001b[38;5;241m.\u001b[39mappend(rain_images)\n\u001b[1;32m     46\u001b[0m all_ground_truth_images\u001b[38;5;241m.\u001b[39mappend(ground_truth_images)\n","Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(rain_streak_folder, ground_truth_folder, image_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m ground_truth_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Loop through all images in the rain streak folder\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrain_streak_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     19\u001b[0m     rain_streak_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rain_streak_folder, img_name)\n\u001b[1;32m     20\u001b[0m     ground_truth_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ground_truth_folder, img_name)  \u001b[38;5;66;03m# Assuming matching names for ground truth\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/rainydata/RainStreak/rain_streak'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/rainydata/RainStreak/rain_streak'","output_type":"error"}]},{"cell_type":"markdown","source":"# Advanced Dynamic Pyramid Model","metadata":{}},{"cell_type":"code","source":"# Notebook 2: Advanced Dynamic Pyramid Model\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, Add\n\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n    \n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        x_scaled = tf.image.resize(inputs, [input_shape[0] // scale_factor, input_shape[1] // scale_factor])\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        x_conv = Conv2D(64 * scale_factor, (kernel_size, kernel_size), activation='relu', padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled])  # Residual connection\n        pyramid_features.append(x_conv)\n\n    fused_features = tf.concat(pyramid_features, axis=-1)  # Concatenate along channel dimension\n    return inputs, fused_features\n\n# Test the dynamic pyramid model\ninput_shape = (256, 256, 3)\ninputs, fused_features = advanced_dynamic_pyramid_model(input_shape)\nprint(\"Input shape:\", inputs.shape)\nprint(\"Fused features shape:\", fused_features.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T21:18:12.614237Z","iopub.execute_input":"2024-09-15T21:18:12.615099Z","iopub.status.idle":"2024-09-15T21:18:12.622119Z","shell.execute_reply.started":"2024-09-15T21:18:12.615057Z","shell.execute_reply":"2024-09-15T21:18:12.621065Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Window Partition and Reverse Functions","metadata":{}},{"cell_type":"code","source":"# Notebook 3: Window Partition and Reverse Functions\n\nimport tensorflow as tf\n\ndef window_partition(x, window_size):\n    patches = tf.image.extract_patches(images=x,\n                                       sizes=[1, window_size, window_size, 1],\n                                       strides=[1, window_size, window_size, 1],\n                                       rates=[1, 1, 1, 1],\n                                       padding='VALID')\n    return patches\n\ndef window_reverse(patches, window_size, input_shape):\n    return tf.reshape(patches, input_shape)\n\n# Test window functions\ninput_shape = (256, 256, 3)\nx = tf.random.uniform(input_shape)\nwindow_size = 4\npatches = window_partition(x, window_size)\nx_reconstructed = window_reverse(patches, window_size, input_shape)\nprint(\"Original shape:\", x.shape)\nprint(\"Reconstructed shape:\", x_reconstructed.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shifted Window Partition Function","metadata":{}},{"cell_type":"code","source":"# Notebook 4: Shifted Window Partition Function\n\nimport tensorflow as tf\n\ndef shifted_window_partition(x, window_size):\n    x_shifted = tf.roll(x, shift=window_size // 2, axis=[1, 2])\n    patches = window_partition(x_shifted, window_size)\n    return patches\n\n# Test shifted window partition function\ninput_shape = (256, 256, 3)\nx = tf.random.uniform(input_shape)\nwindow_size = 4\nshifted_patches = shifted_window_partition(x, window_size)\nprint(\"Shifted patches shape:\", shifted_patches.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recursive Block with Hierarchical Patch Merging","metadata":{}},{"cell_type":"code","source":"# Notebook 5: Recursive Block with Hierarchical Patch Merging\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dense, Add\n\ndef recursive_block(x, iteration, window_size, patch_size, num_heads, key_dim, num_iterations):\n    if iteration == 0:\n        return x\n\n    windows = window_partition(x, window_size)\n    window_shape = (windows.shape[-2], windows.shape[-1])\n    windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], x.shape[-1]))\n\n    x_norm = LayerNormalization()(windows_reshaped)\n    attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x_norm, x_norm)\n\n    attention_scores = tf.reduce_mean(tf.abs(attention), axis=-1, keepdims=True)\n    threshold = 0.1\n    attention = tf.where(attention_scores > threshold, attention, tf.zeros_like(attention))\n\n    x_add = Add()([windows_reshaped, attention])\n    x_reconstructed = window_reverse(x_add, window_size, x.shape)\n\n    x_norm_ffn = LayerNormalization()(x_reconstructed)\n    x_ffn = Dense(128, activation='relu')(x_norm_ffn)\n    x_ffn_out = Dense(x.shape[-1])(x_ffn)\n\n    x_out = Add()([x_reconstructed, x_ffn_out])\n\n    if iteration > 1:\n        x_out = hierarchical_patch_merging(x_out, window_size, patch_size)\n\n    return recursive_block(x_out, iteration - 1, window_size, patch_size, num_heads, key_dim, num_iterations)\n\ndef hierarchical_patch_merging(x, window_size, patch_size):\n    num_patches = patch_size // window_size\n    new_window_size = window_size * 2\n    new_patch_size = patch_size * 2\n\n    x_reshaped = tf.reshape(x, (-1, x.shape[1] // num_patches, num_patches, x.shape[-1]))\n    x_merged = tf.reduce_mean(x_reshaped, axis=2)\n    x_merged = tf.reshape(x_merged, (-1, new_patch_size, new_patch_size, x.shape[-1]))\n    \n    return x_merged\n\n# Test recursive block with hierarchical patch merging\ninput_shape = (256, 256, 64)\nx = tf.random.uniform(input_shape)\nwindow_size = 4\npatch_size = 4\nnum_heads = 4\nkey_dim = 64\nnum_iterations = 3\noutput = recursive_block(x, num_iterations, window_size, patch_size, num_heads, key_dim, num_iterations)\nprint(\"Output shape:\", output.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modified Swin Transformer Block","metadata":{}},{"cell_type":"code","source":"# Notebook 6: Modified Swin Transformer Block\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dense, Add, UpSampling2D\n\ndef modified_swin_transformer_block(fused_features, window_size=4, num_heads=4, key_dim=64, num_iterations=3):\n    x_out = recursive_block(fused_features, num_iterations, window_size, window_size, num_heads, key_dim, num_iterations)\n    \n    shifted_windows = shifted_window_partition(x_out, window_size)\n    shifted_windows_reshaped = tf.reshape(shifted_windows, (-1, window_size * window_size, x_out.shape[-1]))\n    shifted_x_norm = LayerNormalization()(shifted_windows_reshaped)\n    shifted_attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(shifted_x_norm, shifted_x_norm)\n    \n    shifted_x_add = Add()([shifted_windows_reshaped, shifted_attention])\n    shifted_x_reconstructed = window_reverse(shifted_x_add, window_size, x_out.shape)\n    \n    shifted_x_norm_ffn = LayerNormalization()(shifted_x_reconstructed)\n    shifted_x_ffn = Dense(128, activation='relu')(shifted_x_norm_ffn)\n    shifted_x_ffn_out = Dense(x_out.shape[-1])(shifted_x_ffn)\n    \n    x_final = Add()([shifted_x_reconstructed, shifted_x_ffn_out])\n\n    return x_final\n\n# Test modified Swin Transformer block\ninput_shape = (256, 256, 64)\nx = tf.random.uniform(input_shape)\nwindow_size = 4\nnum_heads = 4\nkey_dim = 64\nnum_iterations = 6\nx_out = modified_swin_transformer_block(x, window_size, num_heads, key_dim, num_iterations)\nprint(\"Final output shape:\", x_out.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T21:18:17.438716Z","iopub.execute_input":"2024-09-15T21:18:17.439118Z","iopub.status.idle":"2024-09-15T21:18:17.446906Z","shell.execute_reply.started":"2024-09-15T21:18:17.439083Z","shell.execute_reply":"2024-09-15T21:18:17.445967Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"# Notebook 7: Image Restoration and Enhancement\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, UpSampling2D, Conv2D, Dense\n\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n\n    x = UpSampling2D()(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D()(x)\n    x = Conv2D(num_classes, (3, 3), padding='same')(x)\n\n    return x\n\n# Test image restoration and enhancement\ninput_shape = (256, 256, 64)\nx_out = tf.random.uniform(input_shape)\nrestored_image = image_restoration_and_enhancement(x_out)\nprint(\"Restored image shape:\", restored_image.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T21:18:22.412160Z","iopub.execute_input":"2024-09-15T21:18:22.412547Z","iopub.status.idle":"2024-09-15T21:18:22.418139Z","shell.execute_reply.started":"2024-09-15T21:18:22.412510Z","shell.execute_reply":"2024-09-15T21:18:22.417186Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model Construction","metadata":{}},{"cell_type":"code","source":"# Notebook 8: Create and Summarize Model\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input\n\ndef create_model(input_shape):\n    inputs, fused_features = advanced_dynamic_pyramid_model(input_shape)\n    x_out = modified_swin_transformer_block(fused_features)\n    outputs = image_restoration_and_enhancement(x_out)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Example usage\ninput_shape = (256, 256, 3)\nmodel = create_model(input_shape)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T21:18:27.662921Z","iopub.execute_input":"2024-09-15T21:18:27.663833Z","iopub.status.idle":"2024-09-15T21:18:27.790447Z","shell.execute_reply.started":"2024-09-15T21:18:27.663777Z","shell.execute_reply":"2024-09-15T21:18:27.788977Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# RGB image\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs, fused_features \u001b[38;5;241m=\u001b[39m \u001b[43madvanced_dynamic_pyramid_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Pass to recursive modified Swin Transformer block\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x_out \u001b[38;5;241m=\u001b[39m modified_swin_transformer_block(fused_features)\n","Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36madvanced_dynamic_pyramid_model\u001b[0;34m(input_shape, num_scales)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scale \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_scales):\n\u001b[1;32m      6\u001b[0m     scale_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m----> 7\u001b[0m     x_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Dynamic kernel size based on scale\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     kernel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m scale\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"],"ename":"ValueError","evalue":"A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n","output_type":"error"}]},{"cell_type":"markdown","source":"# Compile the Model","metadata":{}},{"cell_type":"code","source":"# Cell 1: Model Compilation\n\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MeanSquaredError\n\n# Compile the model\ndef compile_model(model):\n    \"\"\"\n    Compile the model with an optimizer and loss function suitable for image restoration.\n    \"\"\"\n    optimizer = Adam(learning_rate=0.0001)\n    loss = MeanSquaredError()  # Use MSE for image restoration\n    model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n    return model\n\n# Assuming you have your model defined\nmodel = compile_model(model)\nmodel.summary()  # Optional: Show the model architecture summary\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Cell 2: Model Training\n\ndef train_model(model, train_data, val_data, batch_size=16, epochs=50):\n    \"\"\"\n    Train the model on training data with validation data for evaluation.\n    \"\"\"\n    history = model.fit(train_data, \n                        validation_data=val_data, \n                        epochs=epochs, \n                        batch_size=batch_size)\n    return history\n\n# Assuming train_data and val_data are prepared\nhistory = train_model(model, train_data, val_data, batch_size=16, epochs=50)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Cell 3: Model Evaluation\n\ndef evaluate_model(model, test_data):\n    \"\"\"\n    Evaluate the model on the test data.\n    \"\"\"\n    results = model.evaluate(test_data)\n    print(f\"Test Loss: {results[0]}, Test MAE: {results[1]}\")\n    return results\n\n# Assuming test_data is prepared\nevaluate_model(model, test_data)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Calculation","metadata":{}},{"cell_type":"code","source":"# Cell 4: PSNR and SSIM Calculation\n\nfrom skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\nimport numpy as np\n\n# PSNR and SSIM evaluation\ndef calculate_psnr_ssim(true_image, restored_image):\n    \"\"\"\n    Calculate PSNR and SSIM between the ground truth and restored images.\n    \"\"\"\n    psnr_value = psnr(true_image, restored_image, data_range=true_image.max() - true_image.min())\n    ssim_value = ssim(true_image, restored_image, multichannel=True)\n    return psnr_value, ssim_value\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Evaluation on Test Images","metadata":{}},{"cell_type":"code","source":"# Cell 5: Model Prediction and Evaluation on Test Images\n\nimport matplotlib.pyplot as plt\n\ndef predict_and_evaluate(model, test_images, ground_truth_images):\n    \"\"\"\n    Predict restored images using the model and evaluate PSNR and SSIM.\n    \"\"\"\n    for i, test_image in enumerate(test_images):\n        # Make predictions on the test image\n        restored_image = model.predict(np.expand_dims(test_image, axis=0))[0]\n        \n        # Rescale the restored image from [-1, 1] to [0, 1] (if necessary)\n        restored_image = (restored_image + 1) / 2\n        \n        # Calculate PSNR and SSIM\n        psnr_value, ssim_value = calculate_psnr_ssim(ground_truth_images[i], restored_image)\n        print(f\"Image {i+1}: PSNR = {psnr_value:.2f}, SSIM = {ssim_value:.4f}\")\n        \n        # Plot the ground truth and restored image for visual comparison\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.title(\"Ground Truth\")\n        plt.imshow(ground_truth_images[i])\n        plt.axis('off')\n        \n        plt.subplot(1, 2, 2)\n        plt.title(\"Restored Image\")\n        plt.imshow(restored_image)\n        plt.axis('off')\n        \n        plt.show()\n\n# Assuming test_images and ground_truth_images are prepared\ntest_images = []  # Add test images here\nground_truth_images = []  # Add ground truth images here\npredict_and_evaluate(model, test_images, ground_truth_images)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Evaluation Across Test Set","metadata":{}},{"cell_type":"code","source":"# Cell 6: Average PSNR and SSIM for the entire test set\n\ndef evaluate_psnr_ssim_on_testset(model, test_images, ground_truth_images):\n    \"\"\"\n    Calculate average PSNR and SSIM for the entire test set.\n    \"\"\"\n    psnr_values = []\n    ssim_values = []\n\n    for i, test_image in enumerate(test_images):\n        restored_image = model.predict(np.expand_dims(test_image, axis=0))[0]\n        restored_image = (restored_image + 1) / 2  # Rescale if necessary\n\n        psnr_value, ssim_value = calculate_psnr_ssim(ground_truth_images[i], restored_image)\n        psnr_values.append(psnr_value)\n        ssim_values.append(ssim_value)\n\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    print(f\"Average PSNR on Test Set: {avg_psnr:.2f}\")\n    print(f\"Average SSIM on Test Set: {avg_ssim:.4f}\")\n\n    return avg_psnr, avg_ssim\n\n# Assuming test_images and ground_truth_images are prepared\navg_psnr, avg_ssim = evaluate_psnr_ssim_on_testset(model, test_images, ground_truth_images)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of Training History","metadata":{}},{"cell_type":"code","source":"# Cell 7: Visualize the training and validation loss\n\ndef plot_training_history(history):\n    \"\"\"\n    Plot the training and validation loss curves.\n    \"\"\"\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# Visualize the training history\nplot_training_history(history)\n","metadata":{},"execution_count":null,"outputs":[]}]}