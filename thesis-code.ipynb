{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dense, Dropout, Add, \n                                     MultiHeadAttention, LayerNormalization, GlobalAveragePooling2D)\nfrom tensorflow.keras import Model\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T20:55:04.976906Z","iopub.execute_input":"2024-09-22T20:55:04.977520Z","iopub.status.idle":"2024-09-22T20:55:18.156910Z","shell.execute_reply.started":"2024-09-22T20:55:04.977475Z","shell.execute_reply":"2024-09-22T20:55:18.155894Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom skimage.io import imread\nfrom skimage import img_as_float\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load and preprocess images from a given folder\ndef load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\n# Data preprocessing and augmentation pipeline\ndef preprocess_and_augment_data(images, augment=False):\n    if augment:\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.01,\n            zoom_range=[0.9, 1.25],\n            horizontal_flip=True,\n            fill_mode='reflect'\n        )\n        augmented_images = datagen.flow(images, batch_size=len(images), shuffle=False)\n        augmented_images = next(augmented_images)  # Fetch the augmented images\n        return augmented_images\n    return images\n\n# Function to load datasets from base folder (for both training and testing)\ndef load_and_preprocess_datasets(base_folder, size=(128, 128), augment=False):\n    rainy_images_train, clear_images_train = [], []\n    rainy_images_test, clear_images_test = [], []  # Corrected initialization\n\n    datasets = ['Rain200L', 'Rain200H']  # Add more datasets if necessary\n\n    for dataset in datasets:\n        train_folder = os.path.join(base_folder, dataset, 'train')\n        test_folder = os.path.join(base_folder, dataset, 'test')\n\n        # Load training data\n        rainy_train_folder = os.path.join(train_folder, 'input')\n        clear_train_folder = os.path.join(train_folder, 'target')\n        rainy_images_train.extend(load_images_from_folder(rainy_train_folder, size))\n        clear_images_train.extend(load_images_from_folder(clear_train_folder, size))\n\n        # Load testing data\n        rainy_test_folder = os.path.join(test_folder, 'input')\n        clear_test_folder = os.path.join(test_folder, 'target')\n        rainy_images_test.extend(load_images_from_folder(rainy_test_folder, size))\n        clear_images_test.extend(load_images_from_folder(clear_test_folder, size))\n\n    # Convert lists to numpy arrays\n    rainy_images_train = np.array(rainy_images_train)\n    clear_images_train = np.array(clear_images_train)\n    rainy_images_test = np.array(rainy_images_test)\n    clear_images_test = np.array(clear_images_test)\n\n    # Apply data augmentation to training data (if augment is True)\n    if augment:\n        rainy_images_train = preprocess_and_augment_data(rainy_images_train, augment=True)\n        clear_images_train = preprocess_and_augment_data(clear_images_train, augment=True)\n\n    return (rainy_images_train, clear_images_train), (rainy_images_test, clear_images_test)\n\n# Example usage\nbase_folder = '/kaggle/input/derainingdata/RainData'\n(train_rainy, train_clear), (test_rainy, test_clear) = load_and_preprocess_datasets(base_folder, size=(128, 128), augment=True)\n\nprint(f\"Training Rainy Images Shape: {train_rainy.shape}\")\nprint(f\"Training Clear Images Shape: {train_clear.shape}\")\nprint(f\"Testing Rainy Images Shape: {test_rainy.shape}\")\nprint(f\"Testing Clear Images Shape: {test_clear.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:02:53.618630Z","iopub.execute_input":"2024-09-22T21:02:53.619376Z","iopub.status.idle":"2024-09-22T21:04:45.396870Z","shell.execute_reply.started":"2024-09-22T21:02:53.619339Z","shell.execute_reply":"2024-09-22T21:04:45.395848Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training Rainy Images Shape: (3600, 128, 128, 3)\nTraining Clear Images Shape: (3600, 128, 128, 3)\nTesting Rainy Images Shape: (400, 128, 128, 3)\nTesting Clear Images Shape: (400, 128, 128, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Advanced Dynamic Pyramid Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.layers import Input, Conv2D, Add, Lambda, LayerNormalization, MultiHeadAttention, Dense, Layer, UpSampling2D\nfrom tensorflow.keras.models import Model\n\n# Advanced Dynamic Pyramid Model\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n\n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        \n        # Resizing each input for different scales\n        x_scaled = Lambda(lambda x: tf.image.resize(x, \n                            [input_shape[0] // scale_factor, input_shape[1] // scale_factor]))(inputs)\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        x_conv = Conv2D(64 * scale_factor, (kernel_size, kernel_size), activation='relu', padding='same')(x_scaled)\n        \n        # Residual connection\n        x_scaled_conv = Conv2D(64 * scale_factor, (1, 1), padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled_conv])\n        \n        # Resize back to original input size before concatenation\n        x_resized = Lambda(lambda x: tf.image.resize(x, \n                              [input_shape[0], input_shape[1]]))(x_conv)\n        \n        pyramid_features.append(x_resized)\n\n    # Concatenate all pyramid features along channel axis\n    fused_features = Lambda(\n        lambda x: tf.concat(x, axis=-1),\n    )(pyramid_features)\n    \n    return inputs, fused_features\n\n# Custom Layer: Window Partition Layer\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        windowed = tf.image.extract_patches(\n            inputs,\n            sizes=[1, self.window_size, self.window_size, 1],\n            strides=[1, self.window_size, self.window_size, 1],\n            rates=[1, 1, 1, 1],\n            padding='VALID'\n        )\n        windowed = tf.reshape(windowed, (batch_size, -1, self.window_size * self.window_size, channels))\n        return windowed\n\n# Custom Layer: Patch Merge Layer\nclass PatchMergeLayer(Layer):\n    def __init__(self, window_size):\n        super(PatchMergeLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        batch_size, num_windows, flattened_window_size, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        merged = tf.reshape(inputs, (batch_size, int(num_windows**0.5), int(num_windows**0.5), self.window_size, self.window_size, channels))\n        merged = tf.transpose(merged, perm=[0, 1, 3, 2, 4, 5])\n        merged = tf.reshape(merged, (batch_size, merged.shape[1] * self.window_size, merged.shape[3] * self.window_size, channels))\n        return merged, self.window_size * 2\n\n# Modified Swin Transformer Block\nclass ModifiedSwinTransformerBlock(Layer):\n    def __init__(self, initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6):\n        super(ModifiedSwinTransformerBlock, self).__init__()\n        self.initial_window_size = initial_window_size\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.num_recursions = num_recursions\n\n    def call(self, fused_features):\n        original_shape = tf.shape(fused_features)\n        return self.recursive_block(fused_features, self.initial_window_size, 2, self.num_recursions, original_shape)\n\n    def recursive_block(self, x, window_size, iteration, recursion, original_shape):\n        if recursion == 0:\n            return x\n        \n        for _ in range(iteration):\n            # Partition the window\n            partition_layer = WindowPartitionLayer(window_size)\n            windows = partition_layer(x)\n\n            window_shape = (tf.shape(windows)[-2], tf.shape(windows)[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], tf.shape(x)[-1]))\n\n            # Ensure input is reshaped correctly for LayerNormalization\n            x_norm = LayerNormalization(axis=-1)(windows_reshaped)\n\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n            attention_scores = tf.reduce_mean(tf.abs(attention), axis=-1, keepdims=True)\n            threshold = 0.1\n            attention = tf.where(attention_scores > threshold, attention, tf.zeros_like(attention))\n\n            x_add = Add()([windows_reshaped, attention])\n            x_reconstructed = self.window_reverse(x_add, original_shape)\n\n            # Apply LayerNormalization and FFN\n            x_norm_ffn = LayerNormalization(axis=-1)(x_reconstructed)\n            x_ffn = Dense(128, activation='relu')(x_norm_ffn)\n            x_ffn_out = Dense(tf.shape(x)[-1])(x_ffn)\n\n            x_out = Add()([x_reconstructed, x_ffn_out])\n            \n            # Merge the patches and continue recursion\n            patch_merge_layer = PatchMergeLayer(window_size)\n            x_out, window_size = patch_merge_layer(x_out)\n\n        return self.recursive_block(x_out, window_size, iteration, recursion - 1, original_shape)\n\n    def window_reverse(self, x, original_shape):\n        batch_size = tf.shape(x)[0]\n        num_windows = tf.shape(x)[1] // (self.initial_window_size * self.initial_window_size)\n        x = tf.reshape(x, (batch_size, num_windows, self.initial_window_size, self.initial_window_size, -1))\n        return tf.reshape(x, (batch_size, original_shape[1], original_shape[2], -1))\n\n# Image Restoration and Enhancement Block\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n\n# Full Model Construction\ndef build_full_model(input_shape):\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_block = ModifiedSwinTransformerBlock(initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6)\n    transformer_output = transformer_block(pyramid_features)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\n# Compile the model\ninput_shape = (256, 256, 3)  # Example input shape\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T22:03:23.726958Z","iopub.execute_input":"2024-09-22T22:03:23.727355Z","iopub.status.idle":"2024-09-22T22:03:24.118257Z","shell.execute_reply.started":"2024-09-22T22:03:23.727320Z","shell.execute_reply":"2024-09-22T22:03:24.117053Z"},"trusted":true},"execution_count":66,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 147\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m    146\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Example input shape\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_full_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Summary of the model\u001b[39;00m\n","Cell \u001b[0;32mIn[66], line 139\u001b[0m, in \u001b[0;36mbuild_full_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m    137\u001b[0m inputs, pyramid_features \u001b[38;5;241m=\u001b[39m advanced_dynamic_pyramid_model(input_shape\u001b[38;5;241m=\u001b[39minput_shape, num_scales\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    138\u001b[0m transformer_block \u001b[38;5;241m=\u001b[39m ModifiedSwinTransformerBlock(initial_window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, key_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_recursions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyramid_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m outputs \u001b[38;5;241m=\u001b[39m image_restoration_and_enhancement(x_out\u001b[38;5;241m=\u001b[39mtransformer_output, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    141\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[66], line 80\u001b[0m, in \u001b[0;36mModifiedSwinTransformerBlock.call\u001b[0;34m(self, fused_features)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, fused_features):\n\u001b[1;32m     79\u001b[0m     original_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(fused_features)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_window_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_recursions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_shape\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[66], line 106\u001b[0m, in \u001b[0;36mModifiedSwinTransformerBlock.recursive_block\u001b[0;34m(self, x, window_size, iteration, recursion, original_shape)\u001b[0m\n\u001b[1;32m    103\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_reverse(x_add, original_shape)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Apply LayerNormalization and FFN\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m x_norm_ffn \u001b[38;5;241m=\u001b[39m \u001b[43mLayerNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reconstructed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m x_ffn \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x_norm_ffn)\n\u001b[1;32m    108\u001b[0m x_ffn_out \u001b[38;5;241m=\u001b[39m Dense(tf\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])(x_ffn)\n","\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling ModifiedSwinTransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'modified_swin_transformer_block_7' (of type ModifiedSwinTransformerBlock). Either the `ModifiedSwinTransformerBlock.call()` method is incorrect, or you need to implement the `ModifiedSwinTransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nShapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None,) for variable path='modified_swin_transformer_block_7/layer_normalization_16/gamma'\u001b[0m\n\nArguments received by ModifiedSwinTransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 256, 256, 448), dtype=float32, sparse=False, name=keras_tensor_637>',)\n  • kwargs=<class 'inspect._empty'>"],"ename":"RuntimeError","evalue":"Exception encountered when calling ModifiedSwinTransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'modified_swin_transformer_block_7' (of type ModifiedSwinTransformerBlock). Either the `ModifiedSwinTransformerBlock.call()` method is incorrect, or you need to implement the `ModifiedSwinTransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nShapes used to initialize variables must be fully-defined (no `None` dimensions). Received: shape=(None,) for variable path='modified_swin_transformer_block_7/layer_normalization_16/gamma'\u001b[0m\n\nArguments received by ModifiedSwinTransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 256, 256, 448), dtype=float32, sparse=False, name=keras_tensor_637>',)\n  • kwargs=<class 'inspect._empty'>","output_type":"error"}]},{"cell_type":"markdown","source":"# Window Partition and Reverse Functions","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\nclass WindowPartitionLayer(Layer):\n    def __init__(self, window_size):\n        super(WindowPartitionLayer, self).__init__()\n        self.window_size = window_size\n\n    def call(self, inputs):\n        patches = tf.image.extract_patches(images=inputs,\n                                           sizes=[1, self.window_size, self.window_size, 1],\n                                           strides=[1, self.window_size, self.window_size, 1],\n                                           rates=[1, 1, 1, 1],\n                                           padding='VALID')\n        return patches\n\ndef window_partition(x, window_size):\n    partition_layer = WindowPartitionLayer(window_size)\n    return partition_layer(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:37:06.398262Z","iopub.execute_input":"2024-09-22T21:37:06.399081Z","iopub.status.idle":"2024-09-22T21:37:06.406498Z","shell.execute_reply.started":"2024-09-22T21:37:06.399041Z","shell.execute_reply":"2024-09-22T21:37:06.405448Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# Patch Merging Function","metadata":{}},{"cell_type":"code","source":"# Patch Merging Function\ndef patch_merge(x, window_size):\n    \"\"\"Merge adjacent patches to create a larger feature window.\"\"\"\n    partition_layer = WindowPartitionLayer(window_size)\n    patches = partition_layer(x)\n\n    # Calculate the new merged patch (by averaging or pooling)\n    merged_patches = tf.reduce_mean(patches, axis=-1, keepdims=True)\n\n    # Reshape into the original window size\n    new_window_size = window_size * 2\n    merged = window_reverse(merged_patches, new_window_size, x.shape)\n\n    return merged, new_window_size","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:30:48.687892Z","iopub.execute_input":"2024-09-22T21:30:48.688248Z","iopub.status.idle":"2024-09-22T21:30:48.694236Z","shell.execute_reply.started":"2024-09-22T21:30:48.688214Z","shell.execute_reply":"2024-09-22T21:30:48.693189Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Modified Swin Transformer Block with Recursion and Iteration","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\n\ndef modified_swin_transformer_block(fused_features, initial_window_size, num_heads, key_dim, num_recursions):\n    # Define a recursive block as a Keras layer\n    class RecursiveBlock(Layer):\n        def __init__(self, window_size, num_heads, key_dim, num_recursions):\n            super().__init__()\n            self.window_size = window_size\n            self.num_heads = num_heads\n            self.key_dim = key_dim\n            self.num_recursions = num_recursions\n\n        def call(self, x):\n            if self.num_recursions <= 0:\n                return x\n            \n            # Partition the input into windows\n            windows = window_partition(x, self.window_size)\n            window_shape = (windows.shape[-2], windows.shape[-1])\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], x.shape[-1]))\n\n            # Layer Normalization before Attention\n            x_norm = LayerNormalization()(windows_reshaped)\n            attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(x_norm, x_norm)\n\n            # Apply reverse window operation\n            x_out = window_reverse(attention, window_shape, x.shape)\n            x_out, window_size = patch_merge(x_out, window_size)\n\n            # Recur on the output\n            return RecursiveBlock(window_size, self.num_heads, self.key_dim, self.num_recursions - 1)(x_out)\n\n    # Instantiate and call the recursive block\n    recursive_layer = RecursiveBlock(initial_window_size, num_heads, key_dim, num_recursions)\n    return recursive_layer(fused_features)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:38:27.279997Z","iopub.execute_input":"2024-09-22T21:38:27.280623Z","iopub.status.idle":"2024-09-22T21:38:27.290579Z","shell.execute_reply.started":"2024-09-22T21:38:27.280583Z","shell.execute_reply":"2024-09-22T21:38:27.289577Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, Conv2D, UpSampling2D\n\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    x = LayerNormalization()(x_out)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = LayerNormalization()(x)\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)\n    return outputs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:37:13.163540Z","iopub.execute_input":"2024-09-22T21:37:13.164274Z","iopub.status.idle":"2024-09-22T21:37:13.171030Z","shell.execute_reply.started":"2024-09-22T21:37:13.164236Z","shell.execute_reply":"2024-09-22T21:37:13.170142Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Construct and Compile the Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n\ndef build_full_model(input_shape):\n    inputs = Input(shape=input_shape)\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    transformer_output = modified_swin_transformer_block(fused_features=pyramid_features, \n                                                         initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6)\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T21:38:31.546294Z","iopub.execute_input":"2024-09-22T21:38:31.546687Z","iopub.status.idle":"2024-09-22T21:38:31.852519Z","shell.execute_reply.started":"2024-09-22T21:38:31.546651Z","shell.execute_reply":"2024-09-22T21:38:31.851174Z"},"trusted":true},"execution_count":53,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     12\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_full_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n","Cell \u001b[0;32mIn[53], line 6\u001b[0m, in \u001b[0;36mbuild_full_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m      5\u001b[0m inputs, pyramid_features \u001b[38;5;241m=\u001b[39m advanced_dynamic_pyramid_model(input_shape\u001b[38;5;241m=\u001b[39minput_shape, num_scales\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_swin_transformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyramid_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43minitial_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_recursions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m image_restoration_and_enhancement(x_out\u001b[38;5;241m=\u001b[39mtransformer_output, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n","Cell \u001b[0;32mIn[52], line 35\u001b[0m, in \u001b[0;36mmodified_swin_transformer_block\u001b[0;34m(fused_features, initial_window_size, num_heads, key_dim, num_recursions)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Instantiate and call the recursive block\u001b[39;00m\n\u001b[1;32m     34\u001b[0m recursive_layer \u001b[38;5;241m=\u001b[39m RecursiveBlock(initial_window_size, num_heads, key_dim, num_recursions)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursive_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_features\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[52], line 27\u001b[0m, in \u001b[0;36mmodified_swin_transformer_block.<locals>.RecursiveBlock.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m attention \u001b[38;5;241m=\u001b[39m MultiHeadAttention(num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, key_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim)(x_norm, x_norm)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Apply reverse window operation\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[43mwindow_reverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m x_out, window_size \u001b[38;5;241m=\u001b[39m patch_merge(x_out, window_size)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Recur on the output\u001b[39;00m\n","Cell \u001b[0;32mIn[25], line 19\u001b[0m, in \u001b[0;36mwindow_reverse\u001b[0;34m(patches, window_size, input_shape)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwindow_reverse\u001b[39m(patches, window_size, input_shape):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reconstruct the feature map from partitioned windows.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     num_windows \u001b[38;5;241m=\u001b[39m (\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m) \u001b[38;5;241m*\u001b[39m (input_shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m window_size)\n\u001b[1;32m     20\u001b[0m     reshaped_patches \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(patches, [num_windows, window_size, window_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     21\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(reshaped_patches, input_shape)\n","\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling RecursiveBlock.call().\n\n\u001b[1munsupported operand type(s) for //: 'int' and 'tuple'\u001b[0m\n\nArguments received by RecursiveBlock.call():\n  • args=('<KerasTensor shape=(None, 256, 256, 448), dtype=float32, sparse=False, name=keras_tensor_239>',)\n  • kwargs=<class 'inspect._empty'>"],"ename":"TypeError","evalue":"Exception encountered when calling RecursiveBlock.call().\n\n\u001b[1munsupported operand type(s) for //: 'int' and 'tuple'\u001b[0m\n\nArguments received by RecursiveBlock.call():\n  • args=('<KerasTensor shape=(None, 256, 256, 448), dtype=float32, sparse=False, name=keras_tensor_239>',)\n  • kwargs=<class 'inspect._empty'>","output_type":"error"}]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for training\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Set up input shape and define the model\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\n\n# Compile the model with Adam optimizer and MSE loss\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Define callbacks for saving the best model and early stopping\ncheckpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Define training parameters\nepochs = 50\nbatch_size = 16\n\n# Train the model\nhistory = model.fit(train_data, train_labels,\n                    validation_data=(val_data, val_labels),\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[checkpoint, early_stopping])\n\n# Save the final model\nmodel.save('final_model.h5')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for evaluation\nimport tensorflow as tf\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Evaluate the model on the test data\ntest_loss, test_accuracy = model.evaluate(test_data, test_labels)\n\n# Print the test results\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Calculation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for PSNR and SSIM calculations\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport numpy as np\n\n# Function to calculate PSNR\ndef calculate_psnr(ground_truth, prediction):\n    return peak_signal_noise_ratio(ground_truth, prediction, data_range=1.0)\n\n# Function to calculate SSIM\ndef calculate_ssim(ground_truth, prediction):\n    return structural_similarity(ground_truth, prediction, multichannel=True, data_range=1.0)\n\n# Use the trained model to predict on the test data\npredictions = model.predict(test_data)\n\n# Initialize lists to store PSNR and SSIM values\npsnr_list = []\nssim_list = []\n\n# Loop through each test image and calculate PSNR and SSIM\nfor i in range(len(test_data)):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    psnr = calculate_psnr(gt_image, pred_image)\n    ssim = calculate_ssim(gt_image, pred_image)\n    \n    psnr_list.append(psnr)\n    ssim_list.append(ssim)\n\n# Calculate average PSNR and SSIM\naverage_psnr = np.mean(psnr_list)\naverage_ssim = np.mean(ssim_list)\n\n# Print the results\nprint(f\"Average PSNR: {average_psnr:.4f}\")\nprint(f\"Average SSIM: {average_ssim:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Evaluation on Test Images","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for visualization\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Predict on test data\npredictions = model.predict(test_data)\n\n# Visualize the results\nnum_images_to_display = 3\n\nfor i in range(num_images_to_display):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    # Display ground truth and prediction side by side\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Ground Truth\")\n    plt.imshow((gt_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Predicted\")\n    plt.imshow((pred_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Evaluation Across Test Set","metadata":{}},{"cell_type":"code","source":"# PSNR and SSIM Evaluation Across the Test Set\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\n# Function to evaluate PSNR and SSIM for the entire test set\ndef evaluate_test_set_psnr_ssim(test_data, test_labels, predictions):\n    psnr_values = []\n    ssim_values = []\n\n    for i in range(len(test_data)):\n        gt_image = test_labels[i]\n        pred_image = predictions[i]\n        \n        psnr = peak_signal_noise_ratio(gt_image, pred_image, data_range=1.0)\n        ssim = structural_similarity(gt_image, pred_image, multichannel=True, data_range=1.0)\n        \n        psnr_values.append(psnr)\n        ssim_values.append(ssim)\n\n    # Compute the average PSNR and SSIM\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    print(f\"Average PSNR: {avg_psnr:.4f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    \n    return avg_psnr, avg_ssim\n\n# Get predictions from the model\npredictions = model.predict(test_data)\n\n# Evaluate PSNR and SSIM on the test set\navg_psnr, avg_ssim = evaluate_test_set_psnr_ssim(test_data, test_labels, predictions)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of Training History","metadata":{}},{"cell_type":"code","source":"# Import libraries for visualization\nimport matplotlib.pyplot as plt\n\n# Plot the training and validation loss\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot the training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Show the plots\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}