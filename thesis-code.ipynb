{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9406778,"sourceType":"datasetVersion","datasetId":5711376}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dense, Dropout, Add, \n                                     MultiHeadAttention, LayerNormalization, GlobalAveragePooling2D)\nfrom tensorflow.keras import Model\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T20:26:28.196502Z","iopub.execute_input":"2024-09-15T20:26:28.197336Z","iopub.status.idle":"2024-09-15T20:26:42.996724Z","shell.execute_reply.started":"2024-09-15T20:26:28.197290Z","shell.execute_reply":"2024-09-15T20:26:42.995900Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder, size=(128, 128)):\n    images = []\n    for filename in glob.glob(os.path.join(folder, '*.png')):\n        img = imread(filename)\n        img = img_as_float(img)\n        img_resized = tf.image.resize(img, size).numpy()\n        images.append(img_resized)\n    return np.array(images)\n\ndef load_and_preprocess_datasets(base_folder):\n    rainy_images = []\n    clear_images = []\n    for i in range(6):  # Assuming 6 datasets\n        dataset_folder = os.path.join(base_folder, f'dataset_{i}')\n        rainy_folder = os.path.join(dataset_folder, 'rainy')\n        clear_folder = os.path.join(dataset_folder, 'clear')\n        \n        rainy_images.extend(load_images_from_folder(rainy_folder))\n        clear_images.extend(load_images_from_folder(clear_folder))\n        \n    return np.array(rainy_images), np.array(clear_images)\n\nbase_folder = 'path_to_your_dataset_folder'\nrainy_images, clear_images = load_and_preprocess_datasets(base_folder)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T20:27:52.733940Z","iopub.execute_input":"2024-09-15T20:27:52.734539Z","iopub.status.idle":"2024-09-15T20:27:53.668672Z","shell.execute_reply.started":"2024-09-15T20:27:52.734504Z","shell.execute_reply":"2024-09-15T20:27:53.667289Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     55\u001b[0m main_dataset_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/rainydata\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with actual path to main dataset folder\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m rain_images, ground_truth_images \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_dataset_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Rain Images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrain_images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Ground Truth Images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mground_truth_images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[2], line 43\u001b[0m, in \u001b[0;36mload_all_datasets\u001b[0;34m(main_dataset_folder, image_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m rain_streak_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(main_dataset_folder, dataset_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrain_streak\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m ground_truth_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(main_dataset_folder, dataset_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m rain_images, ground_truth_images \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrain_streak_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m all_rain_images\u001b[38;5;241m.\u001b[39mappend(rain_images)\n\u001b[1;32m     46\u001b[0m all_ground_truth_images\u001b[38;5;241m.\u001b[39mappend(ground_truth_images)\n","Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(rain_streak_folder, ground_truth_folder, image_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m ground_truth_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Loop through all images in the rain streak folder\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrain_streak_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     19\u001b[0m     rain_streak_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rain_streak_folder, img_name)\n\u001b[1;32m     20\u001b[0m     ground_truth_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ground_truth_folder, img_name)  \u001b[38;5;66;03m# Assuming matching names for ground truth\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/rainydata/RainStreak/rain_streak'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/rainydata/RainStreak/rain_streak'","output_type":"error"}]},{"cell_type":"markdown","source":"# Advanced Dynamic Pyramid Model","metadata":{}},{"cell_type":"code","source":"# Advanced Dynamic Pyramid Model with Multi-Scale Feature Extraction\ndef advanced_dynamic_pyramid_model(input_shape, num_scales=3):\n    inputs = Input(shape=input_shape)\n    pyramid_features = []\n    \n    for scale in range(num_scales):\n        scale_factor = 2 ** scale\n        x_scaled = tf.image.resize(inputs, [input_shape[0] // scale_factor, input_shape[1] // scale_factor])\n        \n        # Dynamic kernel size based on scale\n        kernel_size = 3 + scale\n        x_conv = Conv2D(64 * scale_factor, (kernel_size, kernel_size), activation='relu', padding='same')(x_scaled)\n        x_conv = Add()([x_conv, x_scaled])  # Residual connection\n        pyramid_features.append(x_conv)\n\n    fused_features = tf.concat(pyramid_features, axis=-1)  # Concatenate along channel dimension\n    return inputs, fused_features\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T21:18:12.614237Z","iopub.execute_input":"2024-09-15T21:18:12.615099Z","iopub.status.idle":"2024-09-15T21:18:12.622119Z","shell.execute_reply.started":"2024-09-15T21:18:12.615057Z","shell.execute_reply":"2024-09-15T21:18:12.621065Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Window Partition and Reverse Functions","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, Add, LayerNormalization, MultiHeadAttention, Dense\nfrom tensorflow.image import extract_patches\n\n# Function to partition windows\ndef window_partition(x, window_size):\n    \"\"\"Partition feature map into non-overlapping windows of size window_size.\"\"\"\n    patches = extract_patches(images=x,\n                              sizes=[1, window_size, window_size, 1],\n                              strides=[1, window_size, window_size, 1],\n                              rates=[1, 1, 1, 1],\n                              padding='VALID')\n    return patches\n\n# Function to reverse windows back into the original shape\ndef window_reverse(patches, window_size, input_shape):\n    \"\"\"Reconstruct the feature map from partitioned windows.\"\"\"\n    num_windows = (input_shape[1] // window_size) * (input_shape[2] // window_size)\n    reshaped_patches = tf.reshape(patches, [num_windows, window_size, window_size, -1])\n    reconstructed = tf.reshape(reshaped_patches, input_shape)\n    return reconstructed\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patch Merging Function","metadata":{}},{"cell_type":"code","source":"# Patch Merging Function\ndef patch_merge(x, window_size):\n    \"\"\"Merge adjacent patches to create a larger feature window.\"\"\"\n    # Extract patches (patch size = window size)\n    patches = window_partition(x, window_size)\n    \n    # Calculate the new merged patch (by averaging or pooling)\n    merged_patches = tf.reduce_mean(patches, axis=-1, keepdims=True)\n    \n    # Reshape into the original window size\n    new_window_size = window_size * 2\n    merged = window_reverse(merged_patches, new_window_size, x.shape)\n    \n    return merged, new_window_size\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modified Swin Transformer Block with Recursion and Iteration","metadata":{}},{"cell_type":"code","source":"# Modified Swin Transformer Block with Recursive Patch Merging and Feature Extraction\ndef modified_swin_transformer_block(fused_features, initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6):\n    def recursive_block(x, window_size, iteration, recursion):\n        if recursion == 0:\n            return x\n        \n        for _ in range(iteration):\n            # Partition the feature map into windows\n            windows = window_partition(x, window_size)\n            window_shape = (windows.shape[-2], windows.shape[-1])\n\n            # Reshape windows to apply multi-head attention\n            windows_reshaped = tf.reshape(windows, (-1, window_shape[0] * window_shape[1], fused_features.shape[-1]))\n\n            # Layer Normalization before Attention\n            x_norm = LayerNormalization()(windows_reshaped)\n\n            # Multi-Head Self Attention within windows\n            attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x_norm, x_norm)\n\n            # Apply thresholding to attention scores\n            attention_scores = tf.reduce_mean(tf.abs(attention), axis=-1, keepdims=True)\n            threshold = 0.1  # Example threshold for pruning attention scores\n            attention = tf.where(attention_scores > threshold, attention, tf.zeros_like(attention))\n\n            # Residual connection\n            x_add = Add()([windows_reshaped, attention])\n\n            # Reverse the window partitioning\n            x_reconstructed = window_reverse(x_add, window_size, x.shape)\n\n            # Layer Normalization before Feed-Forward Network\n            x_norm_ffn = LayerNormalization()(x_reconstructed)\n            x_ffn = Dense(128, activation='relu')(x_norm_ffn)\n            x_ffn_out = Dense(fused_features.shape[-1])(x_ffn)\n\n            # Residual connection\n            x_out = Add()([x_reconstructed, x_ffn_out])\n\n            # Patch merging to increase window size\n            x_out, window_size = patch_merge(x_out, window_size)\n\n        return recursive_block(x_out, window_size, iteration, recursion - 1)\n    \n    # Start recursive process\n    x_out = recursive_block(fused_features, initial_window_size, 2, num_recursions)\n    return x_out\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Restoration and Enhancement","metadata":{}},{"cell_type":"code","source":"# Image Restoration and Enhancement Block for final image reconstruction\ndef image_restoration_and_enhancement(x_out, num_classes=3):\n    \"\"\"\n    Image Restoration and Enhancement Block for image reconstruction.\n    - num_classes: Number of output channels (3 for RGB)\n    \"\"\"\n    # Layer normalization before enhancement process\n    x = LayerNormalization()(x_out)\n\n    # Enhancement with convolutional layers\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    \n    # Upsampling for restoring spatial resolution\n    x = UpSampling2D(size=(2, 2))(x)\n\n    # Additional convolution for finer restoration\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    \n    # Layer normalization before final processing\n    x = LayerNormalization()(x)\n    \n    # Final output layer to reconstruct the image with RGB channels\n    outputs = Conv2D(num_classes, (3, 3), padding='same', activation='tanh')(x)  # 'tanh' for pixel values in [-1, 1]\n\n    return outputs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T21:18:22.412160Z","iopub.execute_input":"2024-09-15T21:18:22.412547Z","iopub.status.idle":"2024-09-15T21:18:22.418139Z","shell.execute_reply.started":"2024-09-15T21:18:22.412510Z","shell.execute_reply":"2024-09-15T21:18:22.417186Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Construct and Compile the Model","metadata":{}},{"cell_type":"code","source":"# Import required layers and model creation utilities\nfrom tensorflow.keras.models import Model\n\n# Full Model Construction\ndef build_full_model(input_shape):\n    \"\"\"\n    Construct the full model by integrating dynamic pyramid, recursive modified Swin Transformer,\n    and image restoration blocks.\n    \n    Parameters:\n    - input_shape: Shape of the input image (height, width, channels)\n    \n    Returns:\n    - Model: Full TensorFlow/Keras model ready for training and testing\n    \"\"\"\n    # Step 1: Input layer\n    inputs = Input(shape=input_shape)\n    \n    # Step 2: Advanced Dynamic Pyramid for Multi-Scale Feature Extraction\n    inputs, pyramid_features = advanced_dynamic_pyramid_model(input_shape=input_shape, num_scales=3)\n    \n    # Step 3: Modified Swin Transformer with Recursive Patch Merging (6 recursions, 2 iterations per recursion)\n    transformer_output = modified_swin_transformer_block(fused_features=pyramid_features, \n                                                         initial_window_size=4, num_heads=4, key_dim=64, num_recursions=6)\n    \n    # Step 4: Image Restoration and Enhancement\n    outputs = image_restoration_and_enhancement(x_out=transformer_output, num_classes=3)\n    \n    # Step 5: Model creation\n    model = Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\n# Compile the model with appropriate optimizer and loss function\ninput_shape = (256, 256, 3)  # Example input shape (height, width, channels)\nmodel = build_full_model(input_shape)\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for training\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Set up input shape and define the model\ninput_shape = (256, 256, 3)\nmodel = build_full_model(input_shape)\n\n# Compile the model with Adam optimizer and MSE loss\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n# Define callbacks for saving the best model and early stopping\ncheckpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Define training parameters\nepochs = 50\nbatch_size = 16\n\n# Train the model\nhistory = model.fit(train_data, train_labels,\n                    validation_data=(val_data, val_labels),\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[checkpoint, early_stopping])\n\n# Save the final model\nmodel.save('final_model.h5')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for evaluation\nimport tensorflow as tf\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Evaluate the model on the test data\ntest_loss, test_accuracy = model.evaluate(test_data, test_labels)\n\n# Print the test results\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Calculation","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for PSNR and SSIM calculations\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport numpy as np\n\n# Function to calculate PSNR\ndef calculate_psnr(ground_truth, prediction):\n    return peak_signal_noise_ratio(ground_truth, prediction, data_range=1.0)\n\n# Function to calculate SSIM\ndef calculate_ssim(ground_truth, prediction):\n    return structural_similarity(ground_truth, prediction, multichannel=True, data_range=1.0)\n\n# Use the trained model to predict on the test data\npredictions = model.predict(test_data)\n\n# Initialize lists to store PSNR and SSIM values\npsnr_list = []\nssim_list = []\n\n# Loop through each test image and calculate PSNR and SSIM\nfor i in range(len(test_data)):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    psnr = calculate_psnr(gt_image, pred_image)\n    ssim = calculate_ssim(gt_image, pred_image)\n    \n    psnr_list.append(psnr)\n    ssim_list.append(ssim)\n\n# Calculate average PSNR and SSIM\naverage_psnr = np.mean(psnr_list)\naverage_ssim = np.mean(ssim_list)\n\n# Print the results\nprint(f\"Average PSNR: {average_psnr:.4f}\")\nprint(f\"Average SSIM: {average_ssim:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Evaluation on Test Images","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for visualization\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the trained model\nmodel = tf.keras.models.load_model('best_model.h5')\n\n# Predict on test data\npredictions = model.predict(test_data)\n\n# Visualize the results\nnum_images_to_display = 3\n\nfor i in range(num_images_to_display):\n    gt_image = test_labels[i]\n    pred_image = predictions[i]\n    \n    # Display ground truth and prediction side by side\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Ground Truth\")\n    plt.imshow((gt_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Predicted\")\n    plt.imshow((pred_image * 255).astype(np.uint8))  # Convert to range [0, 255] for display\n    \n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM Evaluation Across Test Set","metadata":{}},{"cell_type":"code","source":"# PSNR and SSIM Evaluation Across the Test Set\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\n# Function to evaluate PSNR and SSIM for the entire test set\ndef evaluate_test_set_psnr_ssim(test_data, test_labels, predictions):\n    psnr_values = []\n    ssim_values = []\n\n    for i in range(len(test_data)):\n        gt_image = test_labels[i]\n        pred_image = predictions[i]\n        \n        psnr = peak_signal_noise_ratio(gt_image, pred_image, data_range=1.0)\n        ssim = structural_similarity(gt_image, pred_image, multichannel=True, data_range=1.0)\n        \n        psnr_values.append(psnr)\n        ssim_values.append(ssim)\n\n    # Compute the average PSNR and SSIM\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n\n    print(f\"Average PSNR: {avg_psnr:.4f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    \n    return avg_psnr, avg_ssim\n\n# Get predictions from the model\npredictions = model.predict(test_data)\n\n# Evaluate PSNR and SSIM on the test set\navg_psnr, avg_ssim = evaluate_test_set_psnr_ssim(test_data, test_labels, predictions)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of Training History","metadata":{}},{"cell_type":"code","source":"# Import libraries for visualization\nimport matplotlib.pyplot as plt\n\n# Plot the training and validation loss\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot the training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Show the plots\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}